{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4976fd",
   "metadata": {},
   "source": [
    "## Bibiliothèque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da05cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 09:59:48.443692: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-24 09:59:48.443735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pyplot as plt\n",
    "from resnet import resnet_v2\n",
    "from keras_flops import get_flops\n",
    "import time\n",
    "%matplotlib inline\n",
    "seed = tf.random.set_seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c602222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(figname):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(accuracy, label = \"train accuracy\")\n",
    "    plt.plot(val_accuracy, label = \"validation accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(l, label = \"train loss\")\n",
    "    plt.plot(val_l, label = \"validation loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(figname)\n",
    "    plt.show()\n",
    "    \n",
    "def inference_time():\n",
    "    scratch = []\n",
    "    pruned = []\n",
    "    for i in range(10):\n",
    "        t1 = time.time()\n",
    "        pred1 = scratch_model(x_test)\n",
    "        t2 = time.time()\n",
    "        scratch.append(t2-t1)\n",
    "\n",
    "        # Pruned model\n",
    "        t3 = time.time()\n",
    "        pred2 = P.model(x_test)\n",
    "        t4 = time.time()\n",
    "        pruned.append(t4-t3)\n",
    "\n",
    "    # display\n",
    "    print(\"Scratch inference time : \", np.mean(scratch), \" s\")\n",
    "    print(\"Pruned inference time : \", np.mean(pruned), \" s\")\n",
    "    return np.mean(pruned)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    somme = 0\n",
    "    for l in model.trainable_variables:\n",
    "        somme += np.count_nonzero(l)\n",
    "    return somme\n",
    "\n",
    "\n",
    "def scratch_hist():   \n",
    "    loss = dico[\"scratch_hist\"][0].history[\"loss\"]\n",
    "    val_loss = dico[\"scratch_hist\"][0].history[\"val_loss\"]\n",
    "    accuracy = dico[\"scratch_hist\"][0].history[\"sparse_categorical_accuracy\"]\n",
    "    val_accuracy =  dico[\"scratch_hist\"][0].history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "    for i in range(len( dico[\"scratch_hist\"])):\n",
    "        if i !=0:\n",
    "            loss = np.append(loss, dico[\"scratch_hist\"][i].history[\"loss\"])\n",
    "            val_loss = np.append(val_loss, dico[\"scratch_hist\"][i].history[\"val_loss\"])\n",
    "            accuracy = np.append(accuracy, dico[\"scratch_hist\"][i].history[\"sparse_categorical_accuracy\"])\n",
    "            val_accuracy =  np.append(val_accuracy, dico[\"scratch_hist\"][i].history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "    dico[\"scratch_hist\"] = (accuracy, val_accuracy, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8cf7b",
   "metadata": {},
   "source": [
    "## Loading cifar10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2339918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "y_test shape: (10000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66b092",
   "metadata": {},
   "source": [
    "## Building Resnet8 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d168fdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 10:00:06.951614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-24 10:00:06.951651: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-24 10:00:06.951682: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big25): /proc/driver/nvidia/version does not exist\n",
      "2022-06-24 10:00:06.953080: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 16, 16, 32)   544         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 32)   0           ['conv2d_5[0][0]',               \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 16, 32)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 8, 8, 64)     18496       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 8, 8, 64)     2112        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 8, 8, 64)     0           ['conv2d_8[0][0]',               \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 8, 8, 64)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_6[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78,666\n",
      "Trainable params: 78,186\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet_v2((32, 32, 3), depth = 8)\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035728c6",
   "metadata": {},
   "source": [
    "## Scratch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b271752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disctionnaire pour enregistrer les infos pertinentes\n",
    "dico = {}\n",
    "scratch_model = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dea2a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02397941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "1562/1562 [==============================] - 57s 36ms/step - loss: 1.5363 - sparse_categorical_accuracy: 0.4602 - val_loss: 2.0577 - val_sparse_categorical_accuracy: 0.3937\n",
      "Epoch 2/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 1.1897 - sparse_categorical_accuracy: 0.5954 - val_loss: 1.3060 - val_sparse_categorical_accuracy: 0.5613\n",
      "Epoch 3/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 1.0571 - sparse_categorical_accuracy: 0.6488 - val_loss: 1.1325 - val_sparse_categorical_accuracy: 0.6206\n",
      "Epoch 4/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.9693 - sparse_categorical_accuracy: 0.6791 - val_loss: 1.2540 - val_sparse_categorical_accuracy: 0.5710\n",
      "Epoch 5/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.9060 - sparse_categorical_accuracy: 0.7062 - val_loss: 0.9671 - val_sparse_categorical_accuracy: 0.6794\n",
      "Epoch 6/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.8507 - sparse_categorical_accuracy: 0.7262 - val_loss: 1.1056 - val_sparse_categorical_accuracy: 0.6372\n",
      "Epoch 7/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.8061 - sparse_categorical_accuracy: 0.7426 - val_loss: 1.1592 - val_sparse_categorical_accuracy: 0.6229\n",
      "Epoch 8/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.7662 - sparse_categorical_accuracy: 0.7585 - val_loss: 0.9555 - val_sparse_categorical_accuracy: 0.6909\n",
      "Epoch 9/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.7314 - sparse_categorical_accuracy: 0.7725 - val_loss: 1.3233 - val_sparse_categorical_accuracy: 0.5968\n",
      "Epoch 10/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.6979 - sparse_categorical_accuracy: 0.7832 - val_loss: 1.1957 - val_sparse_categorical_accuracy: 0.6208\n",
      "Epoch 11/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.6743 - sparse_categorical_accuracy: 0.7942 - val_loss: 0.9702 - val_sparse_categorical_accuracy: 0.6989\n",
      "Epoch 12/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.6482 - sparse_categorical_accuracy: 0.8048 - val_loss: 1.3713 - val_sparse_categorical_accuracy: 0.6147\n",
      "Epoch 13/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.6270 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.9153 - val_sparse_categorical_accuracy: 0.7258\n",
      "Epoch 14/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.6064 - sparse_categorical_accuracy: 0.8217 - val_loss: 0.8032 - val_sparse_categorical_accuracy: 0.7605\n",
      "Epoch 15/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.5850 - sparse_categorical_accuracy: 0.8295 - val_loss: 0.9036 - val_sparse_categorical_accuracy: 0.7365\n",
      "Epoch 16/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.5695 - sparse_categorical_accuracy: 0.8358 - val_loss: 1.2877 - val_sparse_categorical_accuracy: 0.6483\n",
      "Epoch 17/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.8431 - val_loss: 0.8508 - val_sparse_categorical_accuracy: 0.7444\n",
      "Epoch 18/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.5401 - sparse_categorical_accuracy: 0.8489 - val_loss: 0.8505 - val_sparse_categorical_accuracy: 0.7448\n",
      "Epoch 19/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.5281 - sparse_categorical_accuracy: 0.8528 - val_loss: 1.1519 - val_sparse_categorical_accuracy: 0.6696\n",
      "Epoch 20/75\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 0.5109 - sparse_categorical_accuracy: 0.8599 - val_loss: 1.1136 - val_sparse_categorical_accuracy: 0.6904\n",
      "Epoch 21/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.5026 - sparse_categorical_accuracy: 0.8639 - val_loss: 0.9133 - val_sparse_categorical_accuracy: 0.7532\n",
      "Epoch 22/75\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 0.4896 - sparse_categorical_accuracy: 0.8691 - val_loss: 1.0101 - val_sparse_categorical_accuracy: 0.7183\n",
      "Epoch 23/75\n",
      "1562/1562 [==============================] - 56s 36ms/step - loss: 0.4848 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.9021 - val_sparse_categorical_accuracy: 0.7498\n",
      "Epoch 24/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4726 - sparse_categorical_accuracy: 0.8769 - val_loss: 1.3279 - val_sparse_categorical_accuracy: 0.6664\n",
      "Epoch 25/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.4638 - sparse_categorical_accuracy: 0.8803 - val_loss: 1.1202 - val_sparse_categorical_accuracy: 0.7163\n",
      "Epoch 26/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4601 - sparse_categorical_accuracy: 0.8814 - val_loss: 0.9610 - val_sparse_categorical_accuracy: 0.7390\n",
      "Epoch 27/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.4514 - sparse_categorical_accuracy: 0.8853 - val_loss: 1.1661 - val_sparse_categorical_accuracy: 0.6983\n",
      "Epoch 28/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.4451 - sparse_categorical_accuracy: 0.8904 - val_loss: 1.2535 - val_sparse_categorical_accuracy: 0.6936\n",
      "Epoch 29/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.4375 - sparse_categorical_accuracy: 0.8924 - val_loss: 1.0324 - val_sparse_categorical_accuracy: 0.7287\n",
      "Epoch 30/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4309 - sparse_categorical_accuracy: 0.8947 - val_loss: 0.9351 - val_sparse_categorical_accuracy: 0.7507\n",
      "Epoch 31/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4282 - sparse_categorical_accuracy: 0.8978 - val_loss: 1.2369 - val_sparse_categorical_accuracy: 0.6953\n",
      "Epoch 32/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4208 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.9671 - val_sparse_categorical_accuracy: 0.7409\n",
      "Epoch 33/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4178 - sparse_categorical_accuracy: 0.9004 - val_loss: 1.0890 - val_sparse_categorical_accuracy: 0.7269\n",
      "Epoch 34/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.4085 - sparse_categorical_accuracy: 0.9067 - val_loss: 1.6069 - val_sparse_categorical_accuracy: 0.6584\n",
      "Epoch 35/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.4070 - sparse_categorical_accuracy: 0.9063 - val_loss: 1.2088 - val_sparse_categorical_accuracy: 0.7053\n",
      "Epoch 36/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.4016 - sparse_categorical_accuracy: 0.9098 - val_loss: 1.2353 - val_sparse_categorical_accuracy: 0.7164\n",
      "Epoch 37/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3963 - sparse_categorical_accuracy: 0.9115 - val_loss: 1.2090 - val_sparse_categorical_accuracy: 0.7151\n",
      "Epoch 38/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3958 - sparse_categorical_accuracy: 0.9131 - val_loss: 1.5415 - val_sparse_categorical_accuracy: 0.6444\n",
      "Epoch 39/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3897 - sparse_categorical_accuracy: 0.9149 - val_loss: 1.3660 - val_sparse_categorical_accuracy: 0.6823\n",
      "Epoch 40/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3883 - sparse_categorical_accuracy: 0.9176 - val_loss: 1.5261 - val_sparse_categorical_accuracy: 0.6677\n",
      "Epoch 41/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3842 - sparse_categorical_accuracy: 0.9198 - val_loss: 1.1383 - val_sparse_categorical_accuracy: 0.7320\n",
      "Epoch 42/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.9202 - val_loss: 1.3477 - val_sparse_categorical_accuracy: 0.6880\n",
      "Epoch 43/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3749 - sparse_categorical_accuracy: 0.9233 - val_loss: 1.3448 - val_sparse_categorical_accuracy: 0.6571\n",
      "Epoch 44/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3785 - sparse_categorical_accuracy: 0.9213 - val_loss: 1.3606 - val_sparse_categorical_accuracy: 0.6916\n",
      "Epoch 45/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3776 - sparse_categorical_accuracy: 0.9216 - val_loss: 1.4395 - val_sparse_categorical_accuracy: 0.6696\n",
      "Epoch 46/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3704 - sparse_categorical_accuracy: 0.9268 - val_loss: 1.2342 - val_sparse_categorical_accuracy: 0.7069\n",
      "Epoch 47/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.9271 - val_loss: 1.2707 - val_sparse_categorical_accuracy: 0.7272\n",
      "Epoch 48/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3706 - sparse_categorical_accuracy: 0.9269 - val_loss: 1.4567 - val_sparse_categorical_accuracy: 0.6806\n",
      "Epoch 49/75\n",
      "1562/1562 [==============================] - 57s 36ms/step - loss: 0.3661 - sparse_categorical_accuracy: 0.9288 - val_loss: 1.3716 - val_sparse_categorical_accuracy: 0.6776\n",
      "Epoch 50/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3732 - sparse_categorical_accuracy: 0.9253 - val_loss: 1.5523 - val_sparse_categorical_accuracy: 0.6650\n",
      "Epoch 51/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3637 - sparse_categorical_accuracy: 0.9308 - val_loss: 1.1848 - val_sparse_categorical_accuracy: 0.7365\n",
      "Epoch 52/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3573 - sparse_categorical_accuracy: 0.9337 - val_loss: 1.2002 - val_sparse_categorical_accuracy: 0.7241\n",
      "Epoch 53/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3650 - sparse_categorical_accuracy: 0.9301 - val_loss: 1.3802 - val_sparse_categorical_accuracy: 0.7156\n",
      "Epoch 54/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3575 - sparse_categorical_accuracy: 0.9332 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.7378\n",
      "Epoch 55/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3563 - sparse_categorical_accuracy: 0.9352 - val_loss: 1.5673 - val_sparse_categorical_accuracy: 0.6806\n",
      "Epoch 56/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3589 - sparse_categorical_accuracy: 0.9341 - val_loss: 1.4319 - val_sparse_categorical_accuracy: 0.7082\n",
      "Epoch 57/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3577 - sparse_categorical_accuracy: 0.9339 - val_loss: 1.1089 - val_sparse_categorical_accuracy: 0.7528\n",
      "Epoch 58/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3518 - sparse_categorical_accuracy: 0.9371 - val_loss: 1.2288 - val_sparse_categorical_accuracy: 0.7377\n",
      "Epoch 59/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3553 - sparse_categorical_accuracy: 0.9355 - val_loss: 1.1786 - val_sparse_categorical_accuracy: 0.7309\n",
      "Epoch 60/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3512 - sparse_categorical_accuracy: 0.9377 - val_loss: 1.2435 - val_sparse_categorical_accuracy: 0.7192\n",
      "Epoch 61/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3435 - sparse_categorical_accuracy: 0.9419 - val_loss: 1.4676 - val_sparse_categorical_accuracy: 0.6829\n",
      "Epoch 62/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3581 - sparse_categorical_accuracy: 0.9350 - val_loss: 1.7929 - val_sparse_categorical_accuracy: 0.6359\n",
      "Epoch 63/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3547 - sparse_categorical_accuracy: 0.9365 - val_loss: 1.4266 - val_sparse_categorical_accuracy: 0.7074\n",
      "Epoch 64/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3513 - sparse_categorical_accuracy: 0.9387 - val_loss: 1.2479 - val_sparse_categorical_accuracy: 0.7305\n",
      "Epoch 65/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3495 - sparse_categorical_accuracy: 0.9391 - val_loss: 1.1623 - val_sparse_categorical_accuracy: 0.7532\n",
      "Epoch 66/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3509 - sparse_categorical_accuracy: 0.9386 - val_loss: 1.5223 - val_sparse_categorical_accuracy: 0.6957\n",
      "Epoch 67/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3520 - sparse_categorical_accuracy: 0.9382 - val_loss: 1.6007 - val_sparse_categorical_accuracy: 0.6721\n",
      "Epoch 68/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3505 - sparse_categorical_accuracy: 0.9389 - val_loss: 1.6796 - val_sparse_categorical_accuracy: 0.6563\n",
      "Epoch 69/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3477 - sparse_categorical_accuracy: 0.9395 - val_loss: 3.2540 - val_sparse_categorical_accuracy: 0.5373\n",
      "Epoch 70/75\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.3423 - sparse_categorical_accuracy: 0.9431 - val_loss: 1.7722 - val_sparse_categorical_accuracy: 0.6510\n",
      "Epoch 71/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3494 - sparse_categorical_accuracy: 0.9393 - val_loss: 1.4218 - val_sparse_categorical_accuracy: 0.6984\n",
      "Epoch 72/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3379 - sparse_categorical_accuracy: 0.9448 - val_loss: 1.4849 - val_sparse_categorical_accuracy: 0.7009\n",
      "Epoch 73/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3557 - sparse_categorical_accuracy: 0.9385 - val_loss: 1.3234 - val_sparse_categorical_accuracy: 0.7337\n",
      "Epoch 74/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3429 - sparse_categorical_accuracy: 0.9429 - val_loss: 1.2835 - val_sparse_categorical_accuracy: 0.7388\n",
      "Epoch 75/75\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.3403 - sparse_categorical_accuracy: 0.9444 - val_loss: 1.5374 - val_sparse_categorical_accuracy: 0.6836\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 1.5374 - sparse_categorical_accuracy: 0.6836\n",
      "Epoch 1/15\n",
      "1562/1562 [==============================] - 59s 37ms/step - loss: 0.2542 - sparse_categorical_accuracy: 0.9804 - val_loss: 0.9458 - val_sparse_categorical_accuracy: 0.7966\n",
      "Epoch 2/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.2295 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.9503 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 3/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.2198 - sparse_categorical_accuracy: 0.9929 - val_loss: 0.9557 - val_sparse_categorical_accuracy: 0.7974\n",
      "Epoch 4/15\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.2145 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.9613 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 5/15\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.2095 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.9608 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 6/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.2055 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.9691 - val_sparse_categorical_accuracy: 0.7968\n",
      "Epoch 7/15\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.2028 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.9690 - val_sparse_categorical_accuracy: 0.7993\n",
      "Epoch 8/15\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.1998 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.9776 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 9/15\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.1982 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.9772 - val_sparse_categorical_accuracy: 0.8011\n",
      "Epoch 10/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1950 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.9810 - val_sparse_categorical_accuracy: 0.7970\n",
      "Epoch 11/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1935 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.9869 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 12/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1910 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9847 - val_sparse_categorical_accuracy: 0.7974\n",
      "Epoch 13/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9877 - val_sparse_categorical_accuracy: 0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1883 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9904 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 15/15\n",
      "1562/1562 [==============================] - 57s 37ms/step - loss: 0.1860 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.9868 - val_sparse_categorical_accuracy: 0.8002\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.9868 - sparse_categorical_accuracy: 0.8002\n",
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 59s 37ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.9900 - val_sparse_categorical_accuracy: 0.7985\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1839 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9897 - val_sparse_categorical_accuracy: 0.8002\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.9924 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.9917 - val_sparse_categorical_accuracy: 0.8002\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1834 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.9880 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1827 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9912 - val_sparse_categorical_accuracy: 0.8006\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1828 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9887 - val_sparse_categorical_accuracy: 0.7997\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1825 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9929 - val_sparse_categorical_accuracy: 0.8011\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1823 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.9927 - val_sparse_categorical_accuracy: 0.7999\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 0.1816 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.9917 - val_sparse_categorical_accuracy: 0.8004\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.9917 - sparse_categorical_accuracy: 0.8004\n"
     ]
    }
   ],
   "source": [
    "dico[\"scratch_hist\"] = []\n",
    "for EPOCHS in [75,15,10]:\n",
    "    lr /= 10\n",
    "    scratch_model.compile(\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "            loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            )\n",
    "\n",
    "        # Train and evaluate on data.\n",
    "    hist = scratch_model.fit(x_train, y_train, \n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          steps_per_epoch = len(x_train)/BATCH_SIZE,\n",
    "          validation_data =(x_test, y_test),\n",
    "          workers =40,\n",
    "          use_multiprocessing= True,\n",
    "          )\n",
    "\n",
    "    scratch_model.evaluate(x_test, y_test)\n",
    "    dico[\"scratch_hist\"].append(hist)\n",
    "scratch_hist()\n",
    "\n",
    "np.save(\"summary.npy\", dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3c767",
   "metadata": {},
   "source": [
    "## Pruning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "565846c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruning:\n",
    "    def __init__(self, model, pruning_factor = 0.5):\n",
    "        \n",
    "        # attributs liés au model\n",
    "        self.model = model\n",
    "        self.pruning_factor = pruning_factor\n",
    "    \n",
    "    # Tensorflow utils setting\n",
    "    def compile(self,optimizer, loss_fn, metric):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_metric = metric\n",
    "    \n",
    "    # Pruning Function\n",
    "    def pruning(self, P_factor = 0.5):\n",
    "        if P_factor >=1 or P_factor <= 0:\n",
    "            raise ValueError (\"Pruning factor value Error : Pruning factor value should be ]0 ;1[\")\n",
    "        for layer in self.model.layers:\n",
    "            if \"conv\" in layer.name:\n",
    "                \n",
    "                # Récuper les kernels\n",
    "                w = layer.get_weights()[0]\n",
    "                b = layer.get_weights()[1]\n",
    "                \n",
    "                # Calcul du filtre contenant la median\n",
    "                tab = []\n",
    "                for i in range(w.shape[-1]):\n",
    "                    somme = 0\n",
    "                    for j in range(w.shape[-1]):\n",
    "                        if i !=j:\n",
    "                            somme += np.linalg.norm(w[:,:,:,i] - w[:,:,:,j])\n",
    "                    tab.append(somme)\n",
    "                    \n",
    "                # calcul du nombre de filtrer a annuler selon le facteur de pruning\n",
    "                nb_pruned_filters = int(w.shape[-1]*P_factor)\n",
    "                \n",
    "                for i in range(nb_pruned_filters):\n",
    "                    # récupérer l'indice du minimum\n",
    "                    ind_min = np.argmin(tab)\n",
    "                    \n",
    "                    #anuuler le filtre qui minimise la formule précedente\n",
    "                    w[:, :, :, ind_min] = np.zeros(w[:, :, :, ind_min].shape)\n",
    "                    \n",
    "                    # astuce pour déplacer le minimum lorsque il faut annuler plusieurs filtres\n",
    "                    tab[ind_min] = np.sum(tab)\n",
    "                \n",
    "                layer.set_weights([w, b])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    #Training algorithm\n",
    "    def train(self,x_train, y_train, val_data, val_labels, epochs = 100, batch_size= 32):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # training history storage\n",
    "        accur = []\n",
    "        L = []\n",
    "        \n",
    "        # validation history storage\n",
    "        v_accur = []\n",
    "        v_loss = []\n",
    "\n",
    "        if x_train.shape[0] % batch_size == 0:\n",
    "            nb_train_steps = x_train.shape[0] // batch_size\n",
    "        else:\n",
    "            nb_train_steps = (x_train.shape[0] // batch_size) + 1\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch ({epoch +1 }/{epochs})\")\n",
    "            for i in range(nb_train_steps):\n",
    "                # Batching data\n",
    "                x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                x = tf.constant(x)\n",
    "                y = tf.constant(y)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    predictions = self.model(x)\n",
    "                    # calcul de la loss\n",
    "                    loss = self.loss_fn(y, predictions)\n",
    "                    \n",
    "                # Calcul du gradient\n",
    "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "                \n",
    "                # Decente de gradient\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                \n",
    "                #Pruning step\n",
    "                self.pruning(P_factor = self.pruning_factor)\n",
    "                \n",
    "                # Update training metric.\n",
    "                self.acc_metric.update_state(y, predictions)\n",
    "                train_acc = self.acc_metric.result()\n",
    "                \n",
    "                print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(train_acc),  loss), end='\\r')\n",
    "            print(\"\\nValidation Step :\")\n",
    "            \n",
    "            # Validation step\n",
    "            val_accur, val_loss = self.test(val_data, val_labels)\n",
    "                \n",
    "            accur.append(float(train_acc))\n",
    "            L.append(loss)\n",
    "            \n",
    "            v_accur.append(val_accur)\n",
    "            v_loss.append(val_loss)\n",
    "            print(\"\")\n",
    "        return (accur, L, v_accur, v_loss)  \n",
    "\n",
    "    \n",
    "    # Test Step \n",
    "    def test(self,data, labels):\n",
    "        accur = []\n",
    "        l = []\n",
    "        if data.shape[0] % self.batch_size == 0:\n",
    "            nb_test_steps = data.shape[0] // self.batch_size\n",
    "        else:\n",
    "            nb_test_steps = (data.shape[0] // self.batch_size) + 1\n",
    "            \n",
    "        for i in range(nb_test_steps):\n",
    "            # Batching data\n",
    "            x = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            y = labels[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            \n",
    "            x = tf.constant(x)\n",
    "            y = tf.constant(y)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = self.model(x)\n",
    "\n",
    "            # calcul de la loss\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "            # calcul de l'accuracy\n",
    "            self.acc_metric.update_state(y, predictions)\n",
    "            test_acc = self.acc_metric.result()\n",
    "            print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(test_acc),  loss), end='\\r')\n",
    "                \n",
    "            accur.append(float(test_acc))\n",
    "            l.append(float(loss))\n",
    "        print(\"\")        \n",
    "        print(\"Accuracy Moy : {:.4f} ; loss Moy: {:.4f}\" .format(np.mean(accur), np.mean(l) ))\n",
    "        \n",
    "        return (np.mean(accur), np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5752f",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daa7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5d9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/75)\n",
      "Accuracy: 0.2039 ; loss: 1.8142\n",
      "Validation Step :\n",
      "Accuracy: 0.2147 ; loss: 1.8037\n",
      "Accuracy Moy : 0.2098 ; loss Moy: 1.8959\n",
      "\n",
      "Epoch (2/75)\n",
      "Accuracy: 0.2744 ; loss: 1.8347\n",
      "Validation Step :\n",
      "Accuracy: 0.2811 ; loss: 1.6111\n",
      "Accuracy Moy : 0.2779 ; loss Moy: 1.6901\n",
      "\n",
      "Epoch (3/75)\n",
      "Accuracy: 0.3217 ; loss: 1.7029\n",
      "Validation Step :\n",
      "Accuracy: 0.3276 ; loss: 1.6272\n",
      "Accuracy Moy : 0.3247 ; loss Moy: 1.5343\n",
      "\n",
      "Epoch (4/75)\n",
      "Accuracy: 0.3584 ; loss: 1.7056\n",
      "Validation Step :\n",
      "Accuracy: 0.3638 ; loss: 1.5612\n",
      "Accuracy Moy : 0.3612 ; loss Moy: 1.3998\n",
      "\n",
      "Epoch (5/75)\n",
      "Accuracy: 0.3881 ; loss: 1.7121\n",
      "Validation Step :\n",
      "Accuracy: 0.3926 ; loss: 1.5918\n",
      "Accuracy Moy : 0.3904 ; loss Moy: 1.3221\n",
      "\n",
      "Epoch (6/75)\n",
      "Accuracy: 0.4131 ; loss: 1.6639\n",
      "Validation Step :\n",
      "Accuracy: 0.4170 ; loss: 1.5913\n",
      "Accuracy Moy : 0.4151 ; loss Moy: 1.2601\n",
      "\n",
      "Epoch (7/75)\n",
      "Accuracy: 0.4349 ; loss: 1.6099\n",
      "Validation Step :\n",
      "Accuracy: 0.4379 ; loss: 1.5927\n",
      "Accuracy Moy : 0.4365 ; loss Moy: 1.2353\n",
      "\n",
      "Epoch (8/75)\n",
      "Accuracy: 0.4537 ; loss: 1.5656\n",
      "Validation Step :\n",
      "Accuracy: 0.4558 ; loss: 1.7303\n",
      "Accuracy Moy : 0.4548 ; loss Moy: 1.2485\n",
      "\n",
      "Epoch (9/75)\n",
      "Accuracy: 0.4699 ; loss: 1.5893\n",
      "Validation Step :\n",
      "Accuracy: 0.4717 ; loss: 1.6393\n",
      "Accuracy Moy : 0.4708 ; loss Moy: 1.1973\n",
      "\n",
      "Epoch (10/75)\n",
      "Accuracy: 0.4844 ; loss: 1.6618\n",
      "Validation Step :\n",
      "Accuracy: 0.4860 ; loss: 1.5832\n",
      "Accuracy Moy : 0.4852 ; loss Moy: 1.1801\n",
      "\n",
      "Epoch (11/75)\n",
      "Accuracy: 0.4973 ; loss: 1.5509\n",
      "Validation Step :\n",
      "Accuracy: 0.4986 ; loss: 1.7177\n",
      "Accuracy Moy : 0.4979 ; loss Moy: 1.2051\n",
      "\n",
      "Epoch (12/75)\n",
      "Accuracy: 0.5089 ; loss: 1.4500\n",
      "Validation Step :\n",
      "Accuracy: 0.5102 ; loss: 1.3148\n",
      "Accuracy Moy : 0.5096 ; loss Moy: 1.1225\n",
      "\n",
      "Epoch (13/75)\n",
      "Accuracy: 0.5196 ; loss: 1.3867\n",
      "Validation Step :\n",
      "Accuracy: 0.5208 ; loss: 1.2231\n",
      "Accuracy Moy : 0.5202 ; loss Moy: 1.0781\n",
      "\n",
      "Epoch (14/75)\n",
      "Accuracy: 0.5295 ; loss: 1.4401\n",
      "Validation Step :\n",
      "Accuracy: 0.5306 ; loss: 1.2184\n",
      "Accuracy Moy : 0.5301 ; loss Moy: 1.0623\n",
      "\n",
      "Epoch (15/75)\n",
      "Accuracy: 0.5384 ; loss: 1.3097\n",
      "Validation Step :\n",
      "Accuracy: 0.5394 ; loss: 1.2453\n",
      "Accuracy Moy : 0.5390 ; loss Moy: 1.0618\n",
      "\n",
      "Epoch (16/75)\n",
      "Accuracy: 0.5466 ; loss: 1.4455\n",
      "Validation Step :\n",
      "Accuracy: 0.5475 ; loss: 1.3375\n",
      "Accuracy Moy : 0.5471 ; loss Moy: 1.0792\n",
      "\n",
      "Epoch (17/75)\n",
      "Accuracy: 0.5542 ; loss: 1.2350\n",
      "Validation Step :\n",
      "Accuracy: 0.5549 ; loss: 1.2280\n",
      "Accuracy Moy : 0.5546 ; loss Moy: 1.0498\n",
      "\n",
      "Epoch (18/75)\n",
      "Accuracy: 0.5612 ; loss: 1.1457\n",
      "Validation Step :\n",
      "Accuracy: 0.5620 ; loss: 1.1255\n",
      "Accuracy Moy : 0.5616 ; loss Moy: 1.0247\n",
      "\n",
      "Epoch (19/75)\n",
      "Accuracy: 0.5678 ; loss: 1.1508\n",
      "Validation Step :\n",
      "Accuracy: 0.5684 ; loss: 1.2230\n",
      "Accuracy Moy : 0.5681 ; loss Moy: 1.0469\n",
      "\n",
      "Epoch (20/75)\n",
      "Accuracy: 0.5740 ; loss: 1.1314\n",
      "Validation Step :\n",
      "Accuracy: 0.5745 ; loss: 1.1037\n",
      "Accuracy Moy : 0.5742 ; loss Moy: 1.0451\n",
      "\n",
      "Epoch (21/75)\n",
      "Accuracy: 0.5797 ; loss: 1.1738\n",
      "Validation Step :\n",
      "Accuracy: 0.5800 ; loss: 1.1580\n",
      "Accuracy Moy : 0.5799 ; loss Moy: 1.0756\n",
      "\n",
      "Epoch (22/75)\n",
      "Accuracy: 0.5850 ; loss: 1.0842\n",
      "Validation Step :\n",
      "Accuracy: 0.5853 ; loss: 1.1209\n",
      "Accuracy Moy : 0.5852 ; loss Moy: 1.0847\n",
      "\n",
      "Epoch (23/75)\n",
      "Accuracy: 0.5900 ; loss: 1.0719\n",
      "Validation Step :\n",
      "Accuracy: 0.5903 ; loss: 1.1392\n",
      "Accuracy Moy : 0.5901 ; loss Moy: 1.0885\n",
      "\n",
      "Epoch (24/75)\n",
      "Accuracy: 0.5947 ; loss: 1.2424\n",
      "Validation Step :\n",
      "Accuracy: 0.5949 ; loss: 1.1022\n",
      "Accuracy Moy : 0.5948 ; loss Moy: 1.1003\n",
      "\n",
      "Epoch (25/75)\n",
      "Accuracy: 0.5992 ; loss: 1.1373\n",
      "Validation Step :\n",
      "Accuracy: 0.5993 ; loss: 1.2946\n",
      "Accuracy Moy : 0.5992 ; loss Moy: 1.1143\n",
      "\n",
      "Epoch (26/75)\n",
      "Accuracy: 0.6034 ; loss: 1.0036\n",
      "Validation Step :\n",
      "Accuracy: 0.6035 ; loss: 1.2510\n",
      "Accuracy Moy : 0.6034 ; loss Moy: 1.1235\n",
      "\n",
      "Epoch (27/75)\n",
      "Accuracy: 0.6074 ; loss: 0.9581\n",
      "Validation Step :\n",
      "Accuracy: 0.6075 ; loss: 1.2677\n",
      "Accuracy Moy : 0.6074 ; loss Moy: 1.1169\n",
      "\n",
      "Epoch (28/75)\n",
      "Accuracy: 0.6111 ; loss: 0.9976\n",
      "Validation Step :\n",
      "Accuracy: 0.6111 ; loss: 1.2920\n",
      "Accuracy Moy : 0.6111 ; loss Moy: 1.1584\n",
      "\n",
      "Epoch (29/75)\n",
      "Accuracy: 0.6147 ; loss: 0.9946\n",
      "Validation Step :\n",
      "Accuracy: 0.6147 ; loss: 1.2469\n",
      "Accuracy Moy : 0.6147 ; loss Moy: 1.1415\n",
      "\n",
      "Epoch (30/75)\n",
      "Accuracy: 0.6181 ; loss: 1.0465\n",
      "Validation Step :\n",
      "Accuracy: 0.6182 ; loss: 1.2928\n",
      "Accuracy Moy : 0.6182 ; loss Moy: 1.1368\n",
      "\n",
      "Epoch (31/75)\n",
      "Accuracy: 0.6214 ; loss: 1.0293\n",
      "Validation Step :\n",
      "Accuracy: 0.6214 ; loss: 1.3834\n",
      "Accuracy Moy : 0.6214 ; loss Moy: 1.1955\n",
      "\n",
      "Epoch (32/75)\n",
      "Accuracy: 0.6245 ; loss: 1.0245\n",
      "Validation Step :\n",
      "Accuracy: 0.6244 ; loss: 1.4174\n",
      "Accuracy Moy : 0.6245 ; loss Moy: 1.1315\n",
      "\n",
      "Epoch (33/75)\n",
      "Accuracy: 0.6274 ; loss: 1.0788\n",
      "Validation Step :\n",
      "Accuracy: 0.6274 ; loss: 1.4560\n",
      "Accuracy Moy : 0.6274 ; loss Moy: 1.1588\n",
      "\n",
      "Epoch (34/75)\n",
      "Accuracy: 0.6303 ; loss: 1.0642\n",
      "Validation Step :\n",
      "Accuracy: 0.6303 ; loss: 1.2464\n",
      "Accuracy Moy : 0.6303 ; loss Moy: 1.1778\n",
      "\n",
      "Epoch (35/75)\n",
      "Accuracy: 0.6330 ; loss: 0.9949\n",
      "Validation Step :\n",
      "Accuracy: 0.6330 ; loss: 1.3953\n",
      "Accuracy Moy : 0.6330 ; loss Moy: 1.1336\n",
      "\n",
      "Epoch (36/75)\n",
      "Accuracy: 0.6356 ; loss: 0.9526\n",
      "Validation Step :\n",
      "Accuracy: 0.6356 ; loss: 1.2854\n",
      "Accuracy Moy : 0.6357 ; loss Moy: 1.1377\n",
      "\n",
      "Epoch (37/75)\n",
      "Accuracy: 0.6382 ; loss: 1.1114\n",
      "Validation Step :\n",
      "Accuracy: 0.6381 ; loss: 1.3144\n",
      "Accuracy Moy : 0.6382 ; loss Moy: 1.1900\n",
      "\n",
      "Epoch (38/75)\n",
      "Accuracy: 0.6406 ; loss: 0.9886\n",
      "Validation Step :\n",
      "Accuracy: 0.6406 ; loss: 1.4357\n",
      "Accuracy Moy : 0.6406 ; loss Moy: 1.0910\n",
      "\n",
      "Epoch (39/75)\n",
      "Accuracy: 0.6430 ; loss: 1.1455\n",
      "Validation Step :\n",
      "Accuracy: 0.6430 ; loss: 1.3851\n",
      "Accuracy Moy : 0.6430 ; loss Moy: 1.1392\n",
      "\n",
      "Epoch (40/75)\n",
      "Accuracy: 0.6453 ; loss: 1.1376\n",
      "Validation Step :\n",
      "Accuracy: 0.6452 ; loss: 1.3807\n",
      "Accuracy Moy : 0.6453 ; loss Moy: 1.1352\n",
      "\n",
      "Epoch (41/75)\n",
      "Accuracy: 0.6475 ; loss: 1.0434\n",
      "Validation Step :\n",
      "Accuracy: 0.6474 ; loss: 1.3056\n",
      "Accuracy Moy : 0.6475 ; loss Moy: 1.1582\n",
      "\n",
      "Epoch (42/75)\n",
      "Accuracy: 0.6496 ; loss: 0.9730\n",
      "Validation Step :\n",
      "Accuracy: 0.6495 ; loss: 1.4005\n",
      "Accuracy Moy : 0.6496 ; loss Moy: 1.1361\n",
      "\n",
      "Epoch (43/75)\n",
      "Accuracy: 0.6517 ; loss: 0.9851\n",
      "Validation Step :\n",
      "Accuracy: 0.6516 ; loss: 1.2819\n",
      "Accuracy Moy : 0.6517 ; loss Moy: 1.1045\n",
      "\n",
      "Epoch (44/75)\n",
      "Accuracy: 0.6537 ; loss: 1.0016\n",
      "Validation Step :\n",
      "Accuracy: 0.6537 ; loss: 1.3980\n",
      "Accuracy Moy : 0.6537 ; loss Moy: 1.1306\n",
      "\n",
      "Epoch (45/75)\n",
      "Accuracy: 0.6557 ; loss: 0.9851\n",
      "Validation Step :\n",
      "Accuracy: 0.6556 ; loss: 1.4893\n",
      "Accuracy Moy : 0.6556 ; loss Moy: 1.1794\n",
      "\n",
      "Epoch (46/75)\n",
      "Accuracy: 0.6575 ; loss: 1.0224\n",
      "Validation Step :\n",
      "Accuracy: 0.6574 ; loss: 1.5125\n",
      "Accuracy Moy : 0.6575 ; loss Moy: 1.1618\n",
      "\n",
      "Epoch (47/75)\n",
      "Accuracy: 0.6593 ; loss: 0.8409\n",
      "Validation Step :\n",
      "Accuracy: 0.6592 ; loss: 1.4272\n",
      "Accuracy Moy : 0.6593 ; loss Moy: 1.1343\n",
      "\n",
      "Epoch (48/75)\n",
      "Accuracy: 0.6611 ; loss: 0.9154\n",
      "Validation Step :\n",
      "Accuracy: 0.6610 ; loss: 1.5755\n",
      "Accuracy Moy : 0.6611 ; loss Moy: 1.1400\n",
      "\n",
      "Epoch (49/75)\n",
      "Accuracy: 0.6628 ; loss: 0.8522\n",
      "Validation Step :\n",
      "Accuracy: 0.6627 ; loss: 1.7095\n",
      "Accuracy Moy : 0.6628 ; loss Moy: 1.1580\n",
      "\n",
      "Epoch (50/75)\n",
      "Accuracy: 0.6645 ; loss: 0.8590\n",
      "Validation Step :\n",
      "Accuracy: 0.6644 ; loss: 1.5627\n",
      "Accuracy Moy : 0.6645 ; loss Moy: 1.1017\n",
      "\n",
      "Epoch (51/75)\n",
      "Accuracy: 0.6662 ; loss: 0.9363\n",
      "Validation Step :\n",
      "Accuracy: 0.6660 ; loss: 1.7279\n",
      "Accuracy Moy : 0.6661 ; loss Moy: 1.2484\n",
      "\n",
      "Epoch (52/75)\n",
      "Accuracy: 0.6677 ; loss: 0.7950\n",
      "Validation Step :\n",
      "Accuracy: 0.6676 ; loss: 1.6753\n",
      "Accuracy Moy : 0.6676 ; loss Moy: 1.1620\n",
      "\n",
      "Epoch (53/75)\n",
      "Accuracy: 0.6692 ; loss: 0.8738\n",
      "Validation Step :\n",
      "Accuracy: 0.6691 ; loss: 2.1142\n",
      "Accuracy Moy : 0.6692 ; loss Moy: 1.1921\n",
      "\n",
      "Epoch (54/75)\n",
      "Accuracy: 0.6707 ; loss: 0.7998\n",
      "Validation Step :\n",
      "Accuracy: 0.6706 ; loss: 1.6047\n",
      "Accuracy Moy : 0.6706 ; loss Moy: 1.1297\n",
      "\n",
      "Epoch (55/75)\n",
      "Accuracy: 0.6721 ; loss: 0.7849\n",
      "Validation Step :\n",
      "Accuracy: 0.6720 ; loss: 1.6818\n",
      "Accuracy Moy : 0.6721 ; loss Moy: 1.1622\n",
      "\n",
      "Epoch (56/75)\n",
      "Accuracy: 0.6735 ; loss: 0.7876\n",
      "Validation Step :\n",
      "Accuracy: 0.6734 ; loss: 1.7547\n",
      "Accuracy Moy : 0.6735 ; loss Moy: 1.1878\n",
      "\n",
      "Epoch (57/75)\n",
      "Accuracy: 0.6749 ; loss: 0.8192\n",
      "Validation Step :\n",
      "Accuracy: 0.6748 ; loss: 1.7824\n",
      "Accuracy Moy : 0.6748 ; loss Moy: 1.1612\n",
      "\n",
      "Epoch (58/75)\n",
      "Accuracy: 0.6762 ; loss: 0.7742\n",
      "Validation Step :\n",
      "Accuracy: 0.6761 ; loss: 1.9995\n",
      "Accuracy Moy : 0.6761 ; loss Moy: 1.1644\n",
      "\n",
      "Epoch (59/75)\n",
      "Accuracy: 0.6775 ; loss: 0.7235\n",
      "Validation Step :\n",
      "Accuracy: 0.6774 ; loss: 1.6904\n",
      "Accuracy Moy : 0.6775 ; loss Moy: 1.1649\n",
      "\n",
      "Epoch (60/75)\n",
      "Accuracy: 0.6788 ; loss: 0.7819\n",
      "Validation Step :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6787 ; loss: 1.7250\n",
      "Accuracy Moy : 0.6788 ; loss Moy: 1.1797\n",
      "\n",
      "Epoch (61/75)\n",
      "Accuracy: 0.6801 ; loss: 0.8123\n",
      "Validation Step :\n",
      "Accuracy: 0.6800 ; loss: 1.6662\n",
      "Accuracy Moy : 0.6800 ; loss Moy: 1.1579\n",
      "\n",
      "Epoch (62/75)\n",
      "Accuracy: 0.6813 ; loss: 0.7894\n",
      "Validation Step :\n",
      "Accuracy: 0.6812 ; loss: 2.0540\n",
      "Accuracy Moy : 0.6812 ; loss Moy: 1.1503\n",
      "\n",
      "Epoch (63/75)\n",
      "Accuracy: 0.6825 ; loss: 0.8685\n",
      "Validation Step :\n",
      "Accuracy: 0.6824 ; loss: 1.8495\n",
      "Accuracy Moy : 0.6824 ; loss Moy: 1.1943\n",
      "\n",
      "Epoch (64/75)\n",
      "Accuracy: 0.6836 ; loss: 0.8142\n",
      "Validation Step :\n",
      "Accuracy: 0.6835 ; loss: 1.8082\n",
      "Accuracy Moy : 0.6836 ; loss Moy: 1.1697\n",
      "\n",
      "Epoch (65/75)\n",
      "Accuracy: 0.6848 ; loss: 0.7260\n",
      "Validation Step :\n",
      "Accuracy: 0.6847 ; loss: 1.9645\n",
      "Accuracy Moy : 0.6847 ; loss Moy: 1.1627\n",
      "\n",
      "Epoch (66/75)\n",
      "Accuracy: 0.6859 ; loss: 0.7974\n",
      "Validation Step :\n",
      "Accuracy: 0.6858 ; loss: 1.9510\n",
      "Accuracy Moy : 0.6858 ; loss Moy: 1.1967\n",
      "\n",
      "Epoch (67/75)\n",
      "Accuracy: 0.6869 ; loss: 0.7394\n",
      "Validation Step :\n",
      "Accuracy: 0.6868 ; loss: 1.7815\n",
      "Accuracy Moy : 0.6869 ; loss Moy: 1.2205\n",
      "\n",
      "Epoch (68/75)\n",
      "Accuracy: 0.6880 ; loss: 0.7848\n",
      "Validation Step :\n",
      "Accuracy: 0.6879 ; loss: 1.8418\n",
      "Accuracy Moy : 0.6879 ; loss Moy: 1.1996\n",
      "\n",
      "Epoch (69/75)\n",
      "Accuracy: 0.6890 ; loss: 1.0590\n",
      "Validation Step :\n",
      "Accuracy: 0.6888 ; loss: 2.0623\n",
      "Accuracy Moy : 0.6889 ; loss Moy: 1.3168\n",
      "\n",
      "Epoch (70/75)\n",
      "Accuracy: 0.6900 ; loss: 0.8976\n",
      "Validation Step :\n",
      "Accuracy: 0.6898 ; loss: 2.0845\n",
      "Accuracy Moy : 0.6899 ; loss Moy: 1.2284\n",
      "\n",
      "Epoch (71/75)\n",
      "Accuracy: 0.6909 ; loss: 0.7744\n",
      "Validation Step :\n",
      "Accuracy: 0.6908 ; loss: 1.9887\n",
      "Accuracy Moy : 0.6909 ; loss Moy: 1.1994\n",
      "\n",
      "Epoch (72/75)\n",
      "Accuracy: 0.6919 ; loss: 0.8814\n",
      "Validation Step :\n",
      "Accuracy: 0.6918 ; loss: 1.8833\n",
      "Accuracy Moy : 0.6919 ; loss Moy: 1.2429\n",
      "\n",
      "Epoch (73/75)\n",
      "Accuracy: 0.6929 ; loss: 0.8273\n",
      "Validation Step :\n",
      "Accuracy: 0.6927 ; loss: 2.0805\n",
      "Accuracy Moy : 0.6928 ; loss Moy: 1.2436\n",
      "\n",
      "Epoch (74/75)\n",
      "Accuracy: 0.6938 ; loss: 0.7878\n",
      "Validation Step :\n",
      "Accuracy: 0.6937 ; loss: 1.7120\n",
      "Accuracy Moy : 0.6937 ; loss Moy: 1.2406\n",
      "\n",
      "Epoch (75/75)\n",
      "Accuracy: 0.6947 ; loss: 0.7579\n",
      "Validation Step :\n",
      "Accuracy: 0.6946 ; loss: 1.9645\n",
      "Accuracy Moy : 0.6947 ; loss Moy: 1.2166\n",
      "\n",
      "Epoch (1/15)\n",
      "Accuracy: 0.8068 ; loss: 0.5169\n",
      "Validation Step :\n",
      "Accuracy: 0.7830 ; loss: 1.5795\n",
      "Accuracy Moy : 0.7946 ; loss Moy: 1.1356\n",
      "\n",
      "Epoch (2/15)\n",
      "Accuracy: 0.7986 ; loss: 0.4965\n",
      "Validation Step :\n",
      "Accuracy: 0.7873 ; loss: 1.6167\n",
      "Accuracy Moy : 0.7930 ; loss Moy: 1.1444\n",
      "\n",
      "Epoch (3/15)\n",
      "Accuracy: 0.7971 ; loss: 0.4919\n",
      "Validation Step :\n",
      "Accuracy: 0.7897 ; loss: 1.6204\n",
      "Accuracy Moy : 0.7935 ; loss Moy: 1.1530\n",
      "\n",
      "Epoch (4/15)\n",
      "Accuracy: 0.7973 ; loss: 0.4823\n",
      "Validation Step :\n",
      "Accuracy: 0.7918 ; loss: 1.6289\n",
      "Accuracy Moy : 0.7946 ; loss Moy: 1.1628\n",
      "\n",
      "Epoch (5/15)\n",
      "Accuracy: 0.7979 ; loss: 0.4978\n",
      "Validation Step :\n",
      "Accuracy: 0.7934 ; loss: 1.6541\n",
      "Accuracy Moy : 0.7957 ; loss Moy: 1.1699\n",
      "\n",
      "Epoch (6/15)\n",
      "Accuracy: 0.7986 ; loss: 0.4898\n",
      "Validation Step :\n",
      "Accuracy: 0.7949 ; loss: 1.6501\n",
      "Accuracy Moy : 0.7968 ; loss Moy: 1.1799\n",
      "\n",
      "Epoch (7/15)\n",
      "Accuracy: 0.7994 ; loss: 0.4783\n",
      "Validation Step :\n",
      "Accuracy: 0.7962 ; loss: 1.6728\n",
      "Accuracy Moy : 0.7979 ; loss Moy: 1.1881\n",
      "\n",
      "Epoch (8/15)\n",
      "Accuracy: 0.8002 ; loss: 0.4690\n",
      "Validation Step :\n",
      "Accuracy: 0.7974 ; loss: 1.6776\n",
      "Accuracy Moy : 0.7989 ; loss Moy: 1.1962\n",
      "\n",
      "Epoch (9/15)\n",
      "Accuracy: 0.8010 ; loss: 0.4663\n",
      "Validation Step :\n",
      "Accuracy: 0.7985 ; loss: 1.7023\n",
      "Accuracy Moy : 0.7998 ; loss Moy: 1.2055\n",
      "\n",
      "Epoch (10/15)\n",
      "Accuracy: 0.8017 ; loss: 0.4624\n",
      "Validation Step :\n",
      "Accuracy: 0.7994 ; loss: 1.7154\n",
      "Accuracy Moy : 0.8006 ; loss Moy: 1.2134\n",
      "\n",
      "Epoch (11/15)\n",
      "Accuracy: 0.8023 ; loss: 0.4600\n",
      "Validation Step :\n",
      "Accuracy: 0.8002 ; loss: 1.7312\n",
      "Accuracy Moy : 0.8013 ; loss Moy: 1.2217\n",
      "\n",
      "Epoch (12/15)\n",
      "Accuracy: 0.8030 ; loss: 0.4598\n",
      "Validation Step :\n",
      "Accuracy: 0.8010 ; loss: 1.7536\n",
      "Accuracy Moy : 0.8020 ; loss Moy: 1.2298\n",
      "\n",
      "Epoch (13/15)\n",
      "Accuracy: 0.8035 ; loss: 0.4592\n",
      "Validation Step :\n",
      "Accuracy: 0.8017 ; loss: 1.7696\n",
      "Accuracy Moy : 0.8027 ; loss Moy: 1.2373\n",
      "\n",
      "Epoch (14/15)\n",
      "Accuracy: 0.8041 ; loss: 0.4591\n",
      "Validation Step :\n",
      "Accuracy: 0.8024 ; loss: 1.7806\n",
      "Accuracy Moy : 0.8033 ; loss Moy: 1.2435\n",
      "\n",
      "Epoch (15/15)\n",
      "Accuracy: 0.8047 ; loss: 0.4634\n",
      "Validation Step :\n",
      "Accuracy: 0.8031 ; loss: 1.7868\n",
      "Accuracy Moy : 0.8039 ; loss Moy: 1.2521\n",
      "\n",
      "Epoch (1/10)\n",
      "Accuracy: 0.8342 ; loss: 0.4048\n",
      "Validation Step :\n",
      "Accuracy: 0.8055 ; loss: 1.8440\n",
      "Accuracy Moy : 0.8194 ; loss Moy: 1.2240\n",
      "\n",
      "Epoch (2/10)\n",
      "Accuracy: 0.8203 ; loss: 0.4147\n",
      "Validation Step :\n",
      "Accuracy: 0.8070 ; loss: 1.8427\n",
      "Accuracy Moy : 0.8137 ; loss Moy: 1.2214\n",
      "\n",
      "Epoch (3/10)\n",
      "Accuracy: 0.8165 ; loss: 0.4166\n",
      "Validation Step :\n",
      "Accuracy: 0.8078 ; loss: 1.8322\n",
      "Accuracy Moy : 0.8122 ; loss Moy: 1.2201\n",
      "\n",
      "Epoch (4/10)\n",
      "Accuracy: 0.8147 ; loss: 0.4171\n",
      "Validation Step :\n",
      "Accuracy: 0.8083 ; loss: 1.8297\n",
      "Accuracy Moy : 0.8116 ; loss Moy: 1.2192\n",
      "\n",
      "Epoch (5/10)\n",
      "Accuracy: 0.8138 ; loss: 0.4173\n",
      "Validation Step :\n",
      "Accuracy: 0.8087 ; loss: 1.8267\n",
      "Accuracy Moy : 0.8113 ; loss Moy: 1.2188\n",
      "\n",
      "Epoch (6/10)\n",
      "Accuracy: 0.8132 ; loss: 0.4179\n",
      "Validation Step :\n",
      "Accuracy: 0.8090 ; loss: 1.8252\n",
      "Accuracy Moy : 0.8111 ; loss Moy: 1.2185\n",
      "\n",
      "Epoch (7/10)\n",
      "Accuracy: 0.8128 ; loss: 0.4174\n",
      "Validation Step :\n",
      "Accuracy: 0.8092 ; loss: 1.8237\n",
      "Accuracy Moy : 0.8110 ; loss Moy: 1.2184\n",
      "\n",
      "Epoch (8/10)\n",
      "Accuracy: 0.8125 ; loss: 0.4167\n",
      "Validation Step :\n",
      "Accuracy: 0.8094 ; loss: 1.8229\n",
      "Accuracy Moy : 0.8110 ; loss Moy: 1.2187\n",
      "\n",
      "Epoch (9/10)\n",
      "Accuracy: 0.8123 ; loss: 0.4151\n",
      "Validation Step :\n",
      "Accuracy: 0.8095 ; loss: 1.8225\n",
      "Accuracy Moy : 0.8110 ; loss Moy: 1.2190\n",
      "\n",
      "Epoch (10/10)\n",
      "Accuracy: 0.8122 ; loss: 0.4142\n",
      "Validation Step :\n",
      "Accuracy: 0.8097 ; loss: 1.8226\n",
      "Accuracy Moy : 0.8110 ; loss Moy: 1.2193\n",
      "\n",
      "Scratch inference time :  1.2400921821594237  s\n",
      "Pruned inference time :  1.2265587091445922  s\n",
      "Epoch (1/75)\n",
      "Accuracy: 0.2489 ; loss: 2.0151\n",
      "Validation Step :\n",
      "Accuracy: 0.2589 ; loss: 2.1765\n",
      "Accuracy Moy : 0.2546 ; loss Moy: 1.8342\n",
      "\n",
      "Epoch (2/75)\n",
      "Accuracy: 0.3148 ; loss: 1.9415\n",
      "Validation Step :\n",
      "Accuracy: 0.3190 ; loss: 1.8633\n",
      "Accuracy Moy : 0.3170 ; loss Moy: 1.7155\n",
      "\n",
      "Epoch (3/75)\n",
      "Accuracy: 0.3551 ; loss: 1.8154\n",
      "Validation Step :\n",
      "Accuracy: 0.3581 ; loss: 1.6001\n",
      "Accuracy Moy : 0.3567 ; loss Moy: 1.6161\n",
      "\n",
      "Epoch (4/75)\n",
      "Accuracy: 0.3855 ; loss: 1.7650\n",
      "Validation Step :\n",
      "Accuracy: 0.3878 ; loss: 1.6686\n",
      "Accuracy Moy : 0.3867 ; loss Moy: 1.5290\n",
      "\n",
      "Epoch (5/75)\n",
      "Accuracy: 0.4106 ; loss: 1.8025\n",
      "Validation Step :\n",
      "Accuracy: 0.4133 ; loss: 1.7623\n",
      "Accuracy Moy : 0.4120 ; loss Moy: 1.3731\n",
      "\n",
      "Epoch (6/75)\n",
      "Accuracy: 0.4330 ; loss: 1.7485\n",
      "Validation Step :\n",
      "Accuracy: 0.4348 ; loss: 1.8084\n",
      "Accuracy Moy : 0.4339 ; loss Moy: 1.3629\n",
      "\n",
      "Epoch (7/75)\n",
      "Accuracy: 0.4517 ; loss: 1.8282\n",
      "Validation Step :\n",
      "Accuracy: 0.4538 ; loss: 1.5751\n",
      "Accuracy Moy : 0.4527 ; loss Moy: 1.2569\n",
      "\n",
      "Epoch (8/75)\n",
      "Accuracy: 0.4684 ; loss: 1.6960\n",
      "Validation Step :\n",
      "Accuracy: 0.4701 ; loss: 1.5638\n",
      "Accuracy Moy : 0.4693 ; loss Moy: 1.2334\n",
      "\n",
      "Epoch (9/75)\n",
      "Accuracy: 0.4829 ; loss: 1.5956\n",
      "Validation Step :\n",
      "Accuracy: 0.4846 ; loss: 1.5219\n",
      "Accuracy Moy : 0.4838 ; loss Moy: 1.1790\n",
      "\n",
      "Epoch (10/75)\n",
      "Accuracy: 0.4960 ; loss: 1.6313\n",
      "Validation Step :\n",
      "Accuracy: 0.4976 ; loss: 1.4915\n",
      "Accuracy Moy : 0.4968 ; loss Moy: 1.1362\n",
      "\n",
      "Epoch (11/75)\n",
      "Accuracy: 0.5078 ; loss: 1.5597\n",
      "Validation Step :\n",
      "Accuracy: 0.5092 ; loss: 1.4558\n",
      "Accuracy Moy : 0.5085 ; loss Moy: 1.0897\n",
      "\n",
      "Epoch (12/75)\n",
      "Accuracy: 0.5185 ; loss: 1.4943\n",
      "Validation Step :\n",
      "Accuracy: 0.5197 ; loss: 1.4735\n",
      "Accuracy Moy : 0.5191 ; loss Moy: 1.1110\n",
      "\n",
      "Epoch (13/75)\n",
      "Accuracy: 0.5279 ; loss: 1.4575\n",
      "Validation Step :\n",
      "Accuracy: 0.5289 ; loss: 1.4621\n",
      "Accuracy Moy : 0.5284 ; loss Moy: 1.1122\n",
      "\n",
      "Epoch (14/75)\n",
      "Accuracy: 0.5364 ; loss: 1.5157\n",
      "Validation Step :\n",
      "Accuracy: 0.5372 ; loss: 1.5618\n",
      "Accuracy Moy : 0.5368 ; loss Moy: 1.1131\n",
      "\n",
      "Epoch (15/75)\n",
      "Accuracy: 0.5441 ; loss: 1.3042\n",
      "Validation Step :\n",
      "Accuracy: 0.5447 ; loss: 1.5973\n",
      "Accuracy Moy : 0.5445 ; loss Moy: 1.1401\n",
      "\n",
      "Epoch (16/75)\n",
      "Accuracy: 0.5510 ; loss: 1.3694\n",
      "Validation Step :\n",
      "Accuracy: 0.5516 ; loss: 1.6398\n",
      "Accuracy Moy : 0.5513 ; loss Moy: 1.1244\n",
      "\n",
      "Epoch (17/75)\n",
      "Accuracy: 0.5575 ; loss: 1.2986\n",
      "Validation Step :\n",
      "Accuracy: 0.5580 ; loss: 1.6051\n",
      "Accuracy Moy : 0.5577 ; loss Moy: 1.1301\n",
      "\n",
      "Epoch (18/75)\n",
      "Accuracy: 0.5635 ; loss: 1.2714\n",
      "Validation Step :\n",
      "Accuracy: 0.5638 ; loss: 1.5014\n",
      "Accuracy Moy : 0.5637 ; loss Moy: 1.1190\n",
      "\n",
      "Epoch (19/75)\n",
      "Accuracy: 0.5690 ; loss: 1.3430\n",
      "Validation Step :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5693 ; loss: 1.4695\n",
      "Accuracy Moy : 0.5691 ; loss Moy: 1.1445\n",
      "\n",
      "Epoch (20/75)\n",
      "Accuracy: 0.5741 ; loss: 1.2848\n",
      "Validation Step :\n",
      "Accuracy: 0.5744 ; loss: 1.4347\n",
      "Accuracy Moy : 0.5743 ; loss Moy: 1.1173\n",
      "\n",
      "Epoch (21/75)\n",
      "Accuracy: 0.5790 ; loss: 1.3800\n",
      "Validation Step :\n",
      "Accuracy: 0.5793 ; loss: 1.4488\n",
      "Accuracy Moy : 0.5792 ; loss Moy: 1.1181\n",
      "\n",
      "Epoch (22/75)\n",
      "Accuracy: 0.5837 ; loss: 1.3712\n",
      "Validation Step :\n",
      "Accuracy: 0.5838 ; loss: 1.6658\n",
      "Accuracy Moy : 0.5838 ; loss Moy: 1.1489\n",
      "\n",
      "Epoch (23/75)\n",
      "Accuracy: 0.5880 ; loss: 1.1756\n",
      "Validation Step :\n",
      "Accuracy: 0.5881 ; loss: 1.6668\n",
      "Accuracy Moy : 0.5881 ; loss Moy: 1.1281\n",
      "\n",
      "Epoch (24/75)\n",
      "Accuracy: 0.5921 ; loss: 1.3311\n",
      "Validation Step :\n",
      "Accuracy: 0.5922 ; loss: 1.5640\n",
      "Accuracy Moy : 0.5921 ; loss Moy: 1.1439\n",
      "\n",
      "Epoch (25/75)\n",
      "Accuracy: 0.5960 ; loss: 1.2206\n",
      "Validation Step :\n",
      "Accuracy: 0.5960 ; loss: 1.7209\n",
      "Accuracy Moy : 0.5960 ; loss Moy: 1.1345\n",
      "\n",
      "Epoch (26/75)\n",
      "Accuracy: 0.5996 ; loss: 1.2450\n",
      "Validation Step :\n",
      "Accuracy: 0.5997 ; loss: 1.5820\n",
      "Accuracy Moy : 0.5997 ; loss Moy: 1.1083\n",
      "\n",
      "Epoch (27/75)\n",
      "Accuracy: 0.6032 ; loss: 1.1604\n",
      "Validation Step :\n",
      "Accuracy: 0.6032 ; loss: 1.6620\n",
      "Accuracy Moy : 0.6032 ; loss Moy: 1.1199\n",
      "\n",
      "Epoch (28/75)\n",
      "Accuracy: 0.6065 ; loss: 1.1000\n",
      "Validation Step :\n",
      "Accuracy: 0.6066 ; loss: 1.5239\n",
      "Accuracy Moy : 0.6065 ; loss Moy: 1.0976\n",
      "\n",
      "Epoch (29/75)\n",
      "Accuracy: 0.6096 ; loss: 1.1838\n",
      "Validation Step :\n",
      "Accuracy: 0.6097 ; loss: 1.6569\n",
      "Accuracy Moy : 0.6097 ; loss Moy: 1.0920\n",
      "\n",
      "Epoch (30/75)\n",
      "Accuracy: 0.6127 ; loss: 1.1692\n",
      "Validation Step :\n",
      "Accuracy: 0.6127 ; loss: 1.5679\n",
      "Accuracy Moy : 0.6127 ; loss Moy: 1.0924\n",
      "\n",
      "Epoch (31/75)\n",
      "Accuracy: 0.6156 ; loss: 1.1411\n",
      "Validation Step :\n",
      "Accuracy: 0.6156 ; loss: 1.5168\n",
      "Accuracy Moy : 0.6156 ; loss Moy: 1.1006\n",
      "\n",
      "Epoch (32/75)\n",
      "Accuracy: 0.6183 ; loss: 1.1849\n",
      "Validation Step :\n",
      "Accuracy: 0.6184 ; loss: 1.4550\n",
      "Accuracy Moy : 0.6184 ; loss Moy: 1.0788\n",
      "\n",
      "Epoch (33/75)\n",
      "Accuracy: 0.6210 ; loss: 1.1100\n",
      "Validation Step :\n",
      "Accuracy: 0.6211 ; loss: 1.5597\n",
      "Accuracy Moy : 0.6211 ; loss Moy: 1.0565\n",
      "\n",
      "Epoch (34/75)\n",
      "Accuracy: 0.6236 ; loss: 1.0720\n",
      "Validation Step :\n",
      "Accuracy: 0.6236 ; loss: 1.5944\n",
      "Accuracy Moy : 0.6236 ; loss Moy: 1.0839\n",
      "\n",
      "Epoch (35/75)\n",
      "Accuracy: 0.6261 ; loss: 1.1764\n",
      "Validation Step :\n",
      "Accuracy: 0.6261 ; loss: 1.4193\n",
      "Accuracy Moy : 0.6261 ; loss Moy: 1.0724\n",
      "\n",
      "Epoch (36/75)\n",
      "Accuracy: 0.6285 ; loss: 1.1795\n",
      "Validation Step :\n",
      "Accuracy: 0.6285 ; loss: 1.4994\n",
      "Accuracy Moy : 0.6285 ; loss Moy: 1.0740\n",
      "\n",
      "Epoch (37/75)\n",
      "Accuracy: 0.6309 ; loss: 1.0606\n",
      "Validation Step :\n",
      "Accuracy: 0.6308 ; loss: 1.4942\n",
      "Accuracy Moy : 0.6308 ; loss Moy: 1.0864\n",
      "\n",
      "Epoch (38/75)\n",
      "Accuracy: 0.6331 ; loss: 1.1893\n",
      "Validation Step :\n",
      "Accuracy: 0.6330 ; loss: 1.4402\n",
      "Accuracy Moy : 0.6331 ; loss Moy: 1.1114\n",
      "\n",
      "Epoch (39/75)\n",
      "Accuracy: 0.6352 ; loss: 1.1869\n",
      "Validation Step :\n",
      "Accuracy: 0.6352 ; loss: 1.4215\n",
      "Accuracy Moy : 0.6352 ; loss Moy: 1.1114\n",
      "\n",
      "Epoch (40/75)\n",
      "Accuracy: 0.6373 ; loss: 0.9503\n",
      "Validation Step :\n",
      "Accuracy: 0.6373 ; loss: 1.8486\n",
      "Accuracy Moy : 0.6373 ; loss Moy: 1.0779\n",
      "\n",
      "Epoch (41/75)\n",
      "Accuracy: 0.6393 ; loss: 1.0333\n",
      "Validation Step :\n",
      "Accuracy: 0.6393 ; loss: 1.6319\n",
      "Accuracy Moy : 0.6393 ; loss Moy: 1.0968\n",
      "\n",
      "Epoch (42/75)\n",
      "Accuracy: 0.6413 ; loss: 1.1761\n",
      "Validation Step :\n",
      "Accuracy: 0.6412 ; loss: 1.5122\n",
      "Accuracy Moy : 0.6413 ; loss Moy: 1.0913\n",
      "\n",
      "Epoch (43/75)\n",
      "Accuracy: 0.6431 ; loss: 1.0335\n",
      "Validation Step :\n",
      "Accuracy: 0.6431 ; loss: 1.5510\n",
      "Accuracy Moy : 0.6431 ; loss Moy: 1.0882\n",
      "\n",
      "Epoch (44/75)\n",
      "Accuracy: 0.6450 ; loss: 0.9089\n",
      "Validation Step :\n",
      "Accuracy: 0.6449 ; loss: 1.5725\n",
      "Accuracy Moy : 0.6449 ; loss Moy: 1.1221\n",
      "\n",
      "Epoch (45/75)\n",
      "Accuracy: 0.6467 ; loss: 0.9304\n",
      "Validation Step :\n",
      "Accuracy: 0.6467 ; loss: 1.4898\n",
      "Accuracy Moy : 0.6467 ; loss Moy: 1.1391\n",
      "\n",
      "Epoch (46/75)\n",
      "Accuracy: 0.6484 ; loss: 0.9613\n",
      "Validation Step :\n",
      "Accuracy: 0.6483 ; loss: 1.4272\n",
      "Accuracy Moy : 0.6484 ; loss Moy: 1.1556\n",
      "\n",
      "Epoch (47/75)\n",
      "Accuracy: 0.6500 ; loss: 0.9962\n",
      "Validation Step :\n",
      "Accuracy: 0.6500 ; loss: 1.5150\n",
      "Accuracy Moy : 0.6500 ; loss Moy: 1.1141\n",
      "\n",
      "Epoch (48/75)\n",
      "Accuracy: 0.6517 ; loss: 0.8740\n",
      "Validation Step :\n",
      "Accuracy: 0.6516 ; loss: 1.2494\n",
      "Accuracy Moy : 0.6516 ; loss Moy: 1.1011\n",
      "\n",
      "Epoch (49/75)\n",
      "Accuracy: 0.6532 ; loss: 0.9644\n",
      "Validation Step :\n",
      "Accuracy: 0.6531 ; loss: 1.5033\n",
      "Accuracy Moy : 0.6532 ; loss Moy: 1.1110\n",
      "\n",
      "Epoch (50/75)\n",
      "Accuracy: 0.6547 ; loss: 0.9186\n",
      "Validation Step :\n",
      "Accuracy: 0.6546 ; loss: 1.4054\n",
      "Accuracy Moy : 0.6547 ; loss Moy: 1.1129\n",
      "\n",
      "Epoch (51/75)\n",
      "Accuracy: 0.6562 ; loss: 0.9317\n",
      "Validation Step :\n",
      "Accuracy: 0.6561 ; loss: 1.5241\n",
      "Accuracy Moy : 0.6562 ; loss Moy: 1.0835\n",
      "\n",
      "Epoch (52/75)\n",
      "Accuracy: 0.6576 ; loss: 0.8186\n",
      "Validation Step :\n",
      "Accuracy: 0.6576 ; loss: 1.1608\n",
      "Accuracy Moy : 0.6576 ; loss Moy: 1.0771\n",
      "\n",
      "Epoch (53/75)\n",
      "Accuracy: 0.6590 ; loss: 0.9015\n",
      "Validation Step :\n",
      "Accuracy: 0.6590 ; loss: 1.3444\n",
      "Accuracy Moy : 0.6590 ; loss Moy: 1.0930\n",
      "\n",
      "Epoch (54/75)\n",
      "Accuracy: 0.6604 ; loss: 0.7967\n",
      "Validation Step :\n",
      "Accuracy: 0.6603 ; loss: 1.4143\n",
      "Accuracy Moy : 0.6604 ; loss Moy: 1.1135\n",
      "\n",
      "Epoch (55/75)\n",
      "Accuracy: 0.6617 ; loss: 0.8913\n",
      "Validation Step :\n",
      "Accuracy: 0.6616 ; loss: 1.5869\n",
      "Accuracy Moy : 0.6617 ; loss Moy: 1.1204\n",
      "\n",
      "Epoch (56/75)\n",
      "Accuracy: 0.6630 ; loss: 0.8286\n",
      "Validation Step :\n",
      "Accuracy: 0.6629 ; loss: 1.2704\n",
      "Accuracy Moy : 0.6630 ; loss Moy: 1.0957\n",
      "\n",
      "Epoch (57/75)\n",
      "Accuracy: 0.6643 ; loss: 0.7720\n",
      "Validation Step :\n",
      "Accuracy: 0.6642 ; loss: 1.3467\n",
      "Accuracy Moy : 0.6642 ; loss Moy: 1.1194\n",
      "\n",
      "Epoch (58/75)\n",
      "Accuracy: 0.6647 ; loss: 0.8203\r"
     ]
    }
   ],
   "source": [
    "for p in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    accuracy = np.array([])\n",
    "    val_accuracy = np.array([])\n",
    "\n",
    "    l = np.array([])\n",
    "    val_l = np.array([])\n",
    "\n",
    "\n",
    "    P = Pruning(tf.keras.models.clone_model(model), \n",
    "                pruning_factor =p)\n",
    "    lr = 1\n",
    "    for epoch in [75, 15, 10]:\n",
    "        # Paramètre d'entrainement\n",
    "        lr /= 10\n",
    "        P.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "                 loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metric = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        )\n",
    "\n",
    "        # Entrainement         \n",
    "        accur, loss, val_accur, val_loss = P.train(x_train, y_train, \n",
    "                                                   x_test, y_test, \n",
    "                                                   epochs = epoch, \n",
    "                                                   batch_size= 32 )  \n",
    "\n",
    "\n",
    "        # plot hist\n",
    "        accuracy = np.append(accuracy, accur)\n",
    "        val_accuracy = np.append(val_accuracy, val_accur)\n",
    "        l = np.append(l, loss)\n",
    "        val_l = np.append(val_l,val_loss)\n",
    "\n",
    "    \n",
    "    # Afficher les courbes d'entrainement\n",
    "    #plot_hist(f\"Lenet5_P_factor_{p}.png\")\n",
    "    \n",
    "    # inference time\n",
    "    pruned_inf_time = inference_time()\n",
    "    \n",
    "    # Enregister l'historique\n",
    "    dico[f\"P_factor_{p}_hist\"] = (accuracy, val_accuracy, l, val_l)\n",
    "    \n",
    "    # calcul du temps d'inférence\n",
    "    dico[f\"P_factor_{p}_inf_time\"] = pruned_inf_time\n",
    "\n",
    "    \n",
    "    # memory used\n",
    "    dico[f\"nb_params_p_factor_{p}\"] = count_parameters(P.model)\n",
    "    \n",
    "    # sauvegarder les poids\n",
    "    P.model.save_weights(f\"w_Resnet8_p_{p}.h5\")\n",
    "    \n",
    "    # Sauvegarder les données du dictionnaire\n",
    "    np.save(\"summary.npy\", dico)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c317b",
   "metadata": {},
   "source": [
    "## Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(dic, figname, scratch = False):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for p in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        # Train accuracy\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][0], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # Validation accuracy\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dico[f\"P_factor_{p}_hist\"][1], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # train loss\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][2], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # validation loss\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][3], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "    if scratch == True: \n",
    "        # Courbe scratch\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[\"scratch_hist\"][0], label = \"Scratch Train accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dic[\"scratch_hist\"][1], label = \"Scratch Val accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[\"scratch_hist\"][2], label = \"Scratch Train loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "\n",
    "\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[\"scratch_hist\"][3], label = \"Scratch Val loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    \n",
    "    plt.savefig(figname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df765246",
   "metadata": {},
   "outputs": [],
   "source": [
    "figname= f\"test_seed10.png\"\n",
    "eval_plot(dico, figname, scratch = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
