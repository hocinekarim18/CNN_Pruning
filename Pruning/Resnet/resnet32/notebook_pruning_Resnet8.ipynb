{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4976fd",
   "metadata": {},
   "source": [
    "## Bibilioth√®que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2da05cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pyplot as plt\n",
    "from resnet import resnet_v2\n",
    "import time\n",
    "%matplotlib inline\n",
    "seed = tf.random.set_seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c602222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(figname):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(accuracy, label = \"train accuracy\")\n",
    "    plt.plot(val_accuracy, label = \"validation accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(l, label = \"train loss\")\n",
    "    plt.plot(val_l, label = \"validation loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(figname)\n",
    "    plt.show()\n",
    "    \n",
    "def inference_time():\n",
    "    scratch = []\n",
    "    pruned = []\n",
    "    for i in range(10):\n",
    "        t1 = time.time()\n",
    "        pred1 = scratch_model(x_test)\n",
    "        t2 = time.time()\n",
    "        scratch.append(t2-t1)\n",
    "\n",
    "        # Pruned model\n",
    "        t3 = time.time()\n",
    "        pred2 = P.model(x_test)\n",
    "        t4 = time.time()\n",
    "        pruned.append(t4-t3)\n",
    "\n",
    "    # display\n",
    "    print(\"Scratch inference time : \", np.mean(scratch), \" s\")\n",
    "    print(\"Pruned inference time : \", np.mean(pruned), \" s\")\n",
    "    return np.mean(pruned)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    somme = 0\n",
    "    for l in model.trainable_variables:\n",
    "        somme += np.count_nonzero(l)\n",
    "    return somme\n",
    "\n",
    "\n",
    "def scratch_hist():   \n",
    "    loss = dico[\"scratch_hist\"][0].history[\"loss\"]\n",
    "    val_loss = dico[\"scratch_hist\"][0].history[\"val_loss\"]\n",
    "    accuracy = dico[\"scratch_hist\"][0].history[\"sparse_categorical_accuracy\"]\n",
    "    val_accuracy =  dico[\"scratch_hist\"][0].history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "    for i in range(len( dico[\"scratch_hist\"])):\n",
    "        if i !=0:\n",
    "            loss = np.append(loss, dico[\"scratch_hist\"][i].history[\"loss\"])\n",
    "            val_loss = np.append(val_loss, dico[\"scratch_hist\"][i].history[\"val_loss\"])\n",
    "            accuracy = np.append(accuracy, dico[\"scratch_hist\"][i].history[\"sparse_categorical_accuracy\"])\n",
    "            val_accuracy =  np.append(val_accuracy, dico[\"scratch_hist\"][i].history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "    dico[\"scratch_hist\"] = (accuracy, val_accuracy, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8cf7b",
   "metadata": {},
   "source": [
    "## Loading cifar10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2339918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "y_test shape: (10000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66b092",
   "metadata": {},
   "source": [
    "## Building Resnet8 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d168fdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 32, 32, 16)   448         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32, 32, 16)  64          ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 32, 32, 16)  64          ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 32, 32, 16)  64          ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 32, 32, 16)   0           ['activation_76[0][0]',          \n",
      "                                                                  'batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 32, 32, 16)   0           ['add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_78[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 32, 32, 16)  64          ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 32, 32, 16)  64          ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 32, 32, 16)   0           ['activation_78[0][0]',          \n",
      "                                                                  'batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 32, 32, 16)   0           ['add_37[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 32, 32, 16)  64          ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 32, 32, 16)  64          ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 32, 32, 16)   0           ['activation_80[0][0]',          \n",
      "                                                                  'batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 32, 32, 16)   0           ['add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 32, 32, 16)  64          ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 32, 32, 16)  64          ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 32, 32, 16)   0           ['activation_82[0][0]',          \n",
      "                                                                  'batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 32, 32, 16)   0           ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 32, 32, 16)  64          ['conv2d_93[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_85[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 32, 32, 16)  64          ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 32, 32, 16)   0           ['activation_84[0][0]',          \n",
      "                                                                  'batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 32, 32, 16)   0           ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 32, 32, 16)  64          ['conv2d_95[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_87[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 32, 32, 16)  64          ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 32, 32, 16)   0           ['activation_86[0][0]',          \n",
      "                                                                  'batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 32, 32, 16)   0           ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 32, 32, 16)  64          ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 32, 32, 16)  64          ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 32, 32, 16)   0           ['activation_88[0][0]',          \n",
      "                                                                  'batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 32, 32, 16)   0           ['add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 32, 32, 16)  64          ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 32, 32, 16)   2320        ['activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 32, 32, 16)  64          ['conv2d_100[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 32, 32, 16)   0           ['activation_90[0][0]',          \n",
      "                                                                  'batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 32, 32, 16)   0           ['add_43[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 16, 16, 32)   4640        ['activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 16, 16, 32)  128         ['conv2d_101[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 16, 16, 32)   544         ['activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 16, 16, 32)  128         ['conv2d_102[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 16, 16, 32)   0           ['conv2d_103[0][0]',             \n",
      "                                                                  'batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 16, 16, 32)   0           ['add_44[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_94[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 16, 16, 32)  128         ['conv2d_104[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_95[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16, 16, 32)  128         ['conv2d_105[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_45 (Add)                   (None, 16, 16, 32)   0           ['activation_94[0][0]',          \n",
      "                                                                  'batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 16, 16, 32)   0           ['add_45[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_96[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 16, 16, 32)  128         ['conv2d_106[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_97[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 16, 16, 32)  128         ['conv2d_107[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_46 (Add)                   (None, 16, 16, 32)   0           ['activation_96[0][0]',          \n",
      "                                                                  'batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 16, 16, 32)   0           ['add_46[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_98[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 16, 16, 32)  128         ['conv2d_108[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_99[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 16, 16, 32)  128         ['conv2d_109[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 16, 16, 32)   0           ['activation_98[0][0]',          \n",
      "                                                                  'batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 16, 16, 32)   0           ['add_47[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_100[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 16, 16, 32)  128         ['conv2d_110[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 16, 16, 32)   0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_101[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 16, 16, 32)  128         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, 16, 16, 32)   0           ['activation_100[0][0]',         \n",
      "                                                                  'batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 16, 16, 32)   0           ['add_48[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_102[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 16, 16, 32)  128         ['conv2d_112[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 16, 16, 32)   0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_103[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 16, 16, 32)  128         ['conv2d_113[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, 16, 16, 32)   0           ['activation_102[0][0]',         \n",
      "                                                                  'batch_normalization_104[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 16, 16, 32)   0           ['add_49[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_104[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 16, 16, 32)  128         ['conv2d_114[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 16, 16, 32)   0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_105[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 16, 16, 32)  128         ['conv2d_115[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_50 (Add)                   (None, 16, 16, 32)   0           ['activation_104[0][0]',         \n",
      "                                                                  'batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 16, 16, 32)   0           ['add_50[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_106[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 16, 16, 32)  128         ['conv2d_116[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 16, 16, 32)   0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_107[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 16, 16, 32)  128         ['conv2d_117[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_51 (Add)                   (None, 16, 16, 32)   0           ['activation_106[0][0]',         \n",
      "                                                                  'batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 16, 16, 32)   0           ['add_51[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)            (None, 8, 8, 64)     18496       ['activation_108[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 8, 8, 64)    256         ['conv2d_118[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_109[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)            (None, 8, 8, 64)     2112        ['activation_108[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 8, 8, 64)    256         ['conv2d_119[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 8, 8, 64)     0           ['conv2d_120[0][0]',             \n",
      "                                                                  'batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 8, 8, 64)     0           ['add_52[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_110[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 8, 8, 64)    256         ['conv2d_121[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_111[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 8, 8, 64)    256         ['conv2d_122[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 8, 8, 64)     0           ['activation_110[0][0]',         \n",
      "                                                                  'batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 8, 8, 64)     0           ['add_53[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_112[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_113 (Batch  (None, 8, 8, 64)    256         ['conv2d_123[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_113[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_113[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_114 (Batch  (None, 8, 8, 64)    256         ['conv2d_124[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 8, 8, 64)     0           ['activation_112[0][0]',         \n",
      "                                                                  'batch_normalization_114[0][0]']\n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 8, 8, 64)     0           ['add_54[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_114[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 8, 8, 64)    256         ['conv2d_125[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_115[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 8, 8, 64)    256         ['conv2d_126[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 8, 8, 64)     0           ['activation_114[0][0]',         \n",
      "                                                                  'batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 8, 8, 64)     0           ['add_55[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_116[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 8, 8, 64)    256         ['conv2d_127[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_117[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 8, 8, 64)    256         ['conv2d_128[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 8, 8, 64)     0           ['activation_116[0][0]',         \n",
      "                                                                  'batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 8, 8, 64)     0           ['add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_118[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 8, 8, 64)    256         ['conv2d_129[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_119 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_119[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 8, 8, 64)    256         ['conv2d_130[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 8, 8, 64)     0           ['activation_118[0][0]',         \n",
      "                                                                  'batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 8, 8, 64)     0           ['add_57[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_120[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 8, 8, 64)    256         ['conv2d_131[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_121[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 8, 8, 64)    256         ['conv2d_132[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 8, 8, 64)     0           ['activation_120[0][0]',         \n",
      "                                                                  'batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 8, 8, 64)     0           ['add_58[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_122[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 8, 8, 64)    256         ['conv2d_133[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_123[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 8, 8, 64)    256         ['conv2d_134[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 8, 8, 64)     0           ['activation_122[0][0]',         \n",
      "                                                                  'batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 8, 8, 64)     0           ['add_59[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (AveragePo  (None, 1, 1, 64)    0           ['activation_124[0][0]']         \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 64)           0           ['average_pooling2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 10)           650         ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 763,882\n",
      "Trainable params: 760,266\n",
      "Non-trainable params: 3,616\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet_v2((32, 32, 3), depth = 50)\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035728c6",
   "metadata": {},
   "source": [
    "## Scratch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b271752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disctionnaire pour enregistrer les infos pertinentes\n",
    "dico = {}\n",
    "scratch_model = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dea2a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02397941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 12:47:17.915797: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 95s 60ms/step - loss: 1.5433 - sparse_categorical_accuracy: 0.4535 - val_loss: 1.7352 - val_sparse_categorical_accuracy: 0.4102\n",
      "Epoch 2/75\n",
      "1289/1562 [=======================>......] - ETA: 16s - loss: 1.1944 - sparse_categorical_accuracy: 0.5948"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m scratch_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      5\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[1;32m      6\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()],\n\u001b[1;32m      7\u001b[0m         loss\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m         )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Train and evaluate on data.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mscratch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m scratch_model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[1;32m     21\u001b[0m dico[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscratch_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(hist)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dico[\"scratch_hist\"] = []\n",
    "for EPOCHS in [75,15,10]:\n",
    "    lr /= 10\n",
    "    scratch_model.compile(\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "            loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            )\n",
    "\n",
    "        # Train and evaluate on data.\n",
    "    hist = scratch_model.fit(x_train, y_train, \n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          steps_per_epoch = len(x_train)/BATCH_SIZE,\n",
    "          validation_data =(x_test, y_test),\n",
    "          workers =40,\n",
    "          use_multiprocessing= True,\n",
    "          )\n",
    "\n",
    "    scratch_model.evaluate(x_test, y_test)\n",
    "    dico[\"scratch_hist\"].append(hist)\n",
    "scratch_hist()\n",
    "\n",
    "np.save(\"summary.npy\", dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3c767",
   "metadata": {},
   "source": [
    "## Pruning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "565846c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruning:\n",
    "    def __init__(self, model, pruning_factor = 0.5):\n",
    "        \n",
    "        # attributs li√©s au model\n",
    "        self.model = model\n",
    "        self.pruning_factor = pruning_factor\n",
    "    \n",
    "    # Tensorflow utils setting\n",
    "    def compile(self,optimizer, loss_fn, metric):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_metric = metric\n",
    "    \n",
    "    # Pruning Function\n",
    "    def pruning(self, P_factor = 0.5):\n",
    "        if P_factor >=1 or P_factor <= 0:\n",
    "            raise ValueError (\"Pruning factor value Error : Pruning factor value should be ]0 ;1[\")\n",
    "        for layer in self.model.layers:\n",
    "            if \"conv\" in layer.name:\n",
    "                \n",
    "                # R√©cuper les kernels\n",
    "                w = layer.get_weights()[0]\n",
    "                b = layer.get_weights()[1]\n",
    "                \n",
    "                # Calcul du filtre contenant la median\n",
    "                tab = []\n",
    "                for i in range(w.shape[-1]):\n",
    "                    somme = 0\n",
    "                    for j in range(w.shape[-1]):\n",
    "                        if i !=j:\n",
    "                            somme += np.linalg.norm(w[:,:,:,i] - w[:,:,:,j])\n",
    "                    tab.append(somme)\n",
    "                    \n",
    "                # calcul du nombre de filtrer a annuler selon le facteur de pruning\n",
    "                nb_pruned_filters = int(w.shape[-1]*P_factor)\n",
    "                \n",
    "                for i in range(nb_pruned_filters):\n",
    "                    # r√©cup√©rer l'indice du minimum\n",
    "                    ind_min = np.argmin(tab)\n",
    "                    \n",
    "                    #anuuler le filtre qui minimise la formule pr√©cedente\n",
    "                    w[:, :, :, ind_min] = np.zeros(w[:, :, :, ind_min].shape)\n",
    "                    \n",
    "                    # astuce pour d√©placer le minimum lorsque il faut annuler plusieurs filtres\n",
    "                    tab[ind_min] = 1e5\n",
    "                \n",
    "                layer.set_weights([w, b])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    #Training algorithm\n",
    "    def train(self,x_train, y_train, val_data, val_labels, epochs = 100, batch_size= 32):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # training history storage\n",
    "        accur = []\n",
    "        L = []\n",
    "        \n",
    "        # validation history storage\n",
    "        v_accur = []\n",
    "        v_loss = []\n",
    "\n",
    "        if x_train.shape[0] % batch_size == 0:\n",
    "            nb_train_steps = x_train.shape[0] // batch_size\n",
    "        else:\n",
    "            nb_train_steps = (x_train.shape[0] // batch_size) + 1\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch ({epoch +1 }/{epochs})\")\n",
    "            for i in range(nb_train_steps):\n",
    "                # Batching data\n",
    "                x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                x = tf.constant(x)\n",
    "                y = tf.constant(y)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    predictions = self.model(x, training=True)\n",
    "                    # calcul de la loss\n",
    "                    loss = self.loss_fn(y, predictions)\n",
    "        \n",
    "                    \n",
    "                # Calcul du gradient\n",
    "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "                \n",
    "                # Decente de gradient\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                \n",
    "                #Pruning step\n",
    "                self.pruning(P_factor = self.pruning_factor)\n",
    "                \n",
    "                # Update training metric.\n",
    "                self.acc_metric.update_state(y, predictions)\n",
    "                train_acc = self.acc_metric.result()\n",
    "                \n",
    "                print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(train_acc),  loss), end='\\r')\n",
    "            print(\"\\nValidation Step :\")\n",
    "            \n",
    "            # Validation step\n",
    "            val_accur, val_loss = self.test(val_data, val_labels)\n",
    "                \n",
    "            accur.append(float(train_acc))\n",
    "            L.append(loss)\n",
    "            \n",
    "            v_accur.append(val_accur)\n",
    "            v_loss.append(val_loss)\n",
    "            print(\"\")\n",
    "        return (accur, L, v_accur, v_loss)  \n",
    "\n",
    "    \n",
    "    # Test Step \n",
    "    def test(self,data, labels):\n",
    "        accur = []\n",
    "        l = []\n",
    "        if data.shape[0] % self.batch_size == 0:\n",
    "            nb_test_steps = data.shape[0] // self.batch_size\n",
    "        else:\n",
    "            nb_test_steps = (data.shape[0] // self.batch_size) + 1\n",
    "            \n",
    "        for i in range(nb_test_steps):\n",
    "            # Batching data\n",
    "            x = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            y = labels[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            \n",
    "            x = tf.constant(x)\n",
    "            y = tf.constant(y)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = self.model(x)\n",
    "\n",
    "            # calcul de la loss\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "            # calcul de l'accuracy\n",
    "            self.acc_metric.update_state(y, predictions)\n",
    "            test_acc = self.acc_metric.result()\n",
    "            print(\"Accuracy: {:.4f} ; loss: {:}\".format(float(test_acc),  loss), end='\\r')\n",
    "                \n",
    "            accur.append(float(test_acc))\n",
    "            l.append(float(loss))\n",
    "        print(\"\")        \n",
    "        print(\"Accuracy Moy : {:.4f} ; loss Moy: {:.4f}\" .format(np.mean(accur), np.mean(l) ))\n",
    "       \n",
    "        return (np.mean(accur), np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5752f",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d5d9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/1)\n",
      "Accuracy: 0.0759 ; loss: nan4.0366\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m P\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[1;32m     16\u001b[0m          loss_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m          metric \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(),\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Entrainement         \u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m accur, loss, val_accur, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# plot hist\u001b[39;00m\n\u001b[1;32m     28\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(accuracy, accur)\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mPruning.train\u001b[0;34m(self, x_train, y_train, val_data, val_labels, epochs, batch_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y, predictions)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Calcul du gradient\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Decente de gradient\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_weights))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1100\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1094\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1095\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1096\u001b[0m           output_gradients))\n\u001b[1;32m   1097\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1098\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1100\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1109\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/nn_grad.py:577\u001b[0m, in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    568\u001b[0m shape_0, shape_1 \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape_n([op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# in Eager mode.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 577\u001b[0m     \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_backprop_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    587\u001b[0m     gen_nn_ops\u001b[38;5;241m.\u001b[39mconv2d_backprop_filter(\n\u001b[1;32m    588\u001b[0m         op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    589\u001b[0m         shape_1,\n\u001b[1;32m    590\u001b[0m         grad,\n\u001b[1;32m    591\u001b[0m         dilations\u001b[38;5;241m=\u001b[39mdilations,\n\u001b[1;32m    592\u001b[0m         strides\u001b[38;5;241m=\u001b[39mstrides,\n\u001b[1;32m    593\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    594\u001b[0m         explicit_paddings\u001b[38;5;241m=\u001b[39mexplicit_paddings,\n\u001b[1;32m    595\u001b[0m         use_cudnn_on_gpu\u001b[38;5;241m=\u001b[39muse_cudnn_on_gpu,\n\u001b[1;32m    596\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mdata_format)\n\u001b[1;32m    597\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:1240\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   1239\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConv2DBackpropInput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_backprop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrides\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_cudnn_on_gpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpadding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexplicit_paddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdilations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   1246\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in [0.2, 0.7, 0.8, 0.9]:\n",
    "    accuracy = np.array([])\n",
    "    val_accuracy = np.array([])\n",
    "\n",
    "    l = np.array([])\n",
    "    val_l = np.array([])\n",
    "\n",
    "\n",
    "    P = Pruning(tf.keras.models.clone_model(model), \n",
    "                pruning_factor =p)\n",
    "    lr = 1\n",
    "    for epoch in [1, 1, 1]:\n",
    "        # Param√®tre d'entrainement\n",
    "        lr /= 10\n",
    "        P.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "                 loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metric = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        )\n",
    "\n",
    "        # Entrainement         \n",
    "        accur, loss, val_accur, val_loss = P.train(x_train, y_train, \n",
    "                                                   x_test, y_test, \n",
    "                                                   epochs = epoch, \n",
    "                                                   batch_size= 32 )  \n",
    "\n",
    "\n",
    "        # plot hist\n",
    "        accuracy = np.append(accuracy, accur)\n",
    "        val_accuracy = np.append(val_accuracy, val_accur)\n",
    "        l = np.append(l, loss)\n",
    "        val_l = np.append(val_l,val_loss)\n",
    "\n",
    "    \n",
    "    # Afficher les courbes d'entrainement\n",
    "    #plot_hist(f\"Lenet5_P_factor_{p}.png\")\n",
    "    \n",
    "    # inference time\n",
    "    pruned_inf_time = inference_time()\n",
    "    \n",
    "    # Enregister l'historique\n",
    "    dico[f\"P_factor_{p}_hist\"] = (accuracy, val_accuracy, l, val_l)\n",
    "    \n",
    "    # calcul du temps d'inf√©rence\n",
    "    dico[f\"P_factor_{p}_inf_time\"] = pruned_inf_time\n",
    "\n",
    "    \n",
    "    # memory used\n",
    "    dico[f\"nb_params_p_factor_{p}\"] = count_parameters(P.model)\n",
    "    \n",
    "    # sauvegarder les poids\n",
    "    P.model.save_weights(f\"w_Resnet8_p_{p}.h5\")\n",
    "    \n",
    "    # Sauvegarder les donn√©es du dictionnaire\n",
    "    np.save(\"summary.npy\", dico)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c317b",
   "metadata": {},
   "source": [
    "## Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9839b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(dic, figname, scratch = False):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for p in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        # Train accuracy\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][0], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # Validation accuracy\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dico[f\"P_factor_{p}_hist\"][1], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # train loss\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][2], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # validation loss\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][3], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "    if scratch == True: \n",
    "        # Courbe scratch\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[\"scratch_hist\"][0], label = \"Scratch Train accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dic[\"scratch_hist\"][1], label = \"Scratch Val accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[\"scratch_hist\"][2], label = \"Scratch Train loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "\n",
    "\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[\"scratch_hist\"][3], label = \"Scratch Val loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    \n",
    "    plt.savefig(figname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df765246",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m figname\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_seed10.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m dico \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_resnet32.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43meval_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdico\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36meval_plot\u001b[0;34m(dic, figname, scratch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validation accuracy\u001b[39;00m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m222\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mdico\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mP_factor_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_hist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid()\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figname= f\"test_seed10.png\"\n",
    "dico = np.load(\"summary_resnet32.npy\", allow_pickle=True)\n",
    "eval_plot(dico.item(), figname, scratch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78db32fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.09998   , 0.09999091, 0.09999412, 0.09999565, 0.09999655,\n",
       "        0.09999714, 0.09999756, 0.09999787, 0.09999812, 0.0999983 ,\n",
       "        0.09999846, 0.09999859, 0.0999987 , 0.09999879, 0.09999888,\n",
       "        0.09999895, 0.09999901, 0.09999906, 0.09999911, 0.09999916,\n",
       "        0.0999992 , 0.09999923, 0.09999927, 0.0999993 , 0.09999933,\n",
       "        0.09999935, 0.09999938, 0.0999994 , 0.09999942, 0.09999944,\n",
       "        0.09999946, 0.09999948, 0.09999949, 0.09999951, 0.09999952,\n",
       "        0.09999953, 0.09999955, 0.09999956, 0.09999957, 0.09999958,\n",
       "        0.09999959, 0.0999996 , 0.09999961, 0.09999962, 0.09999963,\n",
       "        0.09999964, 0.09999964, 0.09999965, 0.09999966, 0.09999967,\n",
       "        0.09999967, 0.09999968, 0.09999968, 0.09999969, 0.0999997 ,\n",
       "        0.0999997 , 0.0999997 , 0.09999971, 0.09999972, 0.09999972,\n",
       "        0.09999973, 0.09999973, 0.09999973, 0.09999974, 0.09999974,\n",
       "        0.09999975, 0.09999975, 0.09999976, 0.09999976, 0.09999976,\n",
       "        0.09999976, 0.09999977, 0.09999977, 0.09999978, 0.09999978,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ]),\n",
       " array([0.09993445, 0.0999683 , 0.09997909, 0.0999844 , 0.09998756,\n",
       "        0.09998966, 0.09999115, 0.09999226, 0.09999313, 0.09999382,\n",
       "        0.09999438, 0.09999485, 0.09999525, 0.09999559, 0.09999589,\n",
       "        0.09999615, 0.09999637, 0.09999658, 0.09999676, 0.09999692,\n",
       "        0.09999707, 0.0999972 , 0.09999732, 0.09999743, 0.09999754,\n",
       "        0.09999763, 0.09999772, 0.0999978 , 0.09999788, 0.09999795,\n",
       "        0.09999801, 0.09999808, 0.09999814, 0.09999819, 0.09999824,\n",
       "        0.09999829, 0.09999834, 0.09999838, 0.09999842, 0.09999846,\n",
       "        0.0999985 , 0.09999854, 0.09999857, 0.0999986 , 0.09999863,\n",
       "        0.09999866, 0.09999869, 0.09999872, 0.09999874, 0.09999877,\n",
       "        0.09999879, 0.09999882, 0.09999884, 0.09999886, 0.09999888,\n",
       "        0.0999989 , 0.09999892, 0.09999894, 0.09999896, 0.09999898,\n",
       "        0.09999899, 0.09999901, 0.09999902, 0.09999904, 0.09999905,\n",
       "        0.09999907, 0.09999908, 0.0999991 , 0.09999911, 0.09999912,\n",
       "        0.09999913, 0.09999915, 0.09999916, 0.09999917, 0.09999918,\n",
       "        0.09995267, 0.099977  , 0.09998481, 0.09998866, 0.09999095,\n",
       "        0.09999247, 0.09999356, 0.09999437, 0.099995  , 0.0999955 ,\n",
       "        0.09999591, 0.09999625, 0.09999654, 0.09999679, 0.09999701,\n",
       "        0.09995267, 0.099977  , 0.09998481, 0.09998866, 0.09999095,\n",
       "        0.09999247, 0.09999356, 0.09999437, 0.099995  , 0.0999955 ]),\n",
       " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan]),\n",
       " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico.item().get('P_factor_0.6_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb947d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
