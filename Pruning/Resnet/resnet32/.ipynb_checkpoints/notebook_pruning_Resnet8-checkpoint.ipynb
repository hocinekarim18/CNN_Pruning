{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4976fd",
   "metadata": {},
   "source": [
    "## Bibiliothèque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da05cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 11:21:27.870940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-12 11:21:27.870971: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_flops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resnet_v2\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_flops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_flops\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_flops'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pyplot as plt\n",
    "from resnet import resnet_v2\n",
    "from keras_flops import get_flops\n",
    "import time\n",
    "%matplotlib inline\n",
    "seed = tf.random.set_seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c602222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(figname):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(accuracy, label = \"train accuracy\")\n",
    "    plt.plot(val_accuracy, label = \"validation accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(l, label = \"train loss\")\n",
    "    plt.plot(val_l, label = \"validation loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(figname)\n",
    "    plt.show()\n",
    "    \n",
    "def inference_time():\n",
    "    scratch = []\n",
    "    pruned = []\n",
    "    for i in range(10):\n",
    "        t1 = time.time()\n",
    "        pred1 = scratch_model(x_test)\n",
    "        t2 = time.time()\n",
    "        scratch.append(t2-t1)\n",
    "\n",
    "        # Pruned model\n",
    "        t3 = time.time()\n",
    "        pred2 = P.model(x_test)\n",
    "        t4 = time.time()\n",
    "        pruned.append(t4-t3)\n",
    "\n",
    "    # display\n",
    "    print(\"Scratch inference time : \", np.mean(scratch), \" s\")\n",
    "    print(\"Pruned inference time : \", np.mean(pruned), \" s\")\n",
    "    return np.mean(pruned)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    somme = 0\n",
    "    for l in model.trainable_variables:\n",
    "        somme += np.count_nonzero(l)\n",
    "    return somme\n",
    "\n",
    "\n",
    "def scratch_hist():   \n",
    "    loss = dico[\"scratch_hist\"][0].history[\"loss\"]\n",
    "    val_loss = dico[\"scratch_hist\"][0].history[\"val_loss\"]\n",
    "    accuracy = dico[\"scratch_hist\"][0].history[\"sparse_categorical_accuracy\"]\n",
    "    val_accuracy =  dico[\"scratch_hist\"][0].history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "    for i in range(len( dico[\"scratch_hist\"])):\n",
    "        if i !=0:\n",
    "            loss = np.append(loss, dico[\"scratch_hist\"][i].history[\"loss\"])\n",
    "            val_loss = np.append(val_loss, dico[\"scratch_hist\"][i].history[\"val_loss\"])\n",
    "            accuracy = np.append(accuracy, dico[\"scratch_hist\"][i].history[\"sparse_categorical_accuracy\"])\n",
    "            val_accuracy =  np.append(val_accuracy, dico[\"scratch_hist\"][i].history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "    dico[\"scratch_hist\"] = (accuracy, val_accuracy, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8cf7b",
   "metadata": {},
   "source": [
    "## Loading cifar10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2339918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "y_test shape: (10000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66b092",
   "metadata": {},
   "source": [
    "## Building Resnet8 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d168fdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 10:23:15.637190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-27 10:23:15.637246: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-27 10:23:15.637278: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big26): /proc/driver/nvidia/version does not exist\n",
      "2022-06-27 10:23:15.639084: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 16, 16, 32)   544         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 32)   0           ['conv2d_5[0][0]',               \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 16, 32)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 8, 8, 64)     18496       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 8, 8, 64)     0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 8, 8, 64)     36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 8, 8, 64)     2112        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 8, 8, 64)     0           ['conv2d_8[0][0]',               \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 8, 8, 64)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_6[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78,666\n",
      "Trainable params: 78,186\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet_v2((32, 32, 3), depth = 8)\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035728c6",
   "metadata": {},
   "source": [
    "## Scratch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b271752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disctionnaire pour enregistrer les infos pertinentes\n",
    "dico = {}\n",
    "scratch_model = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea2a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02397941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "1562/1562 [==============================] - 63s 39ms/step - loss: 1.5363 - sparse_categorical_accuracy: 0.4602 - val_loss: 2.0577 - val_sparse_categorical_accuracy: 0.3937\n",
      "Epoch 2/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 1.1897 - sparse_categorical_accuracy: 0.5954 - val_loss: 1.3060 - val_sparse_categorical_accuracy: 0.5613\n",
      "Epoch 3/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 1.0571 - sparse_categorical_accuracy: 0.6488 - val_loss: 1.1325 - val_sparse_categorical_accuracy: 0.6206\n",
      "Epoch 4/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.9693 - sparse_categorical_accuracy: 0.6791 - val_loss: 1.2540 - val_sparse_categorical_accuracy: 0.5710\n",
      "Epoch 5/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.9060 - sparse_categorical_accuracy: 0.7062 - val_loss: 0.9671 - val_sparse_categorical_accuracy: 0.6794\n",
      "Epoch 6/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.8507 - sparse_categorical_accuracy: 0.7262 - val_loss: 1.1056 - val_sparse_categorical_accuracy: 0.6372\n",
      "Epoch 7/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.8061 - sparse_categorical_accuracy: 0.7426 - val_loss: 1.1592 - val_sparse_categorical_accuracy: 0.6229\n",
      "Epoch 8/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.7662 - sparse_categorical_accuracy: 0.7585 - val_loss: 0.9555 - val_sparse_categorical_accuracy: 0.6909\n",
      "Epoch 9/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.7314 - sparse_categorical_accuracy: 0.7725 - val_loss: 1.3233 - val_sparse_categorical_accuracy: 0.5968\n",
      "Epoch 10/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.6979 - sparse_categorical_accuracy: 0.7832 - val_loss: 1.1957 - val_sparse_categorical_accuracy: 0.6208\n",
      "Epoch 11/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.6743 - sparse_categorical_accuracy: 0.7942 - val_loss: 0.9702 - val_sparse_categorical_accuracy: 0.6989\n",
      "Epoch 12/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.6482 - sparse_categorical_accuracy: 0.8048 - val_loss: 1.3713 - val_sparse_categorical_accuracy: 0.6147\n",
      "Epoch 13/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.6270 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.9153 - val_sparse_categorical_accuracy: 0.7258\n",
      "Epoch 14/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.6064 - sparse_categorical_accuracy: 0.8217 - val_loss: 0.8032 - val_sparse_categorical_accuracy: 0.7605\n",
      "Epoch 15/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.5850 - sparse_categorical_accuracy: 0.8295 - val_loss: 0.9036 - val_sparse_categorical_accuracy: 0.7365\n",
      "Epoch 16/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.5695 - sparse_categorical_accuracy: 0.8358 - val_loss: 1.2877 - val_sparse_categorical_accuracy: 0.6483\n",
      "Epoch 17/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.8431 - val_loss: 0.8508 - val_sparse_categorical_accuracy: 0.7444\n",
      "Epoch 18/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.5401 - sparse_categorical_accuracy: 0.8489 - val_loss: 0.8505 - val_sparse_categorical_accuracy: 0.7448\n",
      "Epoch 19/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.5281 - sparse_categorical_accuracy: 0.8528 - val_loss: 1.1519 - val_sparse_categorical_accuracy: 0.6696\n",
      "Epoch 20/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.5109 - sparse_categorical_accuracy: 0.8599 - val_loss: 1.1136 - val_sparse_categorical_accuracy: 0.6904\n",
      "Epoch 21/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.5026 - sparse_categorical_accuracy: 0.8639 - val_loss: 0.9133 - val_sparse_categorical_accuracy: 0.7532\n",
      "Epoch 22/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4896 - sparse_categorical_accuracy: 0.8691 - val_loss: 1.0101 - val_sparse_categorical_accuracy: 0.7183\n",
      "Epoch 23/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4848 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.9021 - val_sparse_categorical_accuracy: 0.7498\n",
      "Epoch 24/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4726 - sparse_categorical_accuracy: 0.8769 - val_loss: 1.3279 - val_sparse_categorical_accuracy: 0.6664\n",
      "Epoch 25/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4638 - sparse_categorical_accuracy: 0.8803 - val_loss: 1.1202 - val_sparse_categorical_accuracy: 0.7163\n",
      "Epoch 26/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4601 - sparse_categorical_accuracy: 0.8814 - val_loss: 0.9610 - val_sparse_categorical_accuracy: 0.7390\n",
      "Epoch 27/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4514 - sparse_categorical_accuracy: 0.8853 - val_loss: 1.1661 - val_sparse_categorical_accuracy: 0.6983\n",
      "Epoch 28/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4451 - sparse_categorical_accuracy: 0.8904 - val_loss: 1.2535 - val_sparse_categorical_accuracy: 0.6936\n",
      "Epoch 29/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4375 - sparse_categorical_accuracy: 0.8924 - val_loss: 1.0324 - val_sparse_categorical_accuracy: 0.7287\n",
      "Epoch 30/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4309 - sparse_categorical_accuracy: 0.8947 - val_loss: 0.9351 - val_sparse_categorical_accuracy: 0.7507\n",
      "Epoch 31/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4282 - sparse_categorical_accuracy: 0.8978 - val_loss: 1.2369 - val_sparse_categorical_accuracy: 0.6953\n",
      "Epoch 32/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4208 - sparse_categorical_accuracy: 0.9002 - val_loss: 0.9671 - val_sparse_categorical_accuracy: 0.7409\n",
      "Epoch 33/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4178 - sparse_categorical_accuracy: 0.9004 - val_loss: 1.0890 - val_sparse_categorical_accuracy: 0.7269\n",
      "Epoch 34/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.4085 - sparse_categorical_accuracy: 0.9067 - val_loss: 1.6069 - val_sparse_categorical_accuracy: 0.6584\n",
      "Epoch 35/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4070 - sparse_categorical_accuracy: 0.9063 - val_loss: 1.2088 - val_sparse_categorical_accuracy: 0.7053\n",
      "Epoch 36/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.4016 - sparse_categorical_accuracy: 0.9098 - val_loss: 1.2353 - val_sparse_categorical_accuracy: 0.7164\n",
      "Epoch 37/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3963 - sparse_categorical_accuracy: 0.9115 - val_loss: 1.2090 - val_sparse_categorical_accuracy: 0.7151\n",
      "Epoch 38/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3958 - sparse_categorical_accuracy: 0.9131 - val_loss: 1.5415 - val_sparse_categorical_accuracy: 0.6444\n",
      "Epoch 39/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3897 - sparse_categorical_accuracy: 0.9149 - val_loss: 1.3660 - val_sparse_categorical_accuracy: 0.6823\n",
      "Epoch 40/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3883 - sparse_categorical_accuracy: 0.9176 - val_loss: 1.5261 - val_sparse_categorical_accuracy: 0.6677\n",
      "Epoch 41/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3842 - sparse_categorical_accuracy: 0.9198 - val_loss: 1.1383 - val_sparse_categorical_accuracy: 0.7320\n",
      "Epoch 42/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.9202 - val_loss: 1.3477 - val_sparse_categorical_accuracy: 0.6880\n",
      "Epoch 43/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3749 - sparse_categorical_accuracy: 0.9233 - val_loss: 1.3448 - val_sparse_categorical_accuracy: 0.6571\n",
      "Epoch 44/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3785 - sparse_categorical_accuracy: 0.9213 - val_loss: 1.3606 - val_sparse_categorical_accuracy: 0.6916\n",
      "Epoch 45/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3776 - sparse_categorical_accuracy: 0.9216 - val_loss: 1.4395 - val_sparse_categorical_accuracy: 0.6696\n",
      "Epoch 46/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3704 - sparse_categorical_accuracy: 0.9268 - val_loss: 1.2342 - val_sparse_categorical_accuracy: 0.7069\n",
      "Epoch 47/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.9271 - val_loss: 1.2707 - val_sparse_categorical_accuracy: 0.7272\n",
      "Epoch 48/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3706 - sparse_categorical_accuracy: 0.9269 - val_loss: 1.4567 - val_sparse_categorical_accuracy: 0.6806\n",
      "Epoch 49/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3661 - sparse_categorical_accuracy: 0.9288 - val_loss: 1.3716 - val_sparse_categorical_accuracy: 0.6776\n",
      "Epoch 50/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3732 - sparse_categorical_accuracy: 0.9253 - val_loss: 1.5523 - val_sparse_categorical_accuracy: 0.6650\n",
      "Epoch 51/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3637 - sparse_categorical_accuracy: 0.9308 - val_loss: 1.1848 - val_sparse_categorical_accuracy: 0.7365\n",
      "Epoch 52/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3573 - sparse_categorical_accuracy: 0.9337 - val_loss: 1.2002 - val_sparse_categorical_accuracy: 0.7241\n",
      "Epoch 53/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3650 - sparse_categorical_accuracy: 0.9301 - val_loss: 1.3802 - val_sparse_categorical_accuracy: 0.7156\n",
      "Epoch 54/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3575 - sparse_categorical_accuracy: 0.9332 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.7378\n",
      "Epoch 55/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3563 - sparse_categorical_accuracy: 0.9352 - val_loss: 1.5673 - val_sparse_categorical_accuracy: 0.6806\n",
      "Epoch 56/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3589 - sparse_categorical_accuracy: 0.9341 - val_loss: 1.4319 - val_sparse_categorical_accuracy: 0.7082\n",
      "Epoch 57/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3577 - sparse_categorical_accuracy: 0.9339 - val_loss: 1.1089 - val_sparse_categorical_accuracy: 0.7528\n",
      "Epoch 58/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3518 - sparse_categorical_accuracy: 0.9371 - val_loss: 1.2288 - val_sparse_categorical_accuracy: 0.7377\n",
      "Epoch 59/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3553 - sparse_categorical_accuracy: 0.9355 - val_loss: 1.1786 - val_sparse_categorical_accuracy: 0.7309\n",
      "Epoch 60/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3512 - sparse_categorical_accuracy: 0.9377 - val_loss: 1.2435 - val_sparse_categorical_accuracy: 0.7192\n",
      "Epoch 61/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3435 - sparse_categorical_accuracy: 0.9419 - val_loss: 1.4676 - val_sparse_categorical_accuracy: 0.6829\n",
      "Epoch 62/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3581 - sparse_categorical_accuracy: 0.9350 - val_loss: 1.7929 - val_sparse_categorical_accuracy: 0.6359\n",
      "Epoch 63/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3547 - sparse_categorical_accuracy: 0.9365 - val_loss: 1.4266 - val_sparse_categorical_accuracy: 0.7074\n",
      "Epoch 64/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3513 - sparse_categorical_accuracy: 0.9387 - val_loss: 1.2479 - val_sparse_categorical_accuracy: 0.7305\n",
      "Epoch 65/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3495 - sparse_categorical_accuracy: 0.9391 - val_loss: 1.1623 - val_sparse_categorical_accuracy: 0.7532\n",
      "Epoch 66/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3509 - sparse_categorical_accuracy: 0.9386 - val_loss: 1.5223 - val_sparse_categorical_accuracy: 0.6957\n",
      "Epoch 67/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3520 - sparse_categorical_accuracy: 0.9382 - val_loss: 1.6007 - val_sparse_categorical_accuracy: 0.6721\n",
      "Epoch 68/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3505 - sparse_categorical_accuracy: 0.9389 - val_loss: 1.6796 - val_sparse_categorical_accuracy: 0.6563\n",
      "Epoch 69/75\n",
      "1562/1562 [==============================] - 60s 38ms/step - loss: 0.3477 - sparse_categorical_accuracy: 0.9395 - val_loss: 3.2540 - val_sparse_categorical_accuracy: 0.5373\n",
      "Epoch 70/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3423 - sparse_categorical_accuracy: 0.9431 - val_loss: 1.7722 - val_sparse_categorical_accuracy: 0.6510\n",
      "Epoch 71/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3494 - sparse_categorical_accuracy: 0.9393 - val_loss: 1.4218 - val_sparse_categorical_accuracy: 0.6984\n",
      "Epoch 72/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3379 - sparse_categorical_accuracy: 0.9448 - val_loss: 1.4849 - val_sparse_categorical_accuracy: 0.7009\n",
      "Epoch 73/75\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.3557 - sparse_categorical_accuracy: 0.9385 - val_loss: 1.3234 - val_sparse_categorical_accuracy: 0.7337\n",
      "Epoch 74/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3429 - sparse_categorical_accuracy: 0.9429 - val_loss: 1.2835 - val_sparse_categorical_accuracy: 0.7388\n",
      "Epoch 75/75\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.3403 - sparse_categorical_accuracy: 0.9444 - val_loss: 1.5374 - val_sparse_categorical_accuracy: 0.6836\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 1.5374 - sparse_categorical_accuracy: 0.6836\n",
      "Epoch 1/15\n",
      "1562/1562 [==============================] - 62s 39ms/step - loss: 0.2542 - sparse_categorical_accuracy: 0.9804 - val_loss: 0.9458 - val_sparse_categorical_accuracy: 0.7966\n",
      "Epoch 2/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.2295 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.9503 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 3/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.2198 - sparse_categorical_accuracy: 0.9929 - val_loss: 0.9557 - val_sparse_categorical_accuracy: 0.7974\n",
      "Epoch 4/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.2145 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.9613 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 5/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.2095 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.9608 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 6/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.2055 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.9691 - val_sparse_categorical_accuracy: 0.7968\n",
      "Epoch 7/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.2028 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.9690 - val_sparse_categorical_accuracy: 0.7993\n",
      "Epoch 8/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.1998 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.9776 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 9/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1982 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.9772 - val_sparse_categorical_accuracy: 0.8011\n",
      "Epoch 10/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.1950 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.9810 - val_sparse_categorical_accuracy: 0.7970\n",
      "Epoch 11/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1935 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.9869 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 12/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.1910 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9847 - val_sparse_categorical_accuracy: 0.7974\n",
      "Epoch 13/15\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9877 - val_sparse_categorical_accuracy: 0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1883 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9904 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 15/15\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1860 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.9868 - val_sparse_categorical_accuracy: 0.8002\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.9868 - sparse_categorical_accuracy: 0.8002\n",
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 62s 39ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.9900 - val_sparse_categorical_accuracy: 0.7985\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1839 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9897 - val_sparse_categorical_accuracy: 0.8002\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.9924 - val_sparse_categorical_accuracy: 0.7995\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.9917 - val_sparse_categorical_accuracy: 0.8002\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1834 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.9880 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 60s 39ms/step - loss: 0.1827 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9912 - val_sparse_categorical_accuracy: 0.8006\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1828 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9887 - val_sparse_categorical_accuracy: 0.7997\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1825 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9929 - val_sparse_categorical_accuracy: 0.8011\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1823 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.9927 - val_sparse_categorical_accuracy: 0.7999\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 61s 39ms/step - loss: 0.1816 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.9917 - val_sparse_categorical_accuracy: 0.8004\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.9917 - sparse_categorical_accuracy: 0.8004\n"
     ]
    }
   ],
   "source": [
    "dico[\"scratch_hist\"] = []\n",
    "for EPOCHS in [75,15,10]:\n",
    "    lr /= 10\n",
    "    scratch_model.compile(\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "            loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            )\n",
    "\n",
    "        # Train and evaluate on data.\n",
    "    hist = scratch_model.fit(x_train, y_train, \n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          steps_per_epoch = len(x_train)/BATCH_SIZE,\n",
    "          validation_data =(x_test, y_test),\n",
    "          workers =40,\n",
    "          use_multiprocessing= True,\n",
    "          )\n",
    "\n",
    "    scratch_model.evaluate(x_test, y_test)\n",
    "    dico[\"scratch_hist\"].append(hist)\n",
    "scratch_hist()\n",
    "\n",
    "np.save(\"summary.npy\", dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3c767",
   "metadata": {},
   "source": [
    "## Pruning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565846c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruning:\n",
    "    def __init__(self, model, pruning_factor = 0.5):\n",
    "        \n",
    "        # attributs liés au model\n",
    "        self.model = model\n",
    "        self.pruning_factor = pruning_factor\n",
    "    \n",
    "    # Tensorflow utils setting\n",
    "    def compile(self,optimizer, loss_fn, metric):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_metric = metric\n",
    "    \n",
    "    # Pruning Function\n",
    "    def pruning(self, P_factor = 0.5):\n",
    "        if P_factor >=1 or P_factor <= 0:\n",
    "            raise ValueError (\"Pruning factor value Error : Pruning factor value should be ]0 ;1[\")\n",
    "        for layer in self.model.layers:\n",
    "            if \"conv\" in layer.name:\n",
    "                \n",
    "                # Récuper les kernels\n",
    "                w = layer.get_weights()[0]\n",
    "                b = layer.get_weights()[1]\n",
    "                \n",
    "                # Calcul du filtre contenant la median\n",
    "                tab = []\n",
    "                for i in range(w.shape[-1]):\n",
    "                    somme = 0\n",
    "                    for j in range(w.shape[-1]):\n",
    "                        if i !=j:\n",
    "                            somme += np.linalg.norm(w[:,:,:,i] - w[:,:,:,j])\n",
    "                    tab.append(somme)\n",
    "                    \n",
    "                # calcul du nombre de filtrer a annuler selon le facteur de pruning\n",
    "                nb_pruned_filters = int(w.shape[-1]*P_factor)\n",
    "                \n",
    "                for i in range(nb_pruned_filters):\n",
    "                    # récupérer l'indice du minimum\n",
    "                    ind_min = np.argmin(tab)\n",
    "                    \n",
    "                    #anuuler le filtre qui minimise la formule précedente\n",
    "                    w[:, :, :, ind_min] = np.zeros(w[:, :, :, ind_min].shape)\n",
    "                    \n",
    "                    # astuce pour déplacer le minimum lorsque il faut annuler plusieurs filtres\n",
    "                    tab[ind_min] = np.sum(tab)\n",
    "                \n",
    "                layer.set_weights([w, b])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    #Training algorithm\n",
    "    def train(self,x_train, y_train, val_data, val_labels, epochs = 100, batch_size= 32):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # training history storage\n",
    "        accur = []\n",
    "        L = []\n",
    "        \n",
    "        # validation history storage\n",
    "        v_accur = []\n",
    "        v_loss = []\n",
    "\n",
    "        if x_train.shape[0] % batch_size == 0:\n",
    "            nb_train_steps = x_train.shape[0] // batch_size\n",
    "        else:\n",
    "            nb_train_steps = (x_train.shape[0] // batch_size) + 1\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch ({epoch +1 }/{epochs})\")\n",
    "            for i in range(nb_train_steps):\n",
    "                # Batching data\n",
    "                x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                x = tf.constant(x)\n",
    "                y = tf.constant(y)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    predictions = self.model(x)\n",
    "                    # calcul de la loss\n",
    "                    loss = self.loss_fn(y, predictions)\n",
    "                    \n",
    "                # Calcul du gradient\n",
    "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "                \n",
    "                # Decente de gradient\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                \n",
    "                #Pruning step\n",
    "                self.pruning(P_factor = self.pruning_factor)\n",
    "                \n",
    "                # Update training metric.\n",
    "                self.acc_metric.update_state(y, predictions)\n",
    "                train_acc = self.acc_metric.result()\n",
    "                \n",
    "                print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(train_acc),  loss), end='\\r')\n",
    "            print(\"\\nValidation Step :\")\n",
    "            \n",
    "            # Validation step\n",
    "            val_accur, val_loss = self.test(val_data, val_labels)\n",
    "                \n",
    "            accur.append(float(train_acc))\n",
    "            L.append(loss)\n",
    "            \n",
    "            v_accur.append(val_accur)\n",
    "            v_loss.append(val_loss)\n",
    "            print(\"\")\n",
    "        return (accur, L, v_accur, v_loss)  \n",
    "\n",
    "    \n",
    "    # Test Step \n",
    "    def test(self,data, labels):\n",
    "        accur = []\n",
    "        l = []\n",
    "        if data.shape[0] % self.batch_size == 0:\n",
    "            nb_test_steps = data.shape[0] // self.batch_size\n",
    "        else:\n",
    "            nb_test_steps = (data.shape[0] // self.batch_size) + 1\n",
    "            \n",
    "        for i in range(nb_test_steps):\n",
    "            # Batching data\n",
    "            x = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            y = labels[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            \n",
    "            x = tf.constant(x)\n",
    "            y = tf.constant(y)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = self.model(x)\n",
    "\n",
    "            # calcul de la loss\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "            # calcul de l'accuracy\n",
    "            self.acc_metric.update_state(y, predictions)\n",
    "            test_acc = self.acc_metric.result()\n",
    "            print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(test_acc),  loss), end='\\r')\n",
    "                \n",
    "            accur.append(float(test_acc))\n",
    "            l.append(float(loss))\n",
    "        print(\"\")        \n",
    "        print(\"Accuracy Moy : {:.4f} ; loss Moy: {:.4f}\" .format(np.mean(accur), np.mean(l) ))\n",
    "        \n",
    "        return (np.mean(accur), np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5752f",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25daa7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'scratch_hist': (array([0.46022001, 0.59542   , 0.64877999, 0.67909998, 0.70617998,\n",
       "       0.72618002, 0.74260002, 0.75852001, 0.77245998, 0.78318   ,\n",
       "       0.7942    , 0.80482   , 0.81110001, 0.82169998, 0.82950002,\n",
       "       0.83579999, 0.84307998, 0.84890002, 0.85277998, 0.85993999,\n",
       "       0.86391997, 0.86914003, 0.87058002, 0.87686002, 0.88029999,\n",
       "       0.88139999, 0.88533998, 0.89036   , 0.89241999, 0.89466   ,\n",
       "       0.89780003, 0.90020001, 0.90039998, 0.90674001, 0.90626001,\n",
       "       0.90982002, 0.91149998, 0.91314   , 0.91494   , 0.91758001,\n",
       "       0.91975999, 0.92018002, 0.92325997, 0.92126   , 0.92163998,\n",
       "       0.92677999, 0.92707998, 0.92690003, 0.92882001, 0.92534   ,\n",
       "       0.93076003, 0.93374002, 0.93006003, 0.93324   , 0.93515998,\n",
       "       0.93405998, 0.93386   , 0.93709999, 0.93550003, 0.93768001,\n",
       "       0.94186002, 0.935     , 0.93647999, 0.93870002, 0.93910003,\n",
       "       0.93864   , 0.9382    , 0.93892002, 0.93952   , 0.94309998,\n",
       "       0.93927997, 0.94481999, 0.93848002, 0.94292003, 0.94441998,\n",
       "       0.98040003, 0.98935997, 0.99286002, 0.9946    , 0.99540001,\n",
       "       0.99658   , 0.99651998, 0.99739999, 0.99725997, 0.99786001,\n",
       "       0.99812001, 0.99823999, 0.99823999, 0.99816   , 0.99857998,\n",
       "       0.99923998, 0.99905998, 0.99904001, 0.99892002, 0.99919999,\n",
       "       0.99908   , 0.99910003, 0.99912   , 0.99936002, 0.99927998]), array([0.3937    , 0.56129998, 0.62059999, 0.57099998, 0.67940003,\n",
       "       0.6372    , 0.62290001, 0.69090003, 0.59680003, 0.62080002,\n",
       "       0.69889998, 0.61470002, 0.72579998, 0.76050001, 0.73650002,\n",
       "       0.64829999, 0.74440002, 0.74479997, 0.66960001, 0.6904    ,\n",
       "       0.75319999, 0.71829998, 0.74980003, 0.66640002, 0.71630001,\n",
       "       0.73900002, 0.6983    , 0.6936    , 0.72869998, 0.7507    ,\n",
       "       0.69529998, 0.74089998, 0.72689998, 0.6584    , 0.70529997,\n",
       "       0.71640003, 0.71509999, 0.6444    , 0.68229997, 0.66769999,\n",
       "       0.73199999, 0.68800002, 0.65710002, 0.69160002, 0.66960001,\n",
       "       0.7069    , 0.72719997, 0.68059999, 0.67760003, 0.66500002,\n",
       "       0.73650002, 0.72409999, 0.71560001, 0.7378    , 0.68059999,\n",
       "       0.70819998, 0.75279999, 0.73769999, 0.73089999, 0.71920002,\n",
       "       0.68290001, 0.63590002, 0.70740002, 0.73049998, 0.75319999,\n",
       "       0.69569999, 0.67210001, 0.65630001, 0.53729999, 0.65100002,\n",
       "       0.69840002, 0.70090002, 0.73369998, 0.73879999, 0.68360001,\n",
       "       0.79659998, 0.79809999, 0.7974    , 0.79949999, 0.79979998,\n",
       "       0.79680002, 0.79930001, 0.79949999, 0.80110002, 0.79699999,\n",
       "       0.80000001, 0.7974    , 0.79820001, 0.79949999, 0.80019999,\n",
       "       0.7985    , 0.80019999, 0.79949999, 0.80019999, 0.80000001,\n",
       "       0.80059999, 0.79970002, 0.80110002, 0.7999    , 0.80040002]), array([1.53629923, 1.18974543, 1.05714309, 0.96931392, 0.90598267,\n",
       "       0.85071915, 0.80610961, 0.76619178, 0.73140502, 0.69788092,\n",
       "       0.67428088, 0.648197  , 0.62698162, 0.60636425, 0.58496839,\n",
       "       0.56945783, 0.55339622, 0.54010397, 0.52810019, 0.51089478,\n",
       "       0.50262111, 0.48958278, 0.48477501, 0.4725841 , 0.46381325,\n",
       "       0.46012875, 0.45136216, 0.44508797, 0.43748769, 0.43090129,\n",
       "       0.42824751, 0.42076573, 0.41778982, 0.40854242, 0.40699512,\n",
       "       0.40163144, 0.39626616, 0.39583805, 0.38970497, 0.38830581,\n",
       "       0.38421434, 0.38244197, 0.3748875 , 0.37849388, 0.37757009,\n",
       "       0.37044936, 0.36812437, 0.37058043, 0.36612922, 0.37317845,\n",
       "       0.36370036, 0.35729867, 0.36500657, 0.35752279, 0.35634717,\n",
       "       0.3589159 , 0.35767323, 0.35183051, 0.35526434, 0.35117489,\n",
       "       0.34345195, 0.35814419, 0.35472059, 0.35133833, 0.34952539,\n",
       "       0.35088426, 0.35197672, 0.35052207, 0.34772089, 0.34231183,\n",
       "       0.34942484, 0.33791167, 0.35573927, 0.342922  , 0.34034884,\n",
       "       0.25419962, 0.22951385, 0.21978399, 0.2145358 , 0.20954019,\n",
       "       0.20545481, 0.20277637, 0.19977102, 0.19815834, 0.19497722,\n",
       "       0.19349696, 0.19096978, 0.18962386, 0.18828365, 0.18596813,\n",
       "       0.183826  , 0.18389072, 0.18381545, 0.18383487, 0.18337099,\n",
       "       0.18268193, 0.18276671, 0.18248144, 0.18232472, 0.18162742]), array([2.05771828, 1.30603111, 1.13251412, 1.2540313 , 0.96712655,\n",
       "       1.10556054, 1.15921795, 0.95545793, 1.32326293, 1.19565547,\n",
       "       0.97021931, 1.37131953, 0.91525227, 0.80323279, 0.90361106,\n",
       "       1.28773153, 0.85078019, 0.85046005, 1.15185797, 1.113608  ,\n",
       "       0.91329092, 1.01005173, 0.90213448, 1.32789028, 1.12021887,\n",
       "       0.96103323, 1.16607177, 1.25348914, 1.03236699, 0.93509531,\n",
       "       1.23685777, 0.96705198, 1.08899534, 1.60693038, 1.20875347,\n",
       "       1.23527837, 1.20900905, 1.54148614, 1.36600435, 1.52612841,\n",
       "       1.13829648, 1.34774935, 1.34477603, 1.36057317, 1.43953872,\n",
       "       1.23423862, 1.27067554, 1.45672095, 1.37162876, 1.55226207,\n",
       "       1.1847558 , 1.20016837, 1.38020658, 1.07669878, 1.56733608,\n",
       "       1.43190742, 1.10888243, 1.22881913, 1.17858493, 1.24346435,\n",
       "       1.46763349, 1.79288924, 1.42658722, 1.24790919, 1.16232395,\n",
       "       1.52232325, 1.6007396 , 1.6795814 , 3.25401902, 1.77224123,\n",
       "       1.42179167, 1.48488629, 1.32344687, 1.28349578, 1.53736246,\n",
       "       0.94579035, 0.95029509, 0.95569509, 0.96129102, 0.96077901,\n",
       "       0.96913123, 0.96901202, 0.9775964 , 0.97720313, 0.98098272,\n",
       "       0.98685384, 0.98466825, 0.98765701, 0.99040323, 0.98681867,\n",
       "       0.99001592, 0.98969603, 0.99243134, 0.9917047 , 0.98802137,\n",
       "       0.99120879, 0.98873097, 0.99293292, 0.99270087, 0.99170947])), 'P_factor_0.5_hist': (array([0.20389999, 0.27444544, 0.32165882, 0.35843045, 0.3880724 ,\n",
       "       0.4131    , 0.43489024, 0.4537383 , 0.46985096, 0.48438984,\n",
       "       0.49727845, 0.50892955, 0.51957273, 0.52945542, 0.53842247,\n",
       "       0.54664314, 0.55416137, 0.56122619, 0.56784689, 0.57396048,\n",
       "       0.579656  , 0.58497787, 0.59000218, 0.59472799, 0.59915501,\n",
       "       0.60339159, 0.60738385, 0.61113411, 0.61468786, 0.61813968,\n",
       "       0.62139028, 0.62445027, 0.62740153, 0.63029361, 0.63301337,\n",
       "       0.63564742, 0.63821268, 0.64060086, 0.64301157, 0.64527404,\n",
       "       0.64747059, 0.64962071, 0.65168208, 0.65369922, 0.65565687,\n",
       "       0.65752506, 0.65931278, 0.6610955 , 0.66283447, 0.66450065,\n",
       "       0.66616327, 0.66766977, 0.66919398, 0.67066252, 0.67212003,\n",
       "       0.67351103, 0.67485017, 0.67619193, 0.67752039, 0.67882007,\n",
       "       0.68006271, 0.6812914 , 0.68249255, 0.68364519, 0.68478072,\n",
       "       0.68586355, 0.68691945, 0.68797618, 0.68901235, 0.68996561,\n",
       "       0.69093788, 0.69192392, 0.69287091, 0.69379234, 0.69471335,\n",
       "       0.80677998, 0.79862726, 0.79711765, 0.79733044, 0.79788274,\n",
       "       0.79863715, 0.79943657, 0.80022556, 0.80100566, 0.80170846,\n",
       "       0.80234462, 0.80295068, 0.80354673, 0.80413854, 0.80471236,\n",
       "       0.83424002, 0.82025456, 0.81647056, 0.81470001, 0.81377584,\n",
       "       0.81318283, 0.81280243, 0.81252766, 0.81234908, 0.81223726]), array([0.20979868, 0.27794239, 0.32473634, 0.36119368, 0.39039968,\n",
       "       0.41512192, 0.43646394, 0.45482481, 0.47078213, 0.48521808,\n",
       "       0.49794702, 0.50961445, 0.52022732, 0.5300523 , 0.53895267,\n",
       "       0.54708041, 0.5545592 , 0.561604  , 0.56814926, 0.57423952,\n",
       "       0.5798594 , 0.58515682, 0.59014255, 0.5948296 , 0.59923848,\n",
       "       0.60344811, 0.60744437, 0.61114718, 0.61472448, 0.61817206,\n",
       "       0.62139496, 0.62445674, 0.62741705, 0.63029187, 0.63301384,\n",
       "       0.63565188, 0.63817473, 0.64060914, 0.64300465, 0.64526337,\n",
       "       0.64745755, 0.64959685, 0.65167067, 0.65368708, 0.65562248,\n",
       "       0.65748242, 0.65928259, 0.66106228, 0.66279984, 0.66448443,\n",
       "       0.66609951, 0.66763486, 0.66915867, 0.67063381, 0.67207062,\n",
       "       0.67346061, 0.6748113 , 0.67614861, 0.67747916, 0.67877451,\n",
       "       0.68002005, 0.68124527, 0.68243997, 0.68359638, 0.68473517,\n",
       "       0.68581545, 0.68686437, 0.68792705, 0.68892951, 0.68991088,\n",
       "       0.69088386, 0.69187061, 0.69281153, 0.69373591, 0.69466509,\n",
       "       0.79456359, 0.79297868, 0.79345942, 0.79459539, 0.79570313,\n",
       "       0.79679344, 0.79786369, 0.79885238, 0.79975856, 0.80057865,\n",
       "       0.80131181, 0.80199645, 0.80266289, 0.80331642, 0.80393823,\n",
       "       0.81938997, 0.81370837, 0.81221839, 0.81155525, 0.81128396,\n",
       "       0.81111906, 0.8110383 , 0.81098465, 0.81097845, 0.81100285]), array([1.81423688, 1.8347466 , 1.70290792, 1.7056191 , 1.71209431,\n",
       "       1.66390622, 1.60989773, 1.56557178, 1.58930635, 1.6617682 ,\n",
       "       1.55086684, 1.44996977, 1.38669109, 1.44013453, 1.30973172,\n",
       "       1.4454999 , 1.23503888, 1.14565766, 1.15076494, 1.13135004,\n",
       "       1.17376387, 1.08418679, 1.07190251, 1.24237895, 1.137321  ,\n",
       "       1.00358748, 0.95806187, 0.99759227, 0.99462938, 1.04648447,\n",
       "       1.02933264, 1.0244832 , 1.07880867, 1.06420827, 0.99489892,\n",
       "       0.95261836, 1.11143208, 0.98864657, 1.14546359, 1.13755965,\n",
       "       1.04339445, 0.97297239, 0.98508948, 1.0016371 , 0.9851383 ,\n",
       "       1.02240717, 0.84087956, 0.91541684, 0.85219455, 0.85901761,\n",
       "       0.9363476 , 0.7950424 , 0.87384033, 0.79984623, 0.78491676,\n",
       "       0.78764325, 0.81915647, 0.77422565, 0.72352886, 0.7818619 ,\n",
       "       0.81226099, 0.7893694 , 0.86850023, 0.81421101, 0.72597247,\n",
       "       0.79738069, 0.73942858, 0.78484833, 1.05903542, 0.89760619,\n",
       "       0.7743662 , 0.8814429 , 0.82726276, 0.78783858, 0.75793219,\n",
       "       0.51694036, 0.49646121, 0.4918583 , 0.48226297, 0.49776575,\n",
       "       0.48979545, 0.47827905, 0.46903983, 0.46627545, 0.46237797,\n",
       "       0.46001056, 0.45979112, 0.4592233 , 0.45906574, 0.46342435,\n",
       "       0.40481085, 0.41470674, 0.41658658, 0.41710654, 0.41733551,\n",
       "       0.41786826, 0.41739774, 0.41669258, 0.41512302, 0.41422161]), array([1.89588719, 1.69007018, 1.53428786, 1.39975187, 1.32210727,\n",
       "       1.26014509, 1.23533829, 1.24852845, 1.19732651, 1.18006672,\n",
       "       1.20511475, 1.12250482, 1.07812856, 1.06234872, 1.06179997,\n",
       "       1.07924863, 1.04981061, 1.02474587, 1.04687983, 1.04514572,\n",
       "       1.075631  , 1.08470039, 1.08852069, 1.10026615, 1.11426127,\n",
       "       1.12347369, 1.11686874, 1.15835198, 1.14154717, 1.1368324 ,\n",
       "       1.19547357, 1.13145699, 1.15880631, 1.17776419, 1.13362908,\n",
       "       1.13772552, 1.19004656, 1.09100799, 1.13916408, 1.13518274,\n",
       "       1.15816397, 1.13609972, 1.10445029, 1.13064787, 1.17941504,\n",
       "       1.16180498, 1.1343139 , 1.14000435, 1.15796374, 1.10169542,\n",
       "       1.24837982, 1.16203266, 1.1921306 , 1.12965374, 1.16222942,\n",
       "       1.18777164, 1.16118504, 1.16440221, 1.16490213, 1.17971141,\n",
       "       1.15792897, 1.15032838, 1.19428403, 1.1696928 , 1.16272108,\n",
       "       1.19674382, 1.22048276, 1.19961594, 1.3167756 , 1.22838945,\n",
       "       1.19942755, 1.24285006, 1.24362175, 1.24061617, 1.21657738,\n",
       "       1.1356169 , 1.14441501, 1.15295675, 1.16279211, 1.16992913,\n",
       "       1.17993871, 1.18810282, 1.19616425, 1.20553847, 1.2133535 ,\n",
       "       1.22169182, 1.22980086, 1.23727526, 1.24350096, 1.25213978,\n",
       "       1.22398128, 1.2214067 , 1.22014967, 1.21921047, 1.21875693,\n",
       "       1.21845523, 1.21839664, 1.21872975, 1.21896728, 1.21933485])), 'P_factor_0.5_inf_time': 1.2277433395385742, 'nb_params_p_factor_0.5': 39826},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico = np.load(\"summary.npy\", allow_pickle = True)\n",
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d5d9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/75)\n",
      "Accuracy: 0.2398 ; loss: 1.9929\n",
      "Validation Step :\n",
      "Accuracy: 0.2533 ; loss: 1.9540\n",
      "Accuracy Moy : 0.2469 ; loss Moy: 1.8092\n",
      "\n",
      "Epoch (2/75)\n",
      "Accuracy: 0.3078 ; loss: 1.8754\n",
      "Validation Step :\n",
      "Accuracy: 0.3132 ; loss: 2.0590\n",
      "Accuracy Moy : 0.3107 ; loss Moy: 1.6765\n",
      "\n",
      "Epoch (3/75)\n",
      "Accuracy: 0.3509 ; loss: 1.9006\n",
      "Validation Step :\n",
      "Accuracy: 0.3537 ; loss: 1.7448\n",
      "Accuracy Moy : 0.3524 ; loss Moy: 1.6203\n",
      "\n",
      "Epoch (4/75)\n",
      "Accuracy: 0.3827 ; loss: 1.9930\n",
      "Validation Step :\n",
      "Accuracy: 0.3838 ; loss: 1.9234\n",
      "Accuracy Moy : 0.3832 ; loss Moy: 1.6344\n",
      "\n",
      "Epoch (5/75)\n",
      "Accuracy: 0.4073 ; loss: 2.1064\n",
      "Validation Step :\n",
      "Accuracy: 0.4085 ; loss: 1.8581\n",
      "Accuracy Moy : 0.4079 ; loss Moy: 1.5552\n",
      "\n",
      "Epoch (6/75)\n",
      "Accuracy: 0.4283 ; loss: 2.0260\n",
      "Validation Step :\n",
      "Accuracy: 0.4297 ; loss: 1.9253\n",
      "Accuracy Moy : 0.4290 ; loss Moy: 1.4427\n",
      "\n",
      "Epoch (7/75)\n",
      "Accuracy: 0.4468 ; loss: 2.0721\n",
      "Validation Step :\n",
      "Accuracy: 0.4480 ; loss: 1.8017\n",
      "Accuracy Moy : 0.4475 ; loss Moy: 1.4066\n",
      "\n",
      "Epoch (8/75)\n",
      "Accuracy: 0.4632 ; loss: 2.1939\n",
      "Validation Step :\n",
      "Accuracy: 0.4643 ; loss: 1.8993\n",
      "Accuracy Moy : 0.4637 ; loss Moy: 1.3653\n",
      "\n",
      "Epoch (9/75)\n",
      "Accuracy: 0.4774 ; loss: 2.1861\n",
      "Validation Step :\n",
      "Accuracy: 0.4786 ; loss: 1.6113\n",
      "Accuracy Moy : 0.4780 ; loss Moy: 1.2822\n",
      "\n",
      "Epoch (10/75)\n",
      "Accuracy: 0.4904 ; loss: 2.2444\n",
      "Validation Step :\n",
      "Accuracy: 0.4914 ; loss: 1.4462\n",
      "Accuracy Moy : 0.4909 ; loss Moy: 1.2636\n",
      "\n",
      "Epoch (11/75)\n",
      "Accuracy: 0.5021 ; loss: 1.9771\n",
      "Validation Step :\n",
      "Accuracy: 0.5032 ; loss: 1.2516\n",
      "Accuracy Moy : 0.5027 ; loss Moy: 1.1919\n",
      "\n",
      "Epoch (12/75)\n",
      "Accuracy: 0.5129 ; loss: 1.9094\n",
      "Validation Step :\n",
      "Accuracy: 0.5139 ; loss: 1.3286\n",
      "Accuracy Moy : 0.5134 ; loss Moy: 1.1919\n",
      "\n",
      "Epoch (13/75)\n",
      "Accuracy: 0.5227 ; loss: 1.9276\n",
      "Validation Step :\n",
      "Accuracy: 0.5233 ; loss: 1.4431\n",
      "Accuracy Moy : 0.5230 ; loss Moy: 1.2444\n",
      "\n",
      "Epoch (14/75)\n",
      "Accuracy: 0.5314 ; loss: 1.9499\n",
      "Validation Step :\n",
      "Accuracy: 0.5320 ; loss: 1.3034\n",
      "Accuracy Moy : 0.5317 ; loss Moy: 1.1848\n",
      "\n",
      "Epoch (15/75)\n",
      "Accuracy: 0.5396 ; loss: 1.8648\n",
      "Validation Step :\n",
      "Accuracy: 0.5403 ; loss: 1.0543\n",
      "Accuracy Moy : 0.5400 ; loss Moy: 1.0999\n",
      "\n",
      "Epoch (16/75)\n",
      "Accuracy: 0.5473 ; loss: 1.7757\n",
      "Validation Step :\n",
      "Accuracy: 0.5478 ; loss: 1.2412\n",
      "Accuracy Moy : 0.5476 ; loss Moy: 1.1795\n",
      "\n",
      "Epoch (17/75)\n",
      "Accuracy: 0.5543 ; loss: 1.6589\n",
      "Validation Step :\n",
      "Accuracy: 0.5548 ; loss: 1.1878\n",
      "Accuracy Moy : 0.5546 ; loss Moy: 1.1099\n",
      "\n",
      "Epoch (18/75)\n",
      "Accuracy: 0.5610 ; loss: 1.4461\n",
      "Validation Step :\n",
      "Accuracy: 0.5615 ; loss: 1.1732\n",
      "Accuracy Moy : 0.5612 ; loss Moy: 1.0932\n",
      "\n",
      "Epoch (19/75)\n",
      "Accuracy: 0.5671 ; loss: 1.3409\n",
      "Validation Step :\n",
      "Accuracy: 0.5676 ; loss: 1.1910\n",
      "Accuracy Moy : 0.5674 ; loss Moy: 1.0818\n",
      "\n",
      "Epoch (20/75)\n",
      "Accuracy: 0.5730 ; loss: 1.3489\n",
      "Validation Step :\n",
      "Accuracy: 0.5735 ; loss: 1.1675\n",
      "Accuracy Moy : 0.5732 ; loss Moy: 1.0480\n",
      "\n",
      "Epoch (21/75)\n",
      "Accuracy: 0.5785 ; loss: 1.3944\n",
      "Validation Step :\n",
      "Accuracy: 0.5790 ; loss: 1.1895\n",
      "Accuracy Moy : 0.5788 ; loss Moy: 1.0462\n",
      "\n",
      "Epoch (22/75)\n",
      "Accuracy: 0.5838 ; loss: 1.2753\n",
      "Validation Step :\n",
      "Accuracy: 0.5842 ; loss: 1.0990\n",
      "Accuracy Moy : 0.5841 ; loss Moy: 1.0360\n",
      "\n",
      "Epoch (23/75)\n",
      "Accuracy: 0.5888 ; loss: 1.1897\n",
      "Validation Step :\n",
      "Accuracy: 0.5892 ; loss: 1.1093\n",
      "Accuracy Moy : 0.5890 ; loss Moy: 1.0199\n",
      "\n",
      "Epoch (24/75)\n",
      "Accuracy: 0.5935 ; loss: 1.1830\n",
      "Validation Step :\n",
      "Accuracy: 0.5939 ; loss: 1.1366\n",
      "Accuracy Moy : 0.5937 ; loss Moy: 1.0185\n",
      "\n",
      "Epoch (25/75)\n",
      "Accuracy: 0.5979 ; loss: 1.3207\n",
      "Validation Step :\n",
      "Accuracy: 0.5982 ; loss: 1.2514\n",
      "Accuracy Moy : 0.5981 ; loss Moy: 1.0545\n",
      "\n",
      "Epoch (26/75)\n",
      "Accuracy: 0.6021 ; loss: 1.2504\n",
      "Validation Step :\n",
      "Accuracy: 0.6023 ; loss: 1.2733\n",
      "Accuracy Moy : 0.6022 ; loss Moy: 1.0758\n",
      "\n",
      "Epoch (27/75)\n",
      "Accuracy: 0.6060 ; loss: 1.2086\n",
      "Validation Step :\n",
      "Accuracy: 0.6063 ; loss: 1.2955\n",
      "Accuracy Moy : 0.6061 ; loss Moy: 1.0187\n",
      "\n",
      "Epoch (28/75)\n",
      "Accuracy: 0.6098 ; loss: 1.1789\n",
      "Validation Step :\n",
      "Accuracy: 0.6100 ; loss: 1.3343\n",
      "Accuracy Moy : 0.6099 ; loss Moy: 1.0445\n",
      "\n",
      "Epoch (29/75)\n",
      "Accuracy: 0.6133 ; loss: 1.3028\n",
      "Validation Step :\n",
      "Accuracy: 0.6135 ; loss: 1.5253\n",
      "Accuracy Moy : 0.6135 ; loss Moy: 1.0558\n",
      "\n",
      "Epoch (30/75)\n",
      "Accuracy: 0.6167 ; loss: 1.3077\n",
      "Validation Step :\n",
      "Accuracy: 0.6169 ; loss: 1.3844\n",
      "Accuracy Moy : 0.6168 ; loss Moy: 1.0553\n",
      "\n",
      "Epoch (31/75)\n",
      "Accuracy: 0.6200 ; loss: 1.5104\n",
      "Validation Step :\n",
      "Accuracy: 0.6201 ; loss: 1.5016\n",
      "Accuracy Moy : 0.6201 ; loss Moy: 1.1430\n",
      "\n",
      "Epoch (32/75)\n",
      "Accuracy: 0.6230 ; loss: 1.4391\n",
      "Validation Step :\n",
      "Accuracy: 0.6230 ; loss: 1.5468\n",
      "Accuracy Moy : 0.6230 ; loss Moy: 1.1260\n",
      "\n",
      "Epoch (33/75)\n",
      "Accuracy: 0.6259 ; loss: 1.4326\n",
      "Validation Step :\n",
      "Accuracy: 0.6259 ; loss: 1.5523\n",
      "Accuracy Moy : 0.6259 ; loss Moy: 1.1849\n",
      "\n",
      "Epoch (34/75)\n",
      "Accuracy: 0.6286 ; loss: 1.3382\n",
      "Validation Step :\n",
      "Accuracy: 0.6287 ; loss: 1.2456\n",
      "Accuracy Moy : 0.6287 ; loss Moy: 1.0882\n",
      "\n",
      "Epoch (35/75)\n",
      "Accuracy: 0.6313 ; loss: 1.2345\n",
      "Validation Step :\n",
      "Accuracy: 0.6314 ; loss: 1.3053\n",
      "Accuracy Moy : 0.6314 ; loss Moy: 1.0903\n",
      "\n",
      "Epoch (36/75)\n",
      "Accuracy: 0.6339 ; loss: 1.3267\n",
      "Validation Step :\n",
      "Accuracy: 0.6339 ; loss: 1.1487\n",
      "Accuracy Moy : 0.6339 ; loss Moy: 1.1084\n",
      "\n",
      "Epoch (37/75)\n",
      "Accuracy: 0.6364 ; loss: 1.3275\n",
      "Validation Step :\n",
      "Accuracy: 0.6365 ; loss: 1.1473\n",
      "Accuracy Moy : 0.6364 ; loss Moy: 1.0568\n",
      "\n",
      "Epoch (38/75)\n",
      "Accuracy: 0.6388 ; loss: 1.2467\n",
      "Validation Step :\n",
      "Accuracy: 0.6388 ; loss: 1.2895\n",
      "Accuracy Moy : 0.6388 ; loss Moy: 1.1321\n",
      "\n",
      "Epoch (39/75)\n",
      "Accuracy: 0.6410 ; loss: 1.1662\n",
      "Validation Step :\n",
      "Accuracy: 0.6410 ; loss: 1.1017\n",
      "Accuracy Moy : 0.6410 ; loss Moy: 1.1158\n",
      "\n",
      "Epoch (40/75)\n",
      "Accuracy: 0.6432 ; loss: 1.2646\n",
      "Validation Step :\n",
      "Accuracy: 0.6431 ; loss: 1.1087\n",
      "Accuracy Moy : 0.6432 ; loss Moy: 1.1906\n",
      "\n",
      "Epoch (41/75)\n",
      "Accuracy: 0.6453 ; loss: 1.1835\n",
      "Validation Step :\n",
      "Accuracy: 0.6452 ; loss: 1.2709\n",
      "Accuracy Moy : 0.6452 ; loss Moy: 1.1891\n",
      "\n",
      "Epoch (42/75)\n",
      "Accuracy: 0.6473 ; loss: 1.0547\n",
      "Validation Step :\n",
      "Accuracy: 0.6472 ; loss: 1.2367\n",
      "Accuracy Moy : 0.6472 ; loss Moy: 1.1645\n",
      "\n",
      "Epoch (43/75)\n",
      "Accuracy: 0.6492 ; loss: 1.2638\n",
      "Validation Step :\n",
      "Accuracy: 0.6492 ; loss: 1.3101\n",
      "Accuracy Moy : 0.6492 ; loss Moy: 1.1268\n",
      "\n",
      "Epoch (44/75)\n",
      "Accuracy: 0.6511 ; loss: 1.0251\n",
      "Validation Step :\n",
      "Accuracy: 0.6511 ; loss: 1.1385\n",
      "Accuracy Moy : 0.6511 ; loss Moy: 1.1323\n",
      "\n",
      "Epoch (45/75)\n",
      "Accuracy: 0.6530 ; loss: 0.9611\n",
      "Validation Step :\n",
      "Accuracy: 0.6530 ; loss: 0.9415\n",
      "Accuracy Moy : 0.6530 ; loss Moy: 1.0910\n",
      "\n",
      "Epoch (46/75)\n",
      "Accuracy: 0.6548 ; loss: 1.0548\n",
      "Validation Step :\n",
      "Accuracy: 0.6548 ; loss: 1.0311\n",
      "Accuracy Moy : 0.6548 ; loss Moy: 1.1223\n",
      "\n",
      "Epoch (47/75)\n",
      "Accuracy: 0.6565 ; loss: 1.0932\n",
      "Validation Step :\n",
      "Accuracy: 0.6564 ; loss: 1.1664\n",
      "Accuracy Moy : 0.6565 ; loss Moy: 1.2396\n",
      "\n",
      "Epoch (48/75)\n",
      "Accuracy: 0.6581 ; loss: 0.8707\n",
      "Validation Step :\n",
      "Accuracy: 0.6580 ; loss: 1.0554\n",
      "Accuracy Moy : 0.6581 ; loss Moy: 1.1523\n",
      "\n",
      "Epoch (49/75)\n",
      "Accuracy: 0.6597 ; loss: 1.0531\n",
      "Validation Step :\n",
      "Accuracy: 0.6596 ; loss: 1.1820\n",
      "Accuracy Moy : 0.6597 ; loss Moy: 1.1260\n",
      "\n",
      "Epoch (50/75)\n",
      "Accuracy: 0.6613 ; loss: 0.8993\n",
      "Validation Step :\n",
      "Accuracy: 0.6613 ; loss: 0.9883\n",
      "Accuracy Moy : 0.6613 ; loss Moy: 1.0424\n",
      "\n",
      "Epoch (51/75)\n",
      "Accuracy: 0.6629 ; loss: 1.0349\n",
      "Validation Step :\n",
      "Accuracy: 0.6628 ; loss: 1.1858\n",
      "Accuracy Moy : 0.6628 ; loss Moy: 1.1547\n",
      "\n",
      "Epoch (52/75)\n",
      "Accuracy: 0.6643 ; loss: 1.0225\n",
      "Validation Step :\n",
      "Accuracy: 0.6643 ; loss: 0.8668\n",
      "Accuracy Moy : 0.6643 ; loss Moy: 1.0719\n",
      "\n",
      "Epoch (53/75)\n",
      "Accuracy: 0.6658 ; loss: 0.9685\n",
      "Validation Step :\n",
      "Accuracy: 0.6657 ; loss: 0.9226\n",
      "Accuracy Moy : 0.6658 ; loss Moy: 1.1309\n",
      "\n",
      "Epoch (54/75)\n",
      "Accuracy: 0.6672 ; loss: 1.0433\n",
      "Validation Step :\n",
      "Accuracy: 0.6671 ; loss: 0.8074\n",
      "Accuracy Moy : 0.6672 ; loss Moy: 1.1243\n",
      "\n",
      "Epoch (55/75)\n",
      "Accuracy: 0.6686 ; loss: 0.9114\n",
      "Validation Step :\n",
      "Accuracy: 0.6685 ; loss: 0.8021\n",
      "Accuracy Moy : 0.6685 ; loss Moy: 1.1408\n",
      "\n",
      "Epoch (56/75)\n",
      "Accuracy: 0.6699 ; loss: 0.9905\n",
      "Validation Step :\n",
      "Accuracy: 0.6698 ; loss: 0.8321\n",
      "Accuracy Moy : 0.6699 ; loss Moy: 1.1838\n",
      "\n",
      "Epoch (57/75)\n",
      "Accuracy: 0.6712 ; loss: 0.8503\n",
      "Validation Step :\n",
      "Accuracy: 0.6711 ; loss: 0.7306\n",
      "Accuracy Moy : 0.6711 ; loss Moy: 1.1982\n",
      "\n",
      "Epoch (58/75)\n",
      "Accuracy: 0.6724 ; loss: 0.8532\n",
      "Validation Step :\n",
      "Accuracy: 0.6724 ; loss: 0.5354\n",
      "Accuracy Moy : 0.6724 ; loss Moy: 1.1327\n",
      "\n",
      "Epoch (59/75)\n",
      "Accuracy: 0.6736 ; loss: 0.8299\n",
      "Validation Step :\n",
      "Accuracy: 0.6736 ; loss: 0.7322\n",
      "Accuracy Moy : 0.6736 ; loss Moy: 1.1373\n",
      "\n",
      "Epoch (60/75)\n",
      "Accuracy: 0.6748 ; loss: 0.7402\n",
      "Validation Step :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6748 ; loss: 0.6096\n",
      "Accuracy Moy : 0.6748 ; loss Moy: 1.0747\n",
      "\n",
      "Epoch (61/75)\n",
      "Accuracy: 0.6760 ; loss: 0.8087\n",
      "Validation Step :\n",
      "Accuracy: 0.6759 ; loss: 0.6592\n",
      "Accuracy Moy : 0.6760 ; loss Moy: 1.0696\n",
      "\n",
      "Epoch (62/75)\n",
      "Accuracy: 0.6771 ; loss: 0.8282\n",
      "Validation Step :\n",
      "Accuracy: 0.6771 ; loss: 0.6399\n",
      "Accuracy Moy : 0.6771 ; loss Moy: 1.1081\n",
      "\n",
      "Epoch (63/75)\n",
      "Accuracy: 0.6783 ; loss: 0.7167\n",
      "Validation Step :\n",
      "Accuracy: 0.6782 ; loss: 0.6444\n",
      "Accuracy Moy : 0.6782 ; loss Moy: 1.0972\n",
      "\n",
      "Epoch (64/75)\n",
      "Accuracy: 0.6794 ; loss: 0.8356\n",
      "Validation Step :\n",
      "Accuracy: 0.6793 ; loss: 0.5990\n",
      "Accuracy Moy : 0.6793 ; loss Moy: 1.1914\n",
      "\n",
      "Epoch (65/75)\n",
      "Accuracy: 0.6804 ; loss: 0.8072\n",
      "Validation Step :\n",
      "Accuracy: 0.6803 ; loss: 0.5352\n",
      "Accuracy Moy : 0.6803 ; loss Moy: 1.1517\n",
      "\n",
      "Epoch (66/75)\n",
      "Accuracy: 0.6814 ; loss: 1.0103\n",
      "Validation Step :\n",
      "Accuracy: 0.6813 ; loss: 0.6156\n",
      "Accuracy Moy : 0.6814 ; loss Moy: 1.1647\n",
      "\n",
      "Epoch (67/75)\n",
      "Accuracy: 0.6825 ; loss: 1.0065\n",
      "Validation Step :\n",
      "Accuracy: 0.6823 ; loss: 0.5643\n",
      "Accuracy Moy : 0.6824 ; loss Moy: 1.1690\n",
      "\n",
      "Epoch (68/75)\n",
      "Accuracy: 0.6834 ; loss: 0.9001\n",
      "Validation Step :\n",
      "Accuracy: 0.6833 ; loss: 0.6452\n",
      "Accuracy Moy : 0.6834 ; loss Moy: 1.1302\n",
      "\n",
      "Epoch (69/75)\n",
      "Accuracy: 0.6844 ; loss: 0.9153\n",
      "Validation Step :\n",
      "Accuracy: 0.6843 ; loss: 0.3797\n",
      "Accuracy Moy : 0.6844 ; loss Moy: 1.0984\n",
      "\n",
      "Epoch (70/75)\n",
      "Accuracy: 0.6853 ; loss: 0.8158\n",
      "Validation Step :\n",
      "Accuracy: 0.6852 ; loss: 0.4383\n",
      "Accuracy Moy : 0.6853 ; loss Moy: 1.1330\n",
      "\n",
      "Epoch (71/75)\n",
      "Accuracy: 0.6863 ; loss: 0.7746\n",
      "Validation Step :\n",
      "Accuracy: 0.6862 ; loss: 0.5485\n",
      "Accuracy Moy : 0.6862 ; loss Moy: 1.1424\n",
      "\n",
      "Epoch (72/75)\n",
      "Accuracy: 0.6872 ; loss: 0.9393\n",
      "Validation Step :\n",
      "Accuracy: 0.6871 ; loss: 0.7094\n",
      "Accuracy Moy : 0.6871 ; loss Moy: 1.1905\n",
      "\n",
      "Epoch (73/75)\n",
      "Accuracy: 0.6880 ; loss: 0.9524\n",
      "Validation Step :\n",
      "Accuracy: 0.6879 ; loss: 0.5318\n",
      "Accuracy Moy : 0.6880 ; loss Moy: 1.2207\n",
      "\n",
      "Epoch (74/75)\n",
      "Accuracy: 0.6889 ; loss: 1.0197\n",
      "Validation Step :\n",
      "Accuracy: 0.6888 ; loss: 0.5085\n",
      "Accuracy Moy : 0.6888 ; loss Moy: 1.1732\n",
      "\n",
      "Epoch (75/75)\n",
      "Accuracy: 0.6897 ; loss: 0.8679\n",
      "Validation Step :\n",
      "Accuracy: 0.6896 ; loss: 0.6751\n",
      "Accuracy Moy : 0.6897 ; loss Moy: 1.0962\n",
      "\n",
      "Epoch (1/15)\n",
      "Accuracy: 0.7994 ; loss: 0.5137\n",
      "Validation Step :\n",
      "Accuracy: 0.7808 ; loss: 0.4196\n",
      "Accuracy Moy : 0.7895 ; loss Moy: 0.9871\n",
      "\n",
      "Epoch (2/15)\n",
      "Accuracy: 0.7928 ; loss: 0.5078\n",
      "Validation Step :\n",
      "Accuracy: 0.7842 ; loss: 0.4070\n",
      "Accuracy Moy : 0.7884 ; loss Moy: 0.9900\n",
      "\n",
      "Epoch (3/15)\n",
      "Accuracy: 0.7917 ; loss: 0.5056\n",
      "Validation Step :\n",
      "Accuracy: 0.7860 ; loss: 0.4121\n",
      "Accuracy Moy : 0.7888 ; loss Moy: 0.9931\n",
      "\n",
      "Epoch (4/15)\n",
      "Accuracy: 0.7918 ; loss: 0.5029\n",
      "Validation Step :\n",
      "Accuracy: 0.7875 ; loss: 0.4148\n",
      "Accuracy Moy : 0.7896 ; loss Moy: 0.9958\n",
      "\n",
      "Epoch (5/15)\n",
      "Accuracy: 0.7922 ; loss: 0.5089\n",
      "Validation Step :\n",
      "Accuracy: 0.7887 ; loss: 0.4138\n",
      "Accuracy Moy : 0.7905 ; loss Moy: 0.9994\n",
      "\n",
      "Epoch (6/15)\n",
      "Accuracy: 0.7927 ; loss: 0.5152\n",
      "Validation Step :\n",
      "Accuracy: 0.7898 ; loss: 0.4104\n",
      "Accuracy Moy : 0.7912 ; loss Moy: 1.0037\n",
      "\n",
      "Epoch (7/15)\n",
      "Accuracy: 0.7932 ; loss: 0.5199\n",
      "Validation Step :\n",
      "Accuracy: 0.7906 ; loss: 0.4104\n",
      "Accuracy Moy : 0.7919 ; loss Moy: 1.0069\n",
      "\n",
      "Epoch (8/15)\n",
      "Accuracy: 0.7936 ; loss: 0.5195\n",
      "Validation Step :\n",
      "Accuracy: 0.7914 ; loss: 0.4139\n",
      "Accuracy Moy : 0.7925 ; loss Moy: 1.0106\n",
      "\n",
      "Epoch (9/15)\n",
      "Accuracy: 0.7941 ; loss: 0.5258\n",
      "Validation Step :\n",
      "Accuracy: 0.7921 ; loss: 0.4147\n",
      "Accuracy Moy : 0.7931 ; loss Moy: 1.0128\n",
      "\n",
      "Epoch (10/15)\n",
      "Accuracy: 0.7945 ; loss: 0.5263\n",
      "Validation Step :\n",
      "Accuracy: 0.7927 ; loss: 0.4223\n",
      "Accuracy Moy : 0.7936 ; loss Moy: 1.0160\n",
      "\n",
      "Epoch (11/15)\n",
      "Accuracy: 0.7950 ; loss: 0.5290\n",
      "Validation Step :\n",
      "Accuracy: 0.7933 ; loss: 0.4183\n",
      "Accuracy Moy : 0.7941 ; loss Moy: 1.0197\n",
      "\n",
      "Epoch (12/15)\n",
      "Accuracy: 0.7954 ; loss: 0.5261\n",
      "Validation Step :\n",
      "Accuracy: 0.7939 ; loss: 0.4200\n",
      "Accuracy Moy : 0.7947 ; loss Moy: 1.0241\n",
      "\n",
      "Epoch (13/15)\n",
      "Accuracy: 0.7958 ; loss: 0.5278\n",
      "Validation Step :\n",
      "Accuracy: 0.7944 ; loss: 0.4202\n",
      "Accuracy Moy : 0.7951 ; loss Moy: 1.0270\n",
      "\n",
      "Epoch (14/15)\n",
      "Accuracy: 0.7962 ; loss: 0.5285\n",
      "Validation Step :\n",
      "Accuracy: 0.7949 ; loss: 0.4259\n",
      "Accuracy Moy : 0.7956 ; loss Moy: 1.0307\n",
      "\n",
      "Epoch (15/15)\n",
      "Accuracy: 0.7967 ; loss: 0.5248\n",
      "Validation Step :\n",
      "Accuracy: 0.7954 ; loss: 0.4350\n",
      "Accuracy Moy : 0.7960 ; loss Moy: 1.0348\n",
      "\n",
      "Epoch (1/10)\n",
      "Accuracy: 0.8227 ; loss: 0.4863\n",
      "Validation Step :\n",
      "Accuracy: 0.8008 ; loss: 0.4794\n",
      "Accuracy Moy : 0.8111 ; loss Moy: 1.0125\n",
      "\n",
      "Epoch (2/10)\n",
      "Accuracy: 0.8117 ; loss: 0.4895\n",
      "Validation Step :\n",
      "Accuracy: 0.8018 ; loss: 0.4776\n",
      "Accuracy Moy : 0.8067 ; loss Moy: 1.0110\n",
      "\n",
      "Epoch (3/10)\n",
      "Accuracy: 0.8086 ; loss: 0.4935\n",
      "Validation Step :\n",
      "Accuracy: 0.8022 ; loss: 0.4750\n",
      "Accuracy Moy : 0.8054 ; loss Moy: 1.0100\n",
      "\n",
      "Epoch (4/10)\n",
      "Accuracy: 0.8072 ; loss: 0.4946\n",
      "Validation Step :\n",
      "Accuracy: 0.8025 ; loss: 0.4713\n",
      "Accuracy Moy : 0.8049 ; loss Moy: 1.0094\n",
      "\n",
      "Epoch (5/10)\n",
      "Accuracy: 0.8065 ; loss: 0.4951\n",
      "Validation Step :\n",
      "Accuracy: 0.8027 ; loss: 0.4697\n",
      "Accuracy Moy : 0.8046 ; loss Moy: 1.0089\n",
      "\n",
      "Epoch (6/10)\n",
      "Accuracy: 0.8060 ; loss: 0.4956\n",
      "Validation Step :\n",
      "Accuracy: 0.8029 ; loss: 0.4687\n",
      "Accuracy Moy : 0.8045 ; loss Moy: 1.0087\n",
      "\n",
      "Epoch (7/10)\n",
      "Accuracy: 0.8057 ; loss: 0.4964\n",
      "Validation Step :\n",
      "Accuracy: 0.8030 ; loss: 0.4675\n",
      "Accuracy Moy : 0.8043 ; loss Moy: 1.0086\n",
      "\n",
      "Epoch (8/10)\n",
      "Accuracy: 0.8054 ; loss: 0.4964\n",
      "Validation Step :\n",
      "Accuracy: 0.8031 ; loss: 0.4671\n",
      "Accuracy Moy : 0.8043 ; loss Moy: 1.0084\n",
      "\n",
      "Epoch (9/10)\n",
      "Accuracy: 0.8052 ; loss: 0.4970\n",
      "Validation Step :\n",
      "Accuracy: 0.8031 ; loss: 0.4667\n",
      "Accuracy Moy : 0.8042 ; loss Moy: 1.0085\n",
      "\n",
      "Epoch (10/10)\n",
      "Accuracy: 0.8051 ; loss: 0.4979\n",
      "Validation Step :\n",
      "Accuracy: 0.8032 ; loss: 0.4667\n",
      "Accuracy Moy : 0.8042 ; loss Moy: 1.0086\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scratch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     val_l \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(val_l,val_loss)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Afficher les courbes d'entrainement\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#plot_hist(f\"Lenet5_P_factor_{p}.png\")\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# inference time\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m pruned_inf_time \u001b[38;5;241m=\u001b[39m \u001b[43minference_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Enregister l'historique\u001b[39;00m\n\u001b[1;32m     41\u001b[0m dico[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP_factor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (accuracy, val_accuracy, l, val_l)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36minference_time\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     24\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m     pred1 \u001b[38;5;241m=\u001b[39m \u001b[43mscratch_model\u001b[49m(x_test)\n\u001b[1;32m     26\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     27\u001b[0m     scratch\u001b[38;5;241m.\u001b[39mappend(t2\u001b[38;5;241m-\u001b[39mt1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scratch_model' is not defined"
     ]
    }
   ],
   "source": [
    "for p in [0.6, 0.7, 0.8, 0.9]:\n",
    "    accuracy = np.array([])\n",
    "    val_accuracy = np.array([])\n",
    "\n",
    "    l = np.array([])\n",
    "    val_l = np.array([])\n",
    "\n",
    "\n",
    "    P = Pruning(tf.keras.models.clone_model(model), \n",
    "                pruning_factor =p)\n",
    "    lr = 1\n",
    "    for epoch in [75, 15, 10]:\n",
    "        # Paramètre d'entrainement\n",
    "        lr /= 10\n",
    "        P.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "                 loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metric = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        )\n",
    "\n",
    "        # Entrainement         \n",
    "        accur, loss, val_accur, val_loss = P.train(x_train, y_train, \n",
    "                                                   x_test, y_test, \n",
    "                                                   epochs = epoch, \n",
    "                                                   batch_size= 32 )  \n",
    "\n",
    "\n",
    "        # plot hist\n",
    "        accuracy = np.append(accuracy, accur)\n",
    "        val_accuracy = np.append(val_accuracy, val_accur)\n",
    "        l = np.append(l, loss)\n",
    "        val_l = np.append(val_l,val_loss)\n",
    "\n",
    "    \n",
    "    # Afficher les courbes d'entrainement\n",
    "    #plot_hist(f\"Lenet5_P_factor_{p}.png\")\n",
    "    \n",
    "    # inference time\n",
    "    pruned_inf_time = inference_time()\n",
    "    \n",
    "    # Enregister l'historique\n",
    "    dico[f\"P_factor_{p}_hist\"] = (accuracy, val_accuracy, l, val_l)\n",
    "    \n",
    "    # calcul du temps d'inférence\n",
    "    dico[f\"P_factor_{p}_inf_time\"] = pruned_inf_time\n",
    "\n",
    "    \n",
    "    # memory used\n",
    "    dico[f\"nb_params_p_factor_{p}\"] = count_parameters(P.model)\n",
    "    \n",
    "    # sauvegarder les poids\n",
    "    P.model.save_weights(f\"w_Resnet8_p_{p}.h5\")\n",
    "    \n",
    "    # Sauvegarder les données du dictionnaire\n",
    "    np.save(\"summary.npy\", dico)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c317b",
   "metadata": {},
   "source": [
    "## Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9839b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(dic, figname, scratch = False):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for p in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        # Train accuracy\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][0], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # Validation accuracy\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dico[f\"P_factor_{p}_hist\"][1], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # train loss\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][2], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # validation loss\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][3], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "    if scratch == True: \n",
    "        # Courbe scratch\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[\"scratch_hist\"][0], label = \"Scratch Train accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dic[\"scratch_hist\"][1], label = \"Scratch Val accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[\"scratch_hist\"][2], label = \"Scratch Train loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "\n",
    "\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[\"scratch_hist\"][3], label = \"Scratch Val loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    \n",
    "    plt.savefig(figname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df765246",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m figname\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_seed10.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m dico \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_resnet32.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43meval_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdico\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36meval_plot\u001b[0;34m(dic, figname, scratch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validation accuracy\u001b[39;00m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m222\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mdico\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mP_factor_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_hist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m], label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid()\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figname= f\"test_seed10.png\"\n",
    "dico = np.load(\"summary_resnet32.npy\", allow_pickle=True)\n",
    "eval_plot(dico.item(), figname, scratch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51829d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
