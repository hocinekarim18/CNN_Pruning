{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4976fd",
   "metadata": {},
   "source": [
    "## Bibiliothèque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da05cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 09:58:49.835738: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-23 09:58:49.835769: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pyplot as plt\n",
    "from resnet import resnet_v2\n",
    "from keras_flops import get_flops\n",
    "import time\n",
    "%matplotlib inline\n",
    "seed = tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c602222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(figname):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(accuracy, label = \"train accuracy\")\n",
    "    plt.plot(val_accuracy, label = \"validation accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(l, label = \"train loss\")\n",
    "    plt.plot(val_l, label = \"validation loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(figname)\n",
    "    plt.show()\n",
    "    \n",
    "def inference_time():\n",
    "    scratch = []\n",
    "    pruned = []\n",
    "    for i in range(10):\n",
    "        t1 = time.time()\n",
    "        pred1 = scratch_model(x_test)\n",
    "        t2 = time.time()\n",
    "        scratch.append(t2-t1)\n",
    "\n",
    "        # Pruned model\n",
    "        t3 = time.time()\n",
    "        pred2 = P.model(x_test)\n",
    "        t4 = time.time()\n",
    "        pruned.append(t4-t3)\n",
    "\n",
    "    # display\n",
    "    print(\"Scratch inference time : \", np.mean(scratch), \" s\")\n",
    "    print(\"Pruned inference time : \", np.mean(pruned), \" s\")\n",
    "    return np.mean(pruned)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    somme = 0\n",
    "    for l in model.trainable_variables:\n",
    "        somme += np.count_nonzero(l)\n",
    "    return somme\n",
    "\n",
    "\n",
    "\n",
    "def scratch_hist():   \n",
    "    loss = dico[\"scratch_hist\"][0].history[\"loss\"]\n",
    "    val_loss = dico[\"scratch_hist\"][0].history[\"val_loss\"]\n",
    "    accuracy = dico[\"scratch_hist\"][0].history[\"sparse_categorical_accuracy\"]\n",
    "    val_accuracy =  dico[\"scratch_hist\"][0].history[\"val_sparse_categorical_accuracy\"]\n",
    "\n",
    "    for i in range(len( dico[\"scratch_hist\"])):\n",
    "        if i !=0:\n",
    "            loss = np.append(loss, dico[\"scratch_hist\"][i].history[\"loss\"])\n",
    "            val_loss = np.append(val_loss, dico[\"scratch_hist\"][i].history[\"val_loss\"])\n",
    "            accuracy = np.append(accuracy, dico[\"scratch_hist\"][i].history[\"sparse_categorical_accuracy\"])\n",
    "            val_accuracy =  np.append(val_accuracy, dico[\"scratch_hist\"][i].history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "    dico[\"scratch_hist\"] = (accuracy, val_accuracy, loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8cf7b",
   "metadata": {},
   "source": [
    "## Loading cifar10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2339918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Data Loading ================\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "y_test shape: (10000, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"================ Data Loading ================\")\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Data shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66b092",
   "metadata": {},
   "source": [
    "## Building Lenet5 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d168fdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 14, 14, 6)        0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 5, 5, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 09:58:56.861315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-23 09:58:56.861367: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-23 09:58:56.861400: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big25): /proc/driver/nvidia/version does not exist\n",
      "2022-06-23 09:58:56.861875: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(32,32,3)),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2, padding=\"valid\"),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2, padding=\"valid\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=120, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=84, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "\n",
    "]);\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035728c6",
   "metadata": {},
   "source": [
    "## Scratch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b271752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disctionnaire pour enregistrer les infos pertinentes\n",
    "dico = {}\n",
    "scratch_model = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea2a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02397941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 1.8607 - sparse_categorical_accuracy: 0.3170 - val_loss: 1.6611 - val_sparse_categorical_accuracy: 0.3958\n",
      "Epoch 2/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.5568 - sparse_categorical_accuracy: 0.4337 - val_loss: 1.6055 - val_sparse_categorical_accuracy: 0.4216\n",
      "Epoch 3/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.4382 - sparse_categorical_accuracy: 0.4811 - val_loss: 1.4534 - val_sparse_categorical_accuracy: 0.4750\n",
      "Epoch 4/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.3512 - sparse_categorical_accuracy: 0.5157 - val_loss: 1.4574 - val_sparse_categorical_accuracy: 0.4836\n",
      "Epoch 5/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.2796 - sparse_categorical_accuracy: 0.5405 - val_loss: 1.3486 - val_sparse_categorical_accuracy: 0.5208\n",
      "Epoch 6/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.2160 - sparse_categorical_accuracy: 0.5643 - val_loss: 1.3205 - val_sparse_categorical_accuracy: 0.5416\n",
      "Epoch 7/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.1645 - sparse_categorical_accuracy: 0.5848 - val_loss: 1.2879 - val_sparse_categorical_accuracy: 0.5458\n",
      "Epoch 8/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.1197 - sparse_categorical_accuracy: 0.6034 - val_loss: 1.2917 - val_sparse_categorical_accuracy: 0.5477\n",
      "Epoch 9/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.0820 - sparse_categorical_accuracy: 0.6154 - val_loss: 1.2440 - val_sparse_categorical_accuracy: 0.5612\n",
      "Epoch 10/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.0449 - sparse_categorical_accuracy: 0.6286 - val_loss: 1.2698 - val_sparse_categorical_accuracy: 0.5673\n",
      "Epoch 11/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 1.0177 - sparse_categorical_accuracy: 0.6374 - val_loss: 1.3161 - val_sparse_categorical_accuracy: 0.5570\n",
      "Epoch 12/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.9853 - sparse_categorical_accuracy: 0.6473 - val_loss: 1.4104 - val_sparse_categorical_accuracy: 0.5339\n",
      "Epoch 13/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.9566 - sparse_categorical_accuracy: 0.6596 - val_loss: 1.3534 - val_sparse_categorical_accuracy: 0.5525\n",
      "Epoch 14/75\n",
      "1562/1562 [==============================] - 13s 9ms/step - loss: 0.9331 - sparse_categorical_accuracy: 0.6680 - val_loss: 1.3771 - val_sparse_categorical_accuracy: 0.5611\n",
      "Epoch 15/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.9110 - sparse_categorical_accuracy: 0.6751 - val_loss: 1.3421 - val_sparse_categorical_accuracy: 0.5593\n",
      "Epoch 16/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.8843 - sparse_categorical_accuracy: 0.6828 - val_loss: 1.4415 - val_sparse_categorical_accuracy: 0.5552\n",
      "Epoch 17/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.8734 - sparse_categorical_accuracy: 0.6897 - val_loss: 1.4309 - val_sparse_categorical_accuracy: 0.5644\n",
      "Epoch 18/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.8522 - sparse_categorical_accuracy: 0.6951 - val_loss: 1.4096 - val_sparse_categorical_accuracy: 0.5652\n",
      "Epoch 19/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.8380 - sparse_categorical_accuracy: 0.7008 - val_loss: 1.4366 - val_sparse_categorical_accuracy: 0.5443\n",
      "Epoch 20/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.8236 - sparse_categorical_accuracy: 0.7077 - val_loss: 1.4525 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 21/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7993 - sparse_categorical_accuracy: 0.7142 - val_loss: 1.4301 - val_sparse_categorical_accuracy: 0.5583\n",
      "Epoch 22/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7944 - sparse_categorical_accuracy: 0.7158 - val_loss: 1.5381 - val_sparse_categorical_accuracy: 0.5539\n",
      "Epoch 23/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7837 - sparse_categorical_accuracy: 0.7174 - val_loss: 1.5578 - val_sparse_categorical_accuracy: 0.5382\n",
      "Epoch 24/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7708 - sparse_categorical_accuracy: 0.7241 - val_loss: 1.5389 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 25/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7626 - sparse_categorical_accuracy: 0.7292 - val_loss: 1.6340 - val_sparse_categorical_accuracy: 0.5499\n",
      "Epoch 26/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7582 - sparse_categorical_accuracy: 0.7286 - val_loss: 1.6137 - val_sparse_categorical_accuracy: 0.5455\n",
      "Epoch 27/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7434 - sparse_categorical_accuracy: 0.7350 - val_loss: 1.7190 - val_sparse_categorical_accuracy: 0.5305\n",
      "Epoch 28/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7317 - sparse_categorical_accuracy: 0.7376 - val_loss: 1.7114 - val_sparse_categorical_accuracy: 0.5507\n",
      "Epoch 29/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7339 - sparse_categorical_accuracy: 0.7386 - val_loss: 1.7075 - val_sparse_categorical_accuracy: 0.5428\n",
      "Epoch 30/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.7240 - sparse_categorical_accuracy: 0.7421 - val_loss: 1.6506 - val_sparse_categorical_accuracy: 0.5465\n",
      "Epoch 31/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7127 - sparse_categorical_accuracy: 0.7454 - val_loss: 1.6312 - val_sparse_categorical_accuracy: 0.5575\n",
      "Epoch 32/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7029 - sparse_categorical_accuracy: 0.7491 - val_loss: 1.8312 - val_sparse_categorical_accuracy: 0.5233\n",
      "Epoch 33/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7063 - sparse_categorical_accuracy: 0.7493 - val_loss: 1.7565 - val_sparse_categorical_accuracy: 0.5349\n",
      "Epoch 34/75\n",
      "1562/1562 [==============================] - 13s 8ms/step - loss: 0.7032 - sparse_categorical_accuracy: 0.7512 - val_loss: 1.7373 - val_sparse_categorical_accuracy: 0.5437\n",
      "Epoch 35/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6845 - sparse_categorical_accuracy: 0.7584 - val_loss: 1.7184 - val_sparse_categorical_accuracy: 0.5459\n",
      "Epoch 36/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6870 - sparse_categorical_accuracy: 0.7555 - val_loss: 1.8252 - val_sparse_categorical_accuracy: 0.5445\n",
      "Epoch 37/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6832 - sparse_categorical_accuracy: 0.7561 - val_loss: 1.7790 - val_sparse_categorical_accuracy: 0.5519\n",
      "Epoch 38/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6866 - sparse_categorical_accuracy: 0.7591 - val_loss: 1.8689 - val_sparse_categorical_accuracy: 0.5332\n",
      "Epoch 39/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6736 - sparse_categorical_accuracy: 0.7646 - val_loss: 1.7513 - val_sparse_categorical_accuracy: 0.5568\n",
      "Epoch 40/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6752 - sparse_categorical_accuracy: 0.7628 - val_loss: 1.8532 - val_sparse_categorical_accuracy: 0.5491\n",
      "Epoch 41/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6689 - sparse_categorical_accuracy: 0.7639 - val_loss: 1.8777 - val_sparse_categorical_accuracy: 0.5325\n",
      "Epoch 42/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6724 - sparse_categorical_accuracy: 0.7610 - val_loss: 1.9139 - val_sparse_categorical_accuracy: 0.5448\n",
      "Epoch 43/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6642 - sparse_categorical_accuracy: 0.7646 - val_loss: 1.8692 - val_sparse_categorical_accuracy: 0.5183\n",
      "Epoch 44/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6594 - sparse_categorical_accuracy: 0.7675 - val_loss: 2.1425 - val_sparse_categorical_accuracy: 0.5087\n",
      "Epoch 45/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6493 - sparse_categorical_accuracy: 0.7717 - val_loss: 1.9776 - val_sparse_categorical_accuracy: 0.5418\n",
      "Epoch 46/75\n",
      "1562/1562 [==============================] - 15s 10ms/step - loss: 0.6556 - sparse_categorical_accuracy: 0.7701 - val_loss: 2.0493 - val_sparse_categorical_accuracy: 0.5216\n",
      "Epoch 47/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6591 - sparse_categorical_accuracy: 0.7690 - val_loss: 1.9723 - val_sparse_categorical_accuracy: 0.5454\n",
      "Epoch 48/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6421 - sparse_categorical_accuracy: 0.7743 - val_loss: 1.9947 - val_sparse_categorical_accuracy: 0.5446\n",
      "Epoch 49/75\n",
      "1562/1562 [==============================] - 15s 10ms/step - loss: 0.6527 - sparse_categorical_accuracy: 0.7727 - val_loss: 1.9664 - val_sparse_categorical_accuracy: 0.5378\n",
      "Epoch 50/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6506 - sparse_categorical_accuracy: 0.7734 - val_loss: 1.9774 - val_sparse_categorical_accuracy: 0.5309\n",
      "Epoch 51/75\n",
      "1562/1562 [==============================] - 15s 10ms/step - loss: 0.6466 - sparse_categorical_accuracy: 0.7737 - val_loss: 1.9752 - val_sparse_categorical_accuracy: 0.5474\n",
      "Epoch 52/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6315 - sparse_categorical_accuracy: 0.7773 - val_loss: 2.0461 - val_sparse_categorical_accuracy: 0.5337\n",
      "Epoch 53/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6345 - sparse_categorical_accuracy: 0.7791 - val_loss: 2.1129 - val_sparse_categorical_accuracy: 0.5381\n",
      "Epoch 54/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6418 - sparse_categorical_accuracy: 0.7776 - val_loss: 2.0368 - val_sparse_categorical_accuracy: 0.5404\n",
      "Epoch 55/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6419 - sparse_categorical_accuracy: 0.7798 - val_loss: 2.3614 - val_sparse_categorical_accuracy: 0.5119\n",
      "Epoch 56/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6319 - sparse_categorical_accuracy: 0.7809 - val_loss: 2.1438 - val_sparse_categorical_accuracy: 0.5407\n",
      "Epoch 57/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6291 - sparse_categorical_accuracy: 0.7816 - val_loss: 2.1699 - val_sparse_categorical_accuracy: 0.5325\n",
      "Epoch 58/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6469 - sparse_categorical_accuracy: 0.7763 - val_loss: 2.2276 - val_sparse_categorical_accuracy: 0.5251\n",
      "Epoch 59/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6305 - sparse_categorical_accuracy: 0.7823 - val_loss: 2.0553 - val_sparse_categorical_accuracy: 0.5433\n",
      "Epoch 60/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6333 - sparse_categorical_accuracy: 0.7799 - val_loss: 2.8035 - val_sparse_categorical_accuracy: 0.4709\n",
      "Epoch 61/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6297 - sparse_categorical_accuracy: 0.7850 - val_loss: 2.2043 - val_sparse_categorical_accuracy: 0.5379\n",
      "Epoch 62/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6321 - sparse_categorical_accuracy: 0.7819 - val_loss: 2.2182 - val_sparse_categorical_accuracy: 0.5411\n",
      "Epoch 63/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6319 - sparse_categorical_accuracy: 0.7834 - val_loss: 2.2203 - val_sparse_categorical_accuracy: 0.5343\n",
      "Epoch 64/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6167 - sparse_categorical_accuracy: 0.7892 - val_loss: 2.3164 - val_sparse_categorical_accuracy: 0.5219\n",
      "Epoch 65/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.7830 - val_loss: 2.2223 - val_sparse_categorical_accuracy: 0.5329\n",
      "Epoch 66/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6375 - sparse_categorical_accuracy: 0.7806 - val_loss: 2.2017 - val_sparse_categorical_accuracy: 0.5303\n",
      "Epoch 67/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6236 - sparse_categorical_accuracy: 0.7859 - val_loss: 2.5652 - val_sparse_categorical_accuracy: 0.5059\n",
      "Epoch 68/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6267 - sparse_categorical_accuracy: 0.7839 - val_loss: 2.4061 - val_sparse_categorical_accuracy: 0.5375\n",
      "Epoch 69/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6414 - sparse_categorical_accuracy: 0.7809 - val_loss: 2.4242 - val_sparse_categorical_accuracy: 0.5035\n",
      "Epoch 70/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6225 - sparse_categorical_accuracy: 0.7865 - val_loss: 2.3131 - val_sparse_categorical_accuracy: 0.5269\n",
      "Epoch 71/75\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.6255 - sparse_categorical_accuracy: 0.7854 - val_loss: 2.2585 - val_sparse_categorical_accuracy: 0.5445\n",
      "Epoch 72/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6248 - sparse_categorical_accuracy: 0.7862 - val_loss: 2.3787 - val_sparse_categorical_accuracy: 0.5297\n",
      "Epoch 73/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6288 - sparse_categorical_accuracy: 0.7866 - val_loss: 2.3759 - val_sparse_categorical_accuracy: 0.5210\n",
      "Epoch 74/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6267 - sparse_categorical_accuracy: 0.7862 - val_loss: 2.7721 - val_sparse_categorical_accuracy: 0.5249\n",
      "Epoch 75/75\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.6400 - sparse_categorical_accuracy: 0.7824 - val_loss: 2.2662 - val_sparse_categorical_accuracy: 0.5315\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 2.2662 - sparse_categorical_accuracy: 0.5315\n",
      "Epoch 1/15\n",
      "1562/1562 [==============================] - 15s 10ms/step - loss: 0.3396 - sparse_categorical_accuracy: 0.8803 - val_loss: 2.4543 - val_sparse_categorical_accuracy: 0.5569\n",
      "Epoch 2/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.2662 - sparse_categorical_accuracy: 0.9060 - val_loss: 2.6346 - val_sparse_categorical_accuracy: 0.5580\n",
      "Epoch 3/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.2342 - sparse_categorical_accuracy: 0.9182 - val_loss: 2.7467 - val_sparse_categorical_accuracy: 0.5578\n",
      "Epoch 4/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.2126 - sparse_categorical_accuracy: 0.9265 - val_loss: 2.8459 - val_sparse_categorical_accuracy: 0.5633\n",
      "Epoch 5/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.1962 - sparse_categorical_accuracy: 0.9335 - val_loss: 2.9818 - val_sparse_categorical_accuracy: 0.5586\n",
      "Epoch 6/15\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.1826 - sparse_categorical_accuracy: 0.9384 - val_loss: 3.0715 - val_sparse_categorical_accuracy: 0.5577\n",
      "Epoch 7/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.1716 - sparse_categorical_accuracy: 0.9428 - val_loss: 3.1625 - val_sparse_categorical_accuracy: 0.5615\n",
      "Epoch 8/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.1614 - sparse_categorical_accuracy: 0.9467 - val_loss: 3.2472 - val_sparse_categorical_accuracy: 0.5603\n",
      "Epoch 9/15\n",
      "1562/1562 [==============================] - 15s 9ms/step - loss: 0.1525 - sparse_categorical_accuracy: 0.9505 - val_loss: 3.3303 - val_sparse_categorical_accuracy: 0.5581\n",
      "Epoch 10/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.1443 - sparse_categorical_accuracy: 0.9537 - val_loss: 3.4530 - val_sparse_categorical_accuracy: 0.5597\n",
      "Epoch 11/15\n",
      "1562/1562 [==============================] - 14s 9ms/step - loss: 0.1369 - sparse_categorical_accuracy: 0.9565 - val_loss: 3.5082 - val_sparse_categorical_accuracy: 0.5591\n",
      "Epoch 12/15\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9593 - val_loss: 3.6108 - val_sparse_categorical_accuracy: 0.5570\n",
      "Epoch 13/15\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.1242 - sparse_categorical_accuracy: 0.9614 - val_loss: 3.6981 - val_sparse_categorical_accuracy: 0.5566\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.1182 - sparse_categorical_accuracy: 0.9636 - val_loss: 3.7824 - val_sparse_categorical_accuracy: 0.5568\n",
      "Epoch 15/15\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.1130 - sparse_categorical_accuracy: 0.9656 - val_loss: 3.8690 - val_sparse_categorical_accuracy: 0.5577\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 3.8690 - sparse_categorical_accuracy: 0.5577\n",
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 18s 11ms/step - loss: 0.1023 - sparse_categorical_accuracy: 0.9704 - val_loss: 3.8771 - val_sparse_categorical_accuracy: 0.5588\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9707 - val_loss: 3.8925 - val_sparse_categorical_accuracy: 0.5590\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.1004 - sparse_categorical_accuracy: 0.9710 - val_loss: 3.9048 - val_sparse_categorical_accuracy: 0.5587\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0998 - sparse_categorical_accuracy: 0.9710 - val_loss: 3.9135 - val_sparse_categorical_accuracy: 0.5583\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0993 - sparse_categorical_accuracy: 0.9713 - val_loss: 3.9318 - val_sparse_categorical_accuracy: 0.5588\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0988 - sparse_categorical_accuracy: 0.9715 - val_loss: 3.9346 - val_sparse_categorical_accuracy: 0.5579\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9715 - val_loss: 3.9488 - val_sparse_categorical_accuracy: 0.5579\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0978 - sparse_categorical_accuracy: 0.9716 - val_loss: 3.9615 - val_sparse_categorical_accuracy: 0.5578\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9719 - val_loss: 3.9666 - val_sparse_categorical_accuracy: 0.5573\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 17s 11ms/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9719 - val_loss: 3.9787 - val_sparse_categorical_accuracy: 0.5577\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 3.9787 - sparse_categorical_accuracy: 0.5577\n"
     ]
    }
   ],
   "source": [
    "dico[\"scratch_hist\"] = []\n",
    "for epoch in [75, 15, 10 ]:\n",
    "    lr /= 10\n",
    "    scratch_model.compile(\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "            loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            )\n",
    "\n",
    "        # Train and evaluate on data.\n",
    "    hist = scratch_model.fit(x_train, y_train, \n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs= epoch,\n",
    "          steps_per_epoch = len(x_train)/BATCH_SIZE,\n",
    "          validation_data =(x_test, y_test),\n",
    "          workers =40,\n",
    "          use_multiprocessing= True,\n",
    "          )\n",
    "\n",
    "    scratch_model.evaluate(x_test, y_test)\n",
    "    dico[\"scratch_hist\"].append(hist)\n",
    "scratch_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3c767",
   "metadata": {},
   "source": [
    "## Pruning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565846c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruning:\n",
    "    def __init__(self, model, pruning_factor = 0.5):\n",
    "        \n",
    "        # attributs liés au model\n",
    "        self.model = model\n",
    "        self.pruning_factor = pruning_factor\n",
    "    \n",
    "    # Tensorflow utils setting\n",
    "    def compile(self,optimizer, loss_fn, metric):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_metric = metric\n",
    "    \n",
    "    # Pruning Function\n",
    "    def pruning(self, P_factor = 0.5):\n",
    "        if P_factor >=1 or P_factor <= 0:\n",
    "            raise ValueError (\"Pruning factor value Error : Pruning factor value should be ]0 ;1[\")\n",
    "        for layer in self.model.layers:\n",
    "            if \"conv\" in layer.name:\n",
    "                \n",
    "                # Récuper les kernels\n",
    "                w = layer.get_weights()[0]\n",
    "                b = layer.get_weights()[1]\n",
    "                \n",
    "                # Calcul du filtre contenant la median\n",
    "                tab = []\n",
    "                for i in range(w.shape[-1]):\n",
    "                    somme = 0\n",
    "                    for j in range(w.shape[-1]):\n",
    "                        if i !=j:\n",
    "                            somme += np.linalg.norm(w[:,:,:,i] - w[:,:,:,j])\n",
    "                    tab.append(somme)\n",
    "                    \n",
    "                # calcul du nombre de filtrer a annuler selon le facteur de pruning\n",
    "                nb_pruned_filters = int(w.shape[-1]*P_factor)\n",
    "                \n",
    "                for i in range(nb_pruned_filters):\n",
    "                    # récupérer l'indice du minimum\n",
    "                    ind_min = np.argmin(tab)\n",
    "                    \n",
    "                    #anuuler le filtre qui minimise la formule précedente\n",
    "                    w[:, :, :, ind_min] = np.zeros(w[:, :, :, ind_min].shape)\n",
    "                    \n",
    "                    # astuce pour déplacer le minimum lorsque il faut annuler plusieurs filtres\n",
    "                    tab[ind_min] = np.sum(tab)\n",
    "                \n",
    "                layer.set_weights([w, b])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    #Training algorithm\n",
    "    def train(self,x_train, y_train, val_data, val_labels, epochs = 100, batch_size= 32):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # training history storage\n",
    "        accur = []\n",
    "        L = []\n",
    "        \n",
    "        # validation history storage\n",
    "        v_accur = []\n",
    "        v_loss = []\n",
    "\n",
    "        if x_train.shape[0] % batch_size == 0:\n",
    "            nb_train_steps = x_train.shape[0] // batch_size\n",
    "        else:\n",
    "            nb_train_steps = (x_train.shape[0] // batch_size) + 1\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch ({epoch +1 }/{epochs})\")\n",
    "            for i in range(nb_train_steps):\n",
    "                # Batching data\n",
    "                x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                x = tf.constant(x)\n",
    "                y = tf.constant(y)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    predictions = self.model(x)\n",
    "                    # calcul de la loss\n",
    "                    loss = self.loss_fn(y, predictions)\n",
    "                    \n",
    "                # Calcul du gradient\n",
    "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "                \n",
    "                # Decente de gradient\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                \n",
    "                #Pruning step\n",
    "                self.pruning(P_factor = self.pruning_factor)\n",
    "                \n",
    "                # Update training metric.\n",
    "                self.acc_metric.update_state(y, predictions)\n",
    "                train_acc = self.acc_metric.result()\n",
    "                \n",
    "                print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(train_acc),  loss), end='\\r')\n",
    "            print(\"\\nValidation Step :\")\n",
    "            \n",
    "            # Validation step\n",
    "            val_accur, val_loss = self.test(val_data, val_labels)\n",
    "                \n",
    "            accur.append(float(train_acc))\n",
    "            L.append(loss)\n",
    "            \n",
    "            v_accur.append(val_accur)\n",
    "            v_loss.append(val_loss)\n",
    "            print(\"\")\n",
    "        return (accur, L, v_accur, v_loss)  \n",
    "\n",
    "    \n",
    "    # Test Step \n",
    "    def test(self,data, labels):\n",
    "        accur = []\n",
    "        l = []\n",
    "        if data.shape[0] % self.batch_size == 0:\n",
    "            nb_test_steps = data.shape[0] // self.batch_size\n",
    "        else:\n",
    "            nb_test_steps = (data.shape[0] // self.batch_size) + 1\n",
    "            \n",
    "        for i in range(nb_test_steps):\n",
    "            # Batching data\n",
    "            x = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            y = labels[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            \n",
    "            x = tf.constant(x)\n",
    "            y = tf.constant(y)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = self.model(x)\n",
    "\n",
    "            # calcul de la loss\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "            # calcul de l'accuracy\n",
    "            self.acc_metric.update_state(y, predictions)\n",
    "            test_acc = self.acc_metric.result()\n",
    "            print(\"Accuracy: {:.4f} ; loss: {:.4f}\".format(float(test_acc),  loss), end='\\r')\n",
    "                \n",
    "            accur.append(float(test_acc))\n",
    "            l.append(float(loss))\n",
    "        print(\"\")        \n",
    "        print(\"Accuracy Moy : {:.4f} ; loss Moy: {:.4f}\" .format(np.mean(accur), np.mean(l) ))\n",
    "        \n",
    "        return (np.mean(accur), np.mean(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5752f",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d5d9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/75)\n",
      "Accuracy: 0.2960 ; loss: 2.0955\n",
      "Validation Step :\n",
      "Accuracy: 0.3095 ; loss: 1.8816\n",
      "Accuracy Moy : 0.3030 ; loss Moy: 1.7464\n",
      "\n",
      "Epoch (2/75)\n",
      "Accuracy: 0.3536 ; loss: 2.2954\n",
      "Validation Step :\n",
      "Accuracy: 0.3597 ; loss: 1.4118\n",
      "Accuracy Moy : 0.3568 ; loss Moy: 1.6046\n",
      "\n",
      "Epoch (3/75)\n",
      "Accuracy: 0.3845 ; loss: 2.0056\n",
      "Validation Step :\n",
      "Accuracy: 0.3884 ; loss: 1.2884\n",
      "Accuracy Moy : 0.3866 ; loss Moy: 1.5419\n",
      "\n",
      "Epoch (4/75)\n",
      "Accuracy: 0.4056 ; loss: 2.1174\n",
      "Validation Step :\n",
      "Accuracy: 0.4085 ; loss: 1.2536\n",
      "Accuracy Moy : 0.4072 ; loss Moy: 1.4754\n",
      "\n",
      "Epoch (5/75)\n",
      "Accuracy: 0.4217 ; loss: 1.9130\n",
      "Validation Step :\n",
      "Accuracy: 0.4236 ; loss: 1.0436\n",
      "Accuracy Moy : 0.4227 ; loss Moy: 1.4757\n",
      "\n",
      "Epoch (6/75)\n",
      "Accuracy: 0.4338 ; loss: 1.8894\n",
      "Validation Step :\n",
      "Accuracy: 0.4350 ; loss: 1.0904\n",
      "Accuracy Moy : 0.4345 ; loss Moy: 1.4938\n",
      "\n",
      "Epoch (7/75)\n",
      "Accuracy: 0.4437 ; loss: 1.7347\n",
      "Validation Step :\n",
      "Accuracy: 0.4446 ; loss: 1.3852\n",
      "Accuracy Moy : 0.4442 ; loss Moy: 1.5003\n",
      "\n",
      "Epoch (8/75)\n",
      "Accuracy: 0.4522 ; loss: 1.6011\n",
      "Validation Step :\n",
      "Accuracy: 0.4523 ; loss: 0.9809\n",
      "Accuracy Moy : 0.4523 ; loss Moy: 1.5911\n",
      "\n",
      "Epoch (9/75)\n",
      "Accuracy: 0.4592 ; loss: 1.4699\n",
      "Validation Step :\n",
      "Accuracy: 0.4594 ; loss: 1.0956\n",
      "Accuracy Moy : 0.4593 ; loss Moy: 1.5289\n",
      "\n",
      "Epoch (10/75)\n",
      "Accuracy: 0.4653 ; loss: 1.3132\n",
      "Validation Step :\n",
      "Accuracy: 0.4653 ; loss: 1.2881\n",
      "Accuracy Moy : 0.4653 ; loss Moy: 1.5838\n",
      "\n",
      "Epoch (11/75)\n",
      "Accuracy: 0.4709 ; loss: 1.5444\n",
      "Validation Step :\n",
      "Accuracy: 0.4708 ; loss: 1.1490\n",
      "Accuracy Moy : 0.4708 ; loss Moy: 1.6600\n",
      "\n",
      "Epoch (12/75)\n",
      "Accuracy: 0.4759 ; loss: 1.5164\n",
      "Validation Step :\n",
      "Accuracy: 0.4759 ; loss: 1.4721\n",
      "Accuracy Moy : 0.4758 ; loss Moy: 1.5837\n",
      "\n",
      "Epoch (13/75)\n",
      "Accuracy: 0.4805 ; loss: 1.2636\n",
      "Validation Step :\n",
      "Accuracy: 0.4805 ; loss: 1.3637\n",
      "Accuracy Moy : 0.4805 ; loss Moy: 1.5776\n",
      "\n",
      "Epoch (14/75)\n",
      "Accuracy: 0.4849 ; loss: 1.2959\n",
      "Validation Step :\n",
      "Accuracy: 0.4848 ; loss: 1.1635\n",
      "Accuracy Moy : 0.4849 ; loss Moy: 1.5655\n",
      "\n",
      "Epoch (15/75)\n",
      "Accuracy: 0.4890 ; loss: 1.3454\n",
      "Validation Step :\n",
      "Accuracy: 0.4886 ; loss: 1.5977\n",
      "Accuracy Moy : 0.4888 ; loss Moy: 1.8057\n",
      "\n",
      "Epoch (16/75)\n",
      "Accuracy: 0.4925 ; loss: 1.3153\n",
      "Validation Step :\n",
      "Accuracy: 0.4923 ; loss: 1.1687\n",
      "Accuracy Moy : 0.4924 ; loss Moy: 1.5838\n",
      "\n",
      "Epoch (17/75)\n",
      "Accuracy: 0.4959 ; loss: 1.3144\n",
      "Validation Step :\n",
      "Accuracy: 0.4957 ; loss: 1.0814\n",
      "Accuracy Moy : 0.4958 ; loss Moy: 1.6102\n",
      "\n",
      "Epoch (18/75)\n",
      "Accuracy: 0.4992 ; loss: 1.2348\n",
      "Validation Step :\n",
      "Accuracy: 0.4990 ; loss: 1.1817\n",
      "Accuracy Moy : 0.4991 ; loss Moy: 1.6518\n",
      "\n",
      "Epoch (19/75)\n",
      "Accuracy: 0.5023 ; loss: 1.2695\n",
      "Validation Step :\n",
      "Accuracy: 0.5019 ; loss: 1.4113\n",
      "Accuracy Moy : 0.5021 ; loss Moy: 1.7899\n",
      "\n",
      "Epoch (20/75)\n",
      "Accuracy: 0.5050 ; loss: 1.1835\n",
      "Validation Step :\n",
      "Accuracy: 0.5046 ; loss: 1.3055\n",
      "Accuracy Moy : 0.5048 ; loss Moy: 1.7216\n",
      "\n",
      "Epoch (21/75)\n",
      "Accuracy: 0.5077 ; loss: 1.4888\n",
      "Validation Step :\n",
      "Accuracy: 0.5072 ; loss: 1.1612\n",
      "Accuracy Moy : 0.5075 ; loss Moy: 1.9208\n",
      "\n",
      "Epoch (22/75)\n",
      "Accuracy: 0.5100 ; loss: 1.2803\n",
      "Validation Step :\n",
      "Accuracy: 0.5097 ; loss: 1.1129\n",
      "Accuracy Moy : 0.5099 ; loss Moy: 1.6601\n",
      "\n",
      "Epoch (23/75)\n",
      "Accuracy: 0.5125 ; loss: 1.0288\n",
      "Validation Step :\n",
      "Accuracy: 0.5122 ; loss: 1.1380\n",
      "Accuracy Moy : 0.5123 ; loss Moy: 1.7622\n",
      "\n",
      "Epoch (24/75)\n",
      "Accuracy: 0.5148 ; loss: 1.1665\n",
      "Validation Step :\n",
      "Accuracy: 0.5144 ; loss: 1.0309\n",
      "Accuracy Moy : 0.5146 ; loss Moy: 1.7135\n",
      "\n",
      "Epoch (25/75)\n",
      "Accuracy: 0.5170 ; loss: 1.2013\n",
      "Validation Step :\n",
      "Accuracy: 0.5167 ; loss: 1.1957\n",
      "Accuracy Moy : 0.5168 ; loss Moy: 1.7276\n",
      "\n",
      "Epoch (26/75)\n",
      "Accuracy: 0.5192 ; loss: 1.3655\n",
      "Validation Step :\n",
      "Accuracy: 0.5190 ; loss: 1.3331\n",
      "Accuracy Moy : 0.5191 ; loss Moy: 1.6470\n",
      "\n",
      "Epoch (27/75)\n",
      "Accuracy: 0.5213 ; loss: 1.4190\n",
      "Validation Step :\n",
      "Accuracy: 0.5210 ; loss: 1.4719\n",
      "Accuracy Moy : 0.5212 ; loss Moy: 1.6875\n",
      "\n",
      "Epoch (28/75)\n",
      "Accuracy: 0.5232 ; loss: 1.2570\n",
      "Validation Step :\n",
      "Accuracy: 0.5229 ; loss: 1.5008\n",
      "Accuracy Moy : 0.5231 ; loss Moy: 1.8817\n",
      "\n",
      "Epoch (29/75)\n",
      "Accuracy: 0.5251 ; loss: 1.2015\n",
      "Validation Step :\n",
      "Accuracy: 0.5248 ; loss: 1.8093\n",
      "Accuracy Moy : 0.5250 ; loss Moy: 1.7967\n",
      "\n",
      "Epoch (30/75)\n",
      "Accuracy: 0.5269 ; loss: 0.9213\n",
      "Validation Step :\n",
      "Accuracy: 0.5266 ; loss: 1.0204\n",
      "Accuracy Moy : 0.5268 ; loss Moy: 1.7640\n",
      "\n",
      "Epoch (31/75)\n",
      "Accuracy: 0.5287 ; loss: 1.2811\n",
      "Validation Step :\n",
      "Accuracy: 0.5284 ; loss: 1.6479\n",
      "Accuracy Moy : 0.5285 ; loss Moy: 1.6740\n",
      "\n",
      "Epoch (32/75)\n",
      "Accuracy: 0.5302 ; loss: 1.0272\n",
      "Validation Step :\n",
      "Accuracy: 0.5300 ; loss: 1.2384\n",
      "Accuracy Moy : 0.5301 ; loss Moy: 1.8131\n",
      "\n",
      "Epoch (33/75)\n",
      "Accuracy: 0.5319 ; loss: 1.0707\n",
      "Validation Step :\n",
      "Accuracy: 0.5315 ; loss: 1.6324\n",
      "Accuracy Moy : 0.5317 ; loss Moy: 2.0258\n",
      "\n",
      "Epoch (34/75)\n",
      "Accuracy: 0.5333 ; loss: 0.6607\n",
      "Validation Step :\n",
      "Accuracy: 0.5331 ; loss: 1.4393\n",
      "Accuracy Moy : 0.5332 ; loss Moy: 1.8059\n",
      "\n",
      "Epoch (35/75)\n",
      "Accuracy: 0.5348 ; loss: 1.0321\n",
      "Validation Step :\n",
      "Accuracy: 0.5345 ; loss: 1.3633\n",
      "Accuracy Moy : 0.5347 ; loss Moy: 1.7563\n",
      "\n",
      "Epoch (36/75)\n",
      "Accuracy: 0.5363 ; loss: 0.7109\n",
      "Validation Step :\n",
      "Accuracy: 0.5359 ; loss: 1.6341\n",
      "Accuracy Moy : 0.5361 ; loss Moy: 1.9981\n",
      "\n",
      "Epoch (37/75)\n",
      "Accuracy: 0.5377 ; loss: 1.1941\n",
      "Validation Step :\n",
      "Accuracy: 0.5373 ; loss: 1.1928\n",
      "Accuracy Moy : 0.5375 ; loss Moy: 1.8481\n",
      "\n",
      "Epoch (38/75)\n",
      "Accuracy: 0.5390 ; loss: 0.7439\n",
      "Validation Step :\n",
      "Accuracy: 0.5386 ; loss: 1.3878\n",
      "Accuracy Moy : 0.5388 ; loss Moy: 1.9172\n",
      "\n",
      "Epoch (39/75)\n",
      "Accuracy: 0.5402 ; loss: 0.7563\n",
      "Validation Step :\n",
      "Accuracy: 0.5399 ; loss: 1.4449\n",
      "Accuracy Moy : 0.5400 ; loss Moy: 2.0038\n",
      "\n",
      "Epoch (40/75)\n",
      "Accuracy: 0.5414 ; loss: 0.8842\n",
      "Validation Step :\n",
      "Accuracy: 0.5411 ; loss: 1.6698\n",
      "Accuracy Moy : 0.5412 ; loss Moy: 1.9715\n",
      "\n",
      "Epoch (41/75)\n",
      "Accuracy: 0.5425 ; loss: 0.8849\n",
      "Validation Step :\n",
      "Accuracy: 0.5422 ; loss: 1.2287\n",
      "Accuracy Moy : 0.5424 ; loss Moy: 1.9260\n",
      "\n",
      "Epoch (42/75)\n",
      "Accuracy: 0.5437 ; loss: 0.8421\n",
      "Validation Step :\n",
      "Accuracy: 0.5434 ; loss: 1.0600\n",
      "Accuracy Moy : 0.5436 ; loss Moy: 1.8819\n",
      "\n",
      "Epoch (43/75)\n",
      "Accuracy: 0.5447 ; loss: 0.8044\n",
      "Validation Step :\n",
      "Accuracy: 0.5445 ; loss: 1.1592\n",
      "Accuracy Moy : 0.5446 ; loss Moy: 1.8785\n",
      "\n",
      "Epoch (44/75)\n",
      "Accuracy: 0.5458 ; loss: 0.9591\n",
      "Validation Step :\n",
      "Accuracy: 0.5455 ; loss: 1.4628\n",
      "Accuracy Moy : 0.5457 ; loss Moy: 1.9487\n",
      "\n",
      "Epoch (45/75)\n",
      "Accuracy: 0.5469 ; loss: 0.9442\n",
      "Validation Step :\n",
      "Accuracy: 0.5466 ; loss: 1.0525\n",
      "Accuracy Moy : 0.5468 ; loss Moy: 1.8796\n",
      "\n",
      "Epoch (46/75)\n",
      "Accuracy: 0.5479 ; loss: 0.8965\n",
      "Validation Step :\n",
      "Accuracy: 0.5476 ; loss: 0.9524\n",
      "Accuracy Moy : 0.5478 ; loss Moy: 1.8017\n",
      "\n",
      "Epoch (47/75)\n",
      "Accuracy: 0.5489 ; loss: 0.8129\n",
      "Validation Step :\n",
      "Accuracy: 0.5486 ; loss: 1.1189\n",
      "Accuracy Moy : 0.5488 ; loss Moy: 2.0383\n",
      "\n",
      "Epoch (48/75)\n",
      "Accuracy: 0.5498 ; loss: 0.7926\n",
      "Validation Step :\n",
      "Accuracy: 0.5495 ; loss: 1.1608\n",
      "Accuracy Moy : 0.5497 ; loss Moy: 1.9667\n",
      "\n",
      "Epoch (49/75)\n",
      "Accuracy: 0.5507 ; loss: 0.9765\n",
      "Validation Step :\n",
      "Accuracy: 0.5505 ; loss: 1.1266\n",
      "Accuracy Moy : 0.5506 ; loss Moy: 1.7510\n",
      "\n",
      "Epoch (50/75)\n",
      "Accuracy: 0.5516 ; loss: 0.7484\n",
      "Validation Step :\n",
      "Accuracy: 0.5513 ; loss: 1.1486\n",
      "Accuracy Moy : 0.5515 ; loss Moy: 1.8597\n",
      "\n",
      "Epoch (51/75)\n",
      "Accuracy: 0.5525 ; loss: 0.7963\n",
      "Validation Step :\n",
      "Accuracy: 0.5522 ; loss: 0.8776\n",
      "Accuracy Moy : 0.5524 ; loss Moy: 1.9548\n",
      "\n",
      "Epoch (52/75)\n",
      "Accuracy: 0.5533 ; loss: 0.8210\n",
      "Validation Step :\n",
      "Accuracy: 0.5531 ; loss: 1.1292\n",
      "Accuracy Moy : 0.5532 ; loss Moy: 1.8894\n",
      "\n",
      "Epoch (53/75)\n",
      "Accuracy: 0.5542 ; loss: 0.8429\n",
      "Validation Step :\n",
      "Accuracy: 0.5539 ; loss: 0.7315\n",
      "Accuracy Moy : 0.5540 ; loss Moy: 2.0146\n",
      "\n",
      "Epoch (54/75)\n",
      "Accuracy: 0.5550 ; loss: 0.8165\n",
      "Validation Step :\n",
      "Accuracy: 0.5547 ; loss: 0.8796\n",
      "Accuracy Moy : 0.5548 ; loss Moy: 1.9269\n",
      "\n",
      "Epoch (55/75)\n",
      "Accuracy: 0.5557 ; loss: 0.9985\n",
      "Validation Step :\n",
      "Accuracy: 0.5554 ; loss: 1.2758\n",
      "Accuracy Moy : 0.5556 ; loss Moy: 1.8271\n",
      "\n",
      "Epoch (56/75)\n",
      "Accuracy: 0.5565 ; loss: 1.0790\n",
      "Validation Step :\n",
      "Accuracy: 0.5562 ; loss: 1.4460\n",
      "Accuracy Moy : 0.5563 ; loss Moy: 1.8363\n",
      "\n",
      "Epoch (57/75)\n",
      "Accuracy: 0.5571 ; loss: 0.9354\n",
      "Validation Step :\n",
      "Accuracy: 0.5569 ; loss: 1.5590\n",
      "Accuracy Moy : 0.5570 ; loss Moy: 2.0252\n",
      "\n",
      "Epoch (58/75)\n",
      "Accuracy: 0.5578 ; loss: 1.0262\n",
      "Validation Step :\n",
      "Accuracy: 0.5576 ; loss: 1.3307\n",
      "Accuracy Moy : 0.5577 ; loss Moy: 1.8572\n",
      "\n",
      "Epoch (59/75)\n",
      "Accuracy: 0.5586 ; loss: 1.0109\n",
      "Validation Step :\n",
      "Accuracy: 0.5583 ; loss: 1.1450\n",
      "Accuracy Moy : 0.5584 ; loss Moy: 1.8895\n",
      "\n",
      "Epoch (60/75)\n",
      "Accuracy: 0.5592 ; loss: 0.9065\n",
      "Validation Step :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5590 ; loss: 0.7736\n",
      "Accuracy Moy : 0.5591 ; loss Moy: 1.9587\n",
      "\n",
      "Epoch (61/75)\n",
      "Accuracy: 0.5599 ; loss: 0.6465\n",
      "Validation Step :\n",
      "Accuracy: 0.5597 ; loss: 0.9183\n",
      "Accuracy Moy : 0.5598 ; loss Moy: 1.8899\n",
      "\n",
      "Epoch (62/75)\n",
      "Accuracy: 0.5606 ; loss: 0.7078\n",
      "Validation Step :\n",
      "Accuracy: 0.5603 ; loss: 1.2789\n",
      "Accuracy Moy : 0.5605 ; loss Moy: 1.9702\n",
      "\n",
      "Epoch (63/75)\n",
      "Accuracy: 0.5612 ; loss: 0.8756\n",
      "Validation Step :\n",
      "Accuracy: 0.5609 ; loss: 1.7390\n",
      "Accuracy Moy : 0.5611 ; loss Moy: 1.9730\n",
      "\n",
      "Epoch (64/75)\n",
      "Accuracy: 0.5618 ; loss: 0.8271\n",
      "Validation Step :\n",
      "Accuracy: 0.5616 ; loss: 0.9769\n",
      "Accuracy Moy : 0.5617 ; loss Moy: 1.9629\n",
      "\n",
      "Epoch (65/75)\n",
      "Accuracy: 0.5624 ; loss: 1.1460\n",
      "Validation Step :\n",
      "Accuracy: 0.5621 ; loss: 1.1572\n",
      "Accuracy Moy : 0.5623 ; loss Moy: 2.0665\n",
      "\n",
      "Epoch (66/75)\n",
      "Accuracy: 0.5630 ; loss: 0.8770\n",
      "Validation Step :\n",
      "Accuracy: 0.5627 ; loss: 0.8138\n",
      "Accuracy Moy : 0.5628 ; loss Moy: 2.1113\n",
      "\n",
      "Epoch (67/75)\n",
      "Accuracy: 0.5635 ; loss: 1.4373\n",
      "Validation Step :\n",
      "Accuracy: 0.5632 ; loss: 1.6012\n",
      "Accuracy Moy : 0.5634 ; loss Moy: 2.1136\n",
      "\n",
      "Epoch (68/75)\n",
      "Accuracy: 0.5640 ; loss: 1.3871\n",
      "Validation Step :\n",
      "Accuracy: 0.5638 ; loss: 1.3469\n",
      "Accuracy Moy : 0.5639 ; loss Moy: 2.0126\n",
      "\n",
      "Epoch (69/75)\n",
      "Accuracy: 0.5646 ; loss: 1.1447\n",
      "Validation Step :\n",
      "Accuracy: 0.5643 ; loss: 1.2296\n",
      "Accuracy Moy : 0.5645 ; loss Moy: 2.1146\n",
      "\n",
      "Epoch (70/75)\n",
      "Accuracy: 0.5651 ; loss: 0.9608\n",
      "Validation Step :\n",
      "Accuracy: 0.5648 ; loss: 0.9412\n",
      "Accuracy Moy : 0.5650 ; loss Moy: 2.1004\n",
      "\n",
      "Epoch (71/75)\n",
      "Accuracy: 0.5656 ; loss: 1.4268\n",
      "Validation Step :\n",
      "Accuracy: 0.5654 ; loss: 1.0670\n",
      "Accuracy Moy : 0.5655 ; loss Moy: 1.9377\n",
      "\n",
      "Epoch (72/75)\n",
      "Accuracy: 0.5661 ; loss: 1.0731\n",
      "Validation Step :\n",
      "Accuracy: 0.5659 ; loss: 1.5506\n",
      "Accuracy Moy : 0.5660 ; loss Moy: 2.0011\n",
      "\n",
      "Epoch (73/75)\n",
      "Accuracy: 0.5666 ; loss: 0.8438\n",
      "Validation Step :\n",
      "Accuracy: 0.5664 ; loss: 1.1452\n",
      "Accuracy Moy : 0.5665 ; loss Moy: 2.0865\n",
      "\n",
      "Epoch (74/75)\n",
      "Accuracy: 0.5671 ; loss: 0.9963\n",
      "Validation Step :\n",
      "Accuracy: 0.5669 ; loss: 1.4133\n",
      "Accuracy Moy : 0.5670 ; loss Moy: 2.0099\n",
      "\n",
      "Epoch (75/75)\n",
      "Accuracy: 0.5676 ; loss: 1.1471\n",
      "Validation Step :\n",
      "Accuracy: 0.5673 ; loss: 1.3039\n",
      "Accuracy Moy : 0.5675 ; loss Moy: 1.8795\n",
      "\n",
      "Epoch (1/15)\n",
      "Accuracy: 0.6870 ; loss: 0.7339\n",
      "Validation Step :\n",
      "Accuracy: 0.6558 ; loss: 1.3730\n",
      "Accuracy Moy : 0.6705 ; loss Moy: 1.8510\n",
      "\n",
      "Epoch (2/15)\n",
      "Accuracy: 0.6805 ; loss: 0.7414\n",
      "Validation Step :\n",
      "Accuracy: 0.6656 ; loss: 1.3774\n",
      "Accuracy Moy : 0.6729 ; loss Moy: 1.8816\n",
      "\n",
      "Epoch (3/15)\n",
      "Accuracy: 0.6819 ; loss: 0.7251\n",
      "Validation Step :\n",
      "Accuracy: 0.6717 ; loss: 1.4159\n",
      "Accuracy Moy : 0.6768 ; loss Moy: 1.9105\n",
      "\n",
      "Epoch (4/15)\n",
      "Accuracy: 0.6841 ; loss: 0.7171\n",
      "Validation Step :\n",
      "Accuracy: 0.6764 ; loss: 1.4336\n",
      "Accuracy Moy : 0.6802 ; loss Moy: 1.9342\n",
      "\n",
      "Epoch (5/15)\n",
      "Accuracy: 0.6866 ; loss: 0.7086\n",
      "Validation Step :\n",
      "Accuracy: 0.6803 ; loss: 1.4429\n",
      "Accuracy Moy : 0.6834 ; loss Moy: 1.9622\n",
      "\n",
      "Epoch (6/15)\n",
      "Accuracy: 0.6890 ; loss: 0.6965\n",
      "Validation Step :\n",
      "Accuracy: 0.6838 ; loss: 1.4696\n",
      "Accuracy Moy : 0.6864 ; loss Moy: 1.9921\n",
      "\n",
      "Epoch (7/15)\n",
      "Accuracy: 0.6912 ; loss: 0.6684\n",
      "Validation Step :\n",
      "Accuracy: 0.6866 ; loss: 1.4606\n",
      "Accuracy Moy : 0.6889 ; loss Moy: 2.0187\n",
      "\n",
      "Epoch (8/15)\n",
      "Accuracy: 0.6932 ; loss: 0.6600\n",
      "Validation Step :\n",
      "Accuracy: 0.6892 ; loss: 1.4668\n",
      "Accuracy Moy : 0.6912 ; loss Moy: 2.0435\n",
      "\n",
      "Epoch (9/15)\n",
      "Accuracy: 0.6951 ; loss: 0.6322\n",
      "Validation Step :\n",
      "Accuracy: 0.6915 ; loss: 1.5030\n",
      "Accuracy Moy : 0.6933 ; loss Moy: 2.0727\n",
      "\n",
      "Epoch (10/15)\n",
      "Accuracy: 0.6969 ; loss: 0.6422\n",
      "Validation Step :\n",
      "Accuracy: 0.6936 ; loss: 1.5771\n",
      "Accuracy Moy : 0.6953 ; loss Moy: 2.0966\n",
      "\n",
      "Epoch (11/15)\n",
      "Accuracy: 0.6987 ; loss: 0.6172\n",
      "Validation Step :\n",
      "Accuracy: 0.6956 ; loss: 1.6402\n",
      "Accuracy Moy : 0.6972 ; loss Moy: 2.1256\n",
      "\n",
      "Epoch (12/15)\n",
      "Accuracy: 0.7004 ; loss: 0.6017\n",
      "Validation Step :\n",
      "Accuracy: 0.6975 ; loss: 1.6393\n",
      "Accuracy Moy : 0.6989 ; loss Moy: 2.1524\n",
      "\n",
      "Epoch (13/15)\n",
      "Accuracy: 0.7019 ; loss: 0.5997\n",
      "Validation Step :\n",
      "Accuracy: 0.6993 ; loss: 1.7048\n",
      "Accuracy Moy : 0.7006 ; loss Moy: 2.1779\n",
      "\n",
      "Epoch (14/15)\n",
      "Accuracy: 0.7035 ; loss: 0.6039\n",
      "Validation Step :\n",
      "Accuracy: 0.7010 ; loss: 1.7620\n",
      "Accuracy Moy : 0.7022 ; loss Moy: 2.2042\n",
      "\n",
      "Epoch (15/15)\n",
      "Accuracy: 0.7049 ; loss: 0.5896\n",
      "Validation Step :\n",
      "Accuracy: 0.7026 ; loss: 1.7845\n",
      "Accuracy Moy : 0.7037 ; loss Moy: 2.2314\n",
      "\n",
      "Epoch (1/10)\n",
      "Accuracy: 0.7558 ; loss: 0.4991\n",
      "Validation Step :\n",
      "Accuracy: 0.7126 ; loss: 1.7601\n",
      "Accuracy Moy : 0.7329 ; loss Moy: 2.2060\n",
      "\n",
      "Epoch (2/10)\n",
      "Accuracy: 0.7345 ; loss: 0.5030\n",
      "Validation Step :\n",
      "Accuracy: 0.7147 ; loss: 1.8062\n",
      "Accuracy Moy : 0.7243 ; loss Moy: 2.2014\n",
      "\n",
      "Epoch (3/10)\n",
      "Accuracy: 0.7288 ; loss: 0.5026\n",
      "Validation Step :\n",
      "Accuracy: 0.7160 ; loss: 1.8228\n",
      "Accuracy Moy : 0.7223 ; loss Moy: 2.2002\n",
      "\n",
      "Epoch (4/10)\n",
      "Accuracy: 0.7265 ; loss: 0.4999\n",
      "Validation Step :\n",
      "Accuracy: 0.7169 ; loss: 1.8364\n",
      "Accuracy Moy : 0.7216 ; loss Moy: 2.2007\n",
      "\n",
      "Epoch (5/10)\n",
      "Accuracy: 0.7252 ; loss: 0.4989\n",
      "Validation Step :\n",
      "Accuracy: 0.7176 ; loss: 1.8408\n",
      "Accuracy Moy : 0.7213 ; loss Moy: 2.2023\n",
      "\n",
      "Epoch (6/10)\n",
      "Accuracy: 0.7245 ; loss: 0.4990\n",
      "Validation Step :\n",
      "Accuracy: 0.7181 ; loss: 1.8447\n",
      "Accuracy Moy : 0.7213 ; loss Moy: 2.2040\n",
      "\n",
      "Epoch (7/10)\n",
      "Accuracy: 0.7240 ; loss: 0.4983\n",
      "Validation Step :\n",
      "Accuracy: 0.7186 ; loss: 1.8442\n",
      "Accuracy Moy : 0.7213 ; loss Moy: 2.2061\n",
      "\n",
      "Epoch (8/10)\n",
      "Accuracy: 0.7238 ; loss: 0.4978\n",
      "Validation Step :\n",
      "Accuracy: 0.7190 ; loss: 1.8468\n",
      "Accuracy Moy : 0.7214 ; loss Moy: 2.2082\n",
      "\n",
      "Epoch (9/10)\n",
      "Accuracy: 0.7236 ; loss: 0.4978\n",
      "Validation Step :\n",
      "Accuracy: 0.7194 ; loss: 1.8495\n",
      "Accuracy Moy : 0.7215 ; loss Moy: 2.2110\n",
      "\n",
      "Epoch (10/10)\n",
      "Accuracy: 0.7235 ; loss: 0.4968\n",
      "Validation Step :\n",
      "Accuracy: 0.7198 ; loss: 1.8487\n",
      "Accuracy Moy : 0.7216 ; loss Moy: 2.2131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1037967/4014225201.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  accuracy = np.array(accuracy).reshape((-1))\n",
      "/tmp/ipykernel_1037967/4014225201.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val_accuracy = np.array(val_accuracy).reshape((-1))\n",
      "/tmp/ipykernel_1037967/4014225201.py:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  l= np.array(l).reshape((-1))\n",
      "/tmp/ipykernel_1037967/4014225201.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val_l = np.array(val_l).reshape((-1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch inference time :  0.2953801608085632  s\n",
      "Pruned inference time :  0.29499048471450806  s\n",
      "Epoch (1/75)\n",
      "Accuracy: 0.2550 ; loss: 2.1474\n",
      "Validation Step :\n",
      "Accuracy: 0.2714 ; loss: 1.4907\n",
      "Accuracy Moy : 0.2641 ; loss Moy: 1.7680\n",
      "\n",
      "Epoch (2/75)\n",
      "Accuracy: 0.3109 ; loss: 2.2029\n",
      "Validation Step :\n",
      "Accuracy: 0.3172 ; loss: 1.6252\n",
      "Accuracy Moy : 0.3143 ; loss Moy: 1.6746\n",
      "\n",
      "Epoch (3/75)\n",
      "Accuracy: 0.3392 ; loss: 2.2235\n",
      "Validation Step :\n",
      "Accuracy: 0.3399 ; loss: 1.9142\n",
      "Accuracy Moy : 0.3396 ; loss Moy: 1.7828\n",
      "\n",
      "Epoch (4/75)\n",
      "Accuracy: 0.3560 ; loss: 2.1311\n",
      "Validation Step :\n",
      "Accuracy: 0.3573 ; loss: 1.7408\n",
      "Accuracy Moy : 0.3567 ; loss Moy: 1.6917\n",
      "\n",
      "Epoch (5/75)\n",
      "Accuracy: 0.3704 ; loss: 2.2403\n",
      "Validation Step :\n",
      "Accuracy: 0.3716 ; loss: 1.6067\n",
      "Accuracy Moy : 0.3711 ; loss Moy: 1.6413\n",
      "\n",
      "Epoch (6/75)\n",
      "Accuracy: 0.3829 ; loss: 2.0337\n",
      "Validation Step :\n",
      "Accuracy: 0.3843 ; loss: 1.3674\n",
      "Accuracy Moy : 0.3837 ; loss Moy: 1.5908\n",
      "\n",
      "Epoch (7/75)\n",
      "Accuracy: 0.3947 ; loss: 2.0685\n",
      "Validation Step :\n",
      "Accuracy: 0.3953 ; loss: 1.1987\n",
      "Accuracy Moy : 0.3951 ; loss Moy: 1.6224\n",
      "\n",
      "Epoch (8/75)\n",
      "Accuracy: 0.4047 ; loss: 1.8767\n",
      "Validation Step :\n",
      "Accuracy: 0.4051 ; loss: 1.1061\n",
      "Accuracy Moy : 0.4049 ; loss Moy: 1.6070\n",
      "\n",
      "Epoch (9/75)\n",
      "Accuracy: 0.4139 ; loss: 1.9207\n",
      "Validation Step :\n",
      "Accuracy: 0.4141 ; loss: 1.5797\n",
      "Accuracy Moy : 0.4141 ; loss Moy: 1.6702\n",
      "\n",
      "Epoch (10/75)\n",
      "Accuracy: 0.4225 ; loss: 1.8702\n",
      "Validation Step :\n",
      "Accuracy: 0.4230 ; loss: 1.5307\n",
      "Accuracy Moy : 0.4228 ; loss Moy: 1.6062\n",
      "\n",
      "Epoch (11/75)\n",
      "Accuracy: 0.4308 ; loss: 1.9219\n",
      "Validation Step :\n",
      "Accuracy: 0.4306 ; loss: 1.6778\n",
      "Accuracy Moy : 0.4307 ; loss Moy: 1.7281\n",
      "\n",
      "Epoch (12/75)\n",
      "Accuracy: 0.4380 ; loss: 1.7969\n",
      "Validation Step :\n",
      "Accuracy: 0.4384 ; loss: 1.3569\n",
      "Accuracy Moy : 0.4382 ; loss Moy: 1.5378\n",
      "\n",
      "Epoch (13/75)\n",
      "Accuracy: 0.4452 ; loss: 1.7817\n",
      "Validation Step :\n",
      "Accuracy: 0.4456 ; loss: 1.2320\n",
      "Accuracy Moy : 0.4454 ; loss Moy: 1.5325\n",
      "\n",
      "Epoch (14/75)\n",
      "Accuracy: 0.4519 ; loss: 1.4918\n",
      "Validation Step :\n",
      "Accuracy: 0.4520 ; loss: 1.3357\n",
      "Accuracy Moy : 0.4519 ; loss Moy: 1.5969\n",
      "\n",
      "Epoch (15/75)\n",
      "Accuracy: 0.4579 ; loss: 1.3755\n",
      "Validation Step :\n",
      "Accuracy: 0.4581 ; loss: 1.4026\n",
      "Accuracy Moy : 0.4580 ; loss Moy: 1.5689\n",
      "\n",
      "Epoch (16/75)\n",
      "Accuracy: 0.4637 ; loss: 1.3898\n",
      "Validation Step :\n",
      "Accuracy: 0.4638 ; loss: 1.2226\n",
      "Accuracy Moy : 0.4638 ; loss Moy: 1.5889\n",
      "\n",
      "Epoch (17/75)\n",
      "Accuracy: 0.4691 ; loss: 1.1024\n",
      "Validation Step :\n",
      "Accuracy: 0.4693 ; loss: 1.3335\n",
      "Accuracy Moy : 0.4692 ; loss Moy: 1.5904\n",
      "\n",
      "Epoch (18/75)\n",
      "Accuracy: 0.4742 ; loss: 1.1143\n",
      "Validation Step :\n",
      "Accuracy: 0.4742 ; loss: 1.3589\n",
      "Accuracy Moy : 0.4742 ; loss Moy: 1.5913\n",
      "\n",
      "Epoch (19/75)\n",
      "Accuracy: 0.4789 ; loss: 0.9787\n",
      "Validation Step :\n",
      "Accuracy: 0.4789 ; loss: 1.2902\n",
      "Accuracy Moy : 0.4789 ; loss Moy: 1.5850\n",
      "\n",
      "Epoch (20/75)\n",
      "Accuracy: 0.4834 ; loss: 0.9523\n",
      "Validation Step :\n",
      "Accuracy: 0.4834 ; loss: 1.4059\n",
      "Accuracy Moy : 0.4834 ; loss Moy: 1.6196\n",
      "\n",
      "Epoch (21/75)\n",
      "Accuracy: 0.4876 ; loss: 1.0346\n",
      "Validation Step :\n",
      "Accuracy: 0.4875 ; loss: 1.4289\n",
      "Accuracy Moy : 0.4875 ; loss Moy: 1.6639\n",
      "\n",
      "Epoch (22/75)\n",
      "Accuracy: 0.4915 ; loss: 1.0504\n",
      "Validation Step :\n",
      "Accuracy: 0.4911 ; loss: 1.7225\n",
      "Accuracy Moy : 0.4913 ; loss Moy: 1.8711\n",
      "\n",
      "Epoch (23/75)\n",
      "Accuracy: 0.4950 ; loss: 1.1048\n",
      "Validation Step :\n",
      "Accuracy: 0.4948 ; loss: 1.4492\n",
      "Accuracy Moy : 0.4949 ; loss Moy: 1.6385\n",
      "\n",
      "Epoch (24/75)\n",
      "Accuracy: 0.4985 ; loss: 0.8918\n",
      "Validation Step :\n",
      "Accuracy: 0.4984 ; loss: 1.3028\n",
      "Accuracy Moy : 0.4985 ; loss Moy: 1.7002\n",
      "\n",
      "Epoch (25/75)\n",
      "Accuracy: 0.5019 ; loss: 0.8868\n",
      "Validation Step :\n",
      "Accuracy: 0.5017 ; loss: 1.3380\n",
      "Accuracy Moy : 0.5018 ; loss Moy: 1.7503\n",
      "\n",
      "Epoch (26/75)\n",
      "Accuracy: 0.5050 ; loss: 0.9175\n",
      "Validation Step :\n",
      "Accuracy: 0.5048 ; loss: 1.2047\n",
      "Accuracy Moy : 0.5049 ; loss Moy: 1.7418\n",
      "\n",
      "Epoch (27/75)\n",
      "Accuracy: 0.5080 ; loss: 0.8428\n",
      "Validation Step :\n",
      "Accuracy: 0.5077 ; loss: 1.6766\n",
      "Accuracy Moy : 0.5079 ; loss Moy: 1.8746\n",
      "\n",
      "Epoch (28/75)\n",
      "Accuracy: 0.5108 ; loss: 0.8941\n",
      "Validation Step :\n",
      "Accuracy: 0.5106 ; loss: 1.3188\n",
      "Accuracy Moy : 0.5107 ; loss Moy: 1.7445\n",
      "\n",
      "Epoch (29/75)\n",
      "Accuracy: 0.5135 ; loss: 0.9322\n",
      "Validation Step :\n",
      "Accuracy: 0.5130 ; loss: 1.7457\n",
      "Accuracy Moy : 0.5133 ; loss Moy: 1.8283\n",
      "\n",
      "Epoch (30/75)\n",
      "Accuracy: 0.5159 ; loss: 1.0081\n",
      "Validation Step :\n",
      "Accuracy: 0.5156 ; loss: 1.5465\n",
      "Accuracy Moy : 0.5158 ; loss Moy: 1.7046\n",
      "\n",
      "Epoch (31/75)\n",
      "Accuracy: 0.5184 ; loss: 1.0687\n",
      "Validation Step :\n",
      "Accuracy: 0.5181 ; loss: 1.5135\n",
      "Accuracy Moy : 0.5182 ; loss Moy: 1.7223\n",
      "\n",
      "Epoch (32/75)\n",
      "Accuracy: 0.5207 ; loss: 0.7499\n",
      "Validation Step :\n",
      "Accuracy: 0.5205 ; loss: 1.4477\n",
      "Accuracy Moy : 0.5206 ; loss Moy: 1.7240\n",
      "\n",
      "Epoch (33/75)\n",
      "Accuracy: 0.5230 ; loss: 0.8127\n",
      "Validation Step :\n",
      "Accuracy: 0.5227 ; loss: 1.4507\n",
      "Accuracy Moy : 0.5229 ; loss Moy: 1.7538\n",
      "\n",
      "Epoch (34/75)\n",
      "Accuracy: 0.5251 ; loss: 1.2967\n",
      "Validation Step :\n",
      "Accuracy: 0.5248 ; loss: 2.2781\n",
      "Accuracy Moy : 0.5250 ; loss Moy: 1.7859\n",
      "\n",
      "Epoch (35/75)\n",
      "Accuracy: 0.5271 ; loss: 0.7726\n",
      "Validation Step :\n",
      "Accuracy: 0.5268 ; loss: 1.8021\n",
      "Accuracy Moy : 0.5270 ; loss Moy: 1.7606\n",
      "\n",
      "Epoch (36/75)\n",
      "Accuracy: 0.5291 ; loss: 0.9079\n",
      "Validation Step :\n",
      "Accuracy: 0.5285 ; loss: 2.3717\n",
      "Accuracy Moy : 0.5288 ; loss Moy: 2.5034\n",
      "\n",
      "Epoch (37/75)\n",
      "Accuracy: 0.5307 ; loss: 0.9774\n",
      "Validation Step :\n",
      "Accuracy: 0.5302 ; loss: 2.3854\n",
      "Accuracy Moy : 0.5304 ; loss Moy: 2.2007\n",
      "\n",
      "Epoch (38/75)\n",
      "Accuracy: 0.5323 ; loss: 1.0266\n",
      "Validation Step :\n",
      "Accuracy: 0.5320 ; loss: 2.1637\n",
      "Accuracy Moy : 0.5322 ; loss Moy: 1.7978\n",
      "\n",
      "Epoch (39/75)\n",
      "Accuracy: 0.5341 ; loss: 0.8565\n",
      "Validation Step :\n",
      "Accuracy: 0.5337 ; loss: 2.0881\n",
      "Accuracy Moy : 0.5339 ; loss Moy: 2.1003\n",
      "\n",
      "Epoch (40/75)\n",
      "Accuracy: 0.5357 ; loss: 1.1431\n",
      "Validation Step :\n",
      "Accuracy: 0.5354 ; loss: 1.6772\n",
      "Accuracy Moy : 0.5356 ; loss Moy: 1.8966\n",
      "\n",
      "Epoch (41/75)\n",
      "Accuracy: 0.5374 ; loss: 0.9396\n",
      "Validation Step :\n",
      "Accuracy: 0.5370 ; loss: 1.4671\n",
      "Accuracy Moy : 0.5372 ; loss Moy: 1.9250\n",
      "\n",
      "Epoch (42/75)\n",
      "Accuracy: 0.5389 ; loss: 0.9266\n",
      "Validation Step :\n",
      "Accuracy: 0.5386 ; loss: 1.7852\n",
      "Accuracy Moy : 0.5388 ; loss Moy: 1.8383\n",
      "\n",
      "Epoch (43/75)\n",
      "Accuracy: 0.5404 ; loss: 0.9097\n",
      "Validation Step :\n",
      "Accuracy: 0.5401 ; loss: 1.6905\n",
      "Accuracy Moy : 0.5403 ; loss Moy: 2.0209\n",
      "\n",
      "Epoch (44/75)\n",
      "Accuracy: 0.5418 ; loss: 0.7828\n",
      "Validation Step :\n",
      "Accuracy: 0.5415 ; loss: 2.2178\n",
      "Accuracy Moy : 0.5417 ; loss Moy: 1.9041\n",
      "\n",
      "Epoch (45/75)\n",
      "Accuracy: 0.5433 ; loss: 0.7476\n",
      "Validation Step :\n",
      "Accuracy: 0.5430 ; loss: 1.5647\n",
      "Accuracy Moy : 0.5431 ; loss Moy: 1.9590\n",
      "\n",
      "Epoch (46/75)\n",
      "Accuracy: 0.5446 ; loss: 0.7775\n",
      "Validation Step :\n",
      "Accuracy: 0.5443 ; loss: 1.6385\n",
      "Accuracy Moy : 0.5445 ; loss Moy: 1.8850\n",
      "\n",
      "Epoch (47/75)\n",
      "Accuracy: 0.5460 ; loss: 1.0402\n",
      "Validation Step :\n",
      "Accuracy: 0.5457 ; loss: 2.0920\n",
      "Accuracy Moy : 0.5458 ; loss Moy: 1.9713\n",
      "\n",
      "Epoch (48/75)\n",
      "Accuracy: 0.5472 ; loss: 0.8895\n",
      "Validation Step :\n",
      "Accuracy: 0.5469 ; loss: 1.7465\n",
      "Accuracy Moy : 0.5471 ; loss Moy: 2.0710\n",
      "\n",
      "Epoch (49/75)\n",
      "Accuracy: 0.5485 ; loss: 0.9115\n",
      "Validation Step :\n",
      "Accuracy: 0.5482 ; loss: 1.5293\n",
      "Accuracy Moy : 0.5483 ; loss Moy: 1.9428\n",
      "\n",
      "Epoch (50/75)\n",
      "Accuracy: 0.5497 ; loss: 1.2506\n",
      "Validation Step :\n",
      "Accuracy: 0.5493 ; loss: 2.7830\n",
      "Accuracy Moy : 0.5495 ; loss Moy: 2.1934\n",
      "\n",
      "Epoch (51/75)\n",
      "Accuracy: 0.5508 ; loss: 0.8834\n",
      "Validation Step :\n",
      "Accuracy: 0.5505 ; loss: 2.2999\n",
      "Accuracy Moy : 0.5506 ; loss Moy: 1.9839\n",
      "\n",
      "Epoch (52/75)\n",
      "Accuracy: 0.5519 ; loss: 0.8439\n",
      "Validation Step :\n",
      "Accuracy: 0.5516 ; loss: 1.8789\n",
      "Accuracy Moy : 0.5517 ; loss Moy: 1.9328\n",
      "\n",
      "Epoch (53/75)\n",
      "Accuracy: 0.5530 ; loss: 0.8941\n",
      "Validation Step :\n",
      "Accuracy: 0.5526 ; loss: 1.5331\n",
      "Accuracy Moy : 0.5528 ; loss Moy: 2.3369\n",
      "\n",
      "Epoch (54/75)\n",
      "Accuracy: 0.5540 ; loss: 0.6151\n",
      "Validation Step :\n",
      "Accuracy: 0.5537 ; loss: 2.5100\n",
      "Accuracy Moy : 0.5538 ; loss Moy: 1.9485\n",
      "\n",
      "Epoch (55/75)\n",
      "Accuracy: 0.5550 ; loss: 1.0952\n",
      "Validation Step :\n",
      "Accuracy: 0.5547 ; loss: 2.3968\n",
      "Accuracy Moy : 0.5549 ; loss Moy: 1.9773\n",
      "\n",
      "Epoch (56/75)\n",
      "Accuracy: 0.5560 ; loss: 0.7999\n",
      "Validation Step :\n",
      "Accuracy: 0.5558 ; loss: 2.3974\n",
      "Accuracy Moy : 0.5559 ; loss Moy: 1.9580\n",
      "\n",
      "Epoch (57/75)\n",
      "Accuracy: 0.5570 ; loss: 0.9689\n",
      "Validation Step :\n",
      "Accuracy: 0.5566 ; loss: 2.2615\n",
      "Accuracy Moy : 0.5568 ; loss Moy: 2.2797\n",
      "\n",
      "Epoch (58/75)\n",
      "Accuracy: 0.5579 ; loss: 0.8789\n",
      "Validation Step :\n",
      "Accuracy: 0.5576 ; loss: 2.0695\n",
      "Accuracy Moy : 0.5578 ; loss Moy: 1.9742\n",
      "\n",
      "Epoch (59/75)\n",
      "Accuracy: 0.5589 ; loss: 0.7977\n",
      "Validation Step :\n",
      "Accuracy: 0.5586 ; loss: 1.9935\n",
      "Accuracy Moy : 0.5587 ; loss Moy: 1.9746\n",
      "\n",
      "Epoch (60/75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5598 ; loss: 1.0601\n",
      "Validation Step :\n",
      "Accuracy: 0.5595 ; loss: 1.9404\n",
      "Accuracy Moy : 0.5596 ; loss Moy: 2.0073\n",
      "\n",
      "Epoch (61/75)\n",
      "Accuracy: 0.5607 ; loss: 0.8091\n",
      "Validation Step :\n",
      "Accuracy: 0.5604 ; loss: 1.8766\n",
      "Accuracy Moy : 0.5606 ; loss Moy: 2.0181\n",
      "\n",
      "Epoch (62/75)\n",
      "Accuracy: 0.5616 ; loss: 0.9162\n",
      "Validation Step :\n",
      "Accuracy: 0.5613 ; loss: 2.3960\n",
      "Accuracy Moy : 0.5615 ; loss Moy: 2.0880\n",
      "\n",
      "Epoch (63/75)\n",
      "Accuracy: 0.5624 ; loss: 1.1321\n",
      "Validation Step :\n",
      "Accuracy: 0.5621 ; loss: 2.4303\n",
      "Accuracy Moy : 0.5623 ; loss Moy: 2.0266\n",
      "\n",
      "Epoch (64/75)\n",
      "Accuracy: 0.5633 ; loss: 0.8513\n",
      "Validation Step :\n",
      "Accuracy: 0.5631 ; loss: 2.2133\n",
      "Accuracy Moy : 0.5632 ; loss Moy: 1.9807\n",
      "\n",
      "Epoch (65/75)\n",
      "Accuracy: 0.5641 ; loss: 1.1453\n",
      "Validation Step :\n",
      "Accuracy: 0.5638 ; loss: 2.7322\n",
      "Accuracy Moy : 0.5640 ; loss Moy: 1.9873\n",
      "\n",
      "Epoch (66/75)\n",
      "Accuracy: 0.5648 ; loss: 0.9043\n",
      "Validation Step :\n",
      "Accuracy: 0.5646 ; loss: 1.9472\n",
      "Accuracy Moy : 0.5647 ; loss Moy: 2.1002\n",
      "\n",
      "Epoch (67/75)\n",
      "Accuracy: 0.5656 ; loss: 1.1644\n",
      "Validation Step :\n",
      "Accuracy: 0.5653 ; loss: 1.9760\n",
      "Accuracy Moy : 0.5655 ; loss Moy: 2.0439\n",
      "\n",
      "Epoch (68/75)\n",
      "Accuracy: 0.5664 ; loss: 1.1984\n",
      "Validation Step :\n",
      "Accuracy: 0.5661 ; loss: 2.1005\n",
      "Accuracy Moy : 0.5662 ; loss Moy: 2.0106\n",
      "\n",
      "Epoch (69/75)\n",
      "Accuracy: 0.5671 ; loss: 1.2820\n",
      "Validation Step :\n",
      "Accuracy: 0.5668 ; loss: 2.7147\n",
      "Accuracy Moy : 0.5669 ; loss Moy: 2.0181\n",
      "\n",
      "Epoch (70/75)\n",
      "Accuracy: 0.5678 ; loss: 1.1578\n",
      "Validation Step :\n",
      "Accuracy: 0.5675 ; loss: 2.3254\n",
      "Accuracy Moy : 0.5676 ; loss Moy: 2.1812\n",
      "\n",
      "Epoch (71/75)\n",
      "Accuracy: 0.5684 ; loss: 1.7271\n",
      "Validation Step :\n",
      "Accuracy: 0.5681 ; loss: 1.3551\n",
      "Accuracy Moy : 0.5683 ; loss Moy: 2.3847\n",
      "\n",
      "Epoch (72/75)\n",
      "Accuracy: 0.5690 ; loss: 0.8673\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m P\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[1;32m     16\u001b[0m          loss_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m          metric \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(),\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Entrainement         \u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m accur, loss, val_accur, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# plot hist\u001b[39;00m\n\u001b[1;32m     28\u001b[0m accuracy\u001b[38;5;241m.\u001b[39mappend(accur)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mPruning.train\u001b[0;34m(self, x_train, y_train, val_data, val_labels, epochs, batch_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y, predictions)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calcul du gradient\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Decente de gradient\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_weights))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_gradients \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1079\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_gradients)]\n\u001b[0;32m-> 1081\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1090\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:1488\u001b[0m, in \u001b[0;36m_DivNoNanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1481\u001b[0m x \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mconj(x)\n\u001b[1;32m   1482\u001b[0m y \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mconj(y)\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   1484\u001b[0m     array_ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m   1485\u001b[0m         math_ops\u001b[38;5;241m.\u001b[39mreduce_sum(math_ops\u001b[38;5;241m.\u001b[39mdiv_no_nan(grad, y), rx), sx),\n\u001b[1;32m   1486\u001b[0m     array_ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m   1487\u001b[0m         math_ops\u001b[38;5;241m.\u001b[39mreduce_sum(\n\u001b[0;32m-> 1488\u001b[0m             grad \u001b[38;5;241m*\u001b[39m \u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_no_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_no_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# pylint: disable=invalid-unary-operand-type\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m             ry),\n\u001b[1;32m   1490\u001b[0m         sy))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1652\u001b[0m, in \u001b[0;36mdiv_no_nan\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.divide_no_nan\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.divide_no_nan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv_no_nan\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_endpoints(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv_no_nan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiv_no_nan\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1629\u001b[0m   \u001b[38;5;124;03m\"\"\"Computes a safe divide which returns 0 if `y` (denominator) is zero.\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \n\u001b[1;32m   1631\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;124;03m    The element-wise value of the x divided by y.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv_no_nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m   1653\u001b[0m     x \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1654\u001b[0m     y \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:6717\u001b[0m, in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname_scope\u001b[39m(name, default_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skip_on_eager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   6693\u001b[0m   \u001b[38;5;124;03m\"\"\"Internal-only entry point for `name_scope*`.\u001b[39;00m\n\u001b[1;32m   6694\u001b[0m \n\u001b[1;32m   6695\u001b[0m \u001b[38;5;124;03m  Internal ops do not use the public API and instead rely on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6715\u001b[0m \u001b[38;5;124;03m    `name_scope*` context manager.\u001b[39;00m\n\u001b[1;32m   6716\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6717\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   6718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m internal_name_scope_v1(name, default_name, values)\n\u001b[1;32m   6720\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m skip_on_eager:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/context.py:2141\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecuting_eagerly\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m():\n\u001b[1;32m   2089\u001b[0m   \u001b[38;5;124;03m\"\"\"Checks whether the current thread has eager execution enabled.\u001b[39;00m\n\u001b[1;32m   2090\u001b[0m \n\u001b[1;32m   2091\u001b[0m \u001b[38;5;124;03m  Eager execution is enabled by default and this API returns `True`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;124;03m    `True` if the current thread has eager execution enabled.\u001b[39;00m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2141\u001b[0m   ctx \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2142\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_execution_mode \u001b[38;5;241m==\u001b[39m EAGER_MODE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    accuracy = np.array([])\n",
    "    val_accuracy = np.array([])\n",
    "\n",
    "    l = np.array([])\n",
    "    val_l = np.array([])\n",
    "\n",
    "\n",
    "    P = Pruning(tf.keras.models.clone_model(model), \n",
    "                pruning_factor =p)\n",
    "    lr = 1\n",
    "    for epoch in [75,15,10]:\n",
    "        # Paramètre d'entrainement\n",
    "        lr /= 10\n",
    "        P.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "                 loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                 metric = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        )\n",
    "\n",
    "        # Entrainement         \n",
    "        accur, loss, val_accur, val_loss = P.train(x_train, y_train, \n",
    "                                                   x_test, y_test, \n",
    "                                                   epochs = epoch, \n",
    "                                                   batch_size= 32 )  \n",
    "\n",
    "\n",
    "        # plot hist\n",
    "        accuracy = np.append(accuracy, accur)\n",
    "        val_accuracy = np.append(val_accuracy, val_accur)\n",
    "        l = np.append(l, loss)\n",
    "        val_l = np.append(val_l,val_loss)\n",
    "    \n",
    "    # Afficher les courbes d'entrainement\n",
    "    #plot_hist(f\"Lenet5_P_factor_{p}.png\")\n",
    "    \n",
    "    # inference time\n",
    "    pruned_inf_time = inference_time()\n",
    "    \n",
    "    # Enregister l'historique\n",
    "    dico[f\"P_factor_{p}_hist\"] = (accuracy, val_accuracy, l, val_l)\n",
    "    \n",
    "    # calcul du temps d'inférence\n",
    "    dico[f\"P_factor_{p}_inf_time\"] = pruned_inf_time\n",
    "\n",
    "    \n",
    "    # memory used\n",
    "    dico[f\"nb_params_p_factor_{p}\"] = count_parameters(P.model)\n",
    "    \n",
    "    # sauvegarder les poids\n",
    "    P.model.save_weights(f\"w_Lenet5_p_{p}.h5\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c317b",
   "metadata": {},
   "source": [
    "## Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(dic, figname, scratch = False):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for p in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        # Train accuracy\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][0], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # Validation accuracy\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dico[f\"P_factor_{p}_hist\"][1], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation accuracy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # train loss\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][2], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Train loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        # validation loss\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[f\"P_factor_{p}_hist\"][3], label = f\"{p}\")\n",
    "\n",
    "        plt.title(\"Validation loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "    if scratch == True: \n",
    "        # Courbe scratch\n",
    "        plt.subplot(221)\n",
    "        plt.plot(dic[\"scratch_hist\"][0], label = \"Scratch Train accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(222)\n",
    "        plt.plot(dic[\"scratch_hist\"][1], label = \"Scratch Val accur\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "\n",
    "        plt.subplot(223)\n",
    "        plt.plot(dic[\"scratch_hist\"][2], label = \"Scratch Train loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "\n",
    "\n",
    "        plt.subplot(224)\n",
    "        plt.plot(dic[\"scratch_hist\"][3], label = \"Scratch Val loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "    \n",
    "    \n",
    "    plt.savefig(figname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df765246",
   "metadata": {},
   "outputs": [],
   "source": [
    "figname= f\"test2_seed42.png\"\n",
    "eval_plot(dico, figname, scratch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data\n",
    "np.save(\"summary.npy\", dico)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
